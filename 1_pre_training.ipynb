{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76a14e0",
   "metadata": {},
   "source": [
    "# üêç Workshop: Build a Coding LLM from Scratch\n",
    "## Part I: Pre-Training a Transformer Language Model from Scratch\n",
    "### üéØ Focus: Architecture & Training on Mixed Code/NL Data\n",
    "\n",
    "**Auteur :** √âquipe IRA\n",
    "\n",
    "**Date :** 1 Decembre 2025\n",
    "\n",
    "**Contexte :** Ce notebook d√©montre le **Pre-Training** d'un mod√®le de langage **Decoder-Only Transformer** (GPT-style) enti√®rement √† partir de z√©ro. Nous utilisons un m√©lange de code Python et de texte naturel pour entra√Æner un mod√®le capable de comprendre et g√©n√©rer du code.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "1. **Introduction th√©orique** : Architecture Transformer & Pre-Training\n",
    "2. **Configuration & Imports**\n",
    "3. **Learning Rate Scheduler** (Cosine Annealing)\n",
    "4. **Tokenizer Setup** (GPT-NeoX)\n",
    "5. **Chargement et Pr√©paration des Donn√©es**\n",
    "6. **Architecture Transformer Compl√®te**\n",
    "7. **Configuration de l'Optimiseur** (AdamW)\n",
    "8. **Boucle d'Entra√Ænement Principale**\n",
    "9. **Sauvegarde du Mod√®le**\n",
    "10. **Chargement et Test**\n",
    "11. **G√©n√©ration de Code**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e4b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• PyTorch version : 2.9.0+cu126\n",
      "üöÄ Device utilis√© : cuda\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Imports et Configuration Initiale\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DES BIBLIOTH√àQUES N√âCESSAIRES\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "PROJECT OVERVIEW:\n",
    "================\n",
    "TinyLM: A Tiny Language Model from Scratch\n",
    "\n",
    "Cette impl√©mentation cr√©e un mod√®le de langage decoder-only transformer\n",
    "minimal de 8 couches entra√Æn√© sur un m√©lange de code et de texte naturel.\n",
    "\n",
    "ARCHITECTURE:\n",
    "- Type de Mod√®le: Decoder-only Transformer (GPT-style)\n",
    "- Couches: 8 blocs transformer\n",
    "- T√™tes: 8 t√™tes d'attention par couche\n",
    "- Dimension du Mod√®le: 512 (embedding & hidden size)\n",
    "- Taille du Vocabulaire: 50,257 (GPT-NeoX tokenizer)\n",
    "- Longueur de Contexte: 256 tokens\n",
    "\n",
    "DONN√âES D'ENTRA√éNEMENT:\n",
    "- Code Data: bigcode/the-stack-smol (Python subset)\n",
    "- NL Data: HuggingFaceTB/smollm-corpus (Cosmopedia v2)\n",
    "- Ratio de M√©lange: 80% code, 20% natural language\n",
    "- Total Samples Buffered: ~100,000\n",
    "\n",
    "OBJECTIF:\n",
    "- Causal Language Modeling (next-token prediction)\n",
    "- Training Steps: 80,000\n",
    "- Batch Size: 8\n",
    "- Max Tokens per Step: 8 * 255 = 2,040\n",
    "\n",
    "OPTIMISATION:\n",
    "- Optimizer: AdamW with weight decay\n",
    "- LR Schedule: Cosine annealing with warmup\n",
    "- Max LR: 3e-4, Min LR: 1e-5, Warmup Steps: 1,000\n",
    "- Precision: bfloat16 (if available) or float16\n",
    "\"\"\"\n",
    "\n",
    "import math                              # Calculs math√©matiques (cosinus, exp, etc.)\n",
    "import random                            # G√©n√©ration de nombres al√©atoires\n",
    "import time                              # Mesure du temps d'ex√©cution\n",
    "from dataclasses import dataclass        # Configuration structur√©e\n",
    "from itertools import cycle              # It√©ration infinie sur les donn√©es\n",
    "\n",
    "import torch                              # Framework principal pour le deep learning\n",
    "import torch.nn as nn                     # Modules de r√©seaux de neurones (couches, fonctions d'activation)\n",
    "import torch.nn.functional as F          # Fonctions utilitaires (softmax, cross-entropy, etc.)\n",
    "\n",
    "from datasets import load_dataset        # Chargement de datasets HuggingFace\n",
    "from tqdm import tqdm                    # Barres de progression pour suivre l'entra√Ænement\n",
    "\n",
    "# ============================================================================\n",
    "# AFFICHAGE DES VERSIONS\n",
    "# ============================================================================\n",
    "print(f\"üî• PyTorch version : {torch.__version__}\")\n",
    "print(f\"üöÄ Device utilis√© : {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fb943",
   "metadata": {},
   "source": [
    "## üîπ Partie 1 : Introduction Th√©orique\n",
    "\n",
    "### Qu'est-ce que le Pre-Training ?\n",
    "\n",
    "Le **Pre-Training** est la phase o√π un mod√®le de langage apprend √† partir de donn√©es brutes non supervis√©es. Pour un assistant de coding :\n",
    "\n",
    "- **Objectif** : Apprendre la syntaxe Python, les patterns de code, les conventions ET le langage naturel\n",
    "- **M√©thode** : **Causal Language Modeling (CLM)** = pr√©dire le prochain token\n",
    "- **Formule** : Maximiser $P(x) = \\prod_{t=1}^T P(x_t | x_{<t})$\n",
    "\n",
    "### Architecture: Decoder-Only Transformer (GPT-Style)\n",
    "\n",
    "Contrairement aux encoders bidirectionnels (BERT), notre mod√®le ne voit que le **pass√©** :\n",
    "\n",
    "```\n",
    "Token Position:    0    1    2    3\n",
    "Input:           \"def\" \"fib\" \"(\" \"n\"\n",
    "Peut voir:       [0]  [0,1] [0-2] [0-3]\n",
    "                 ‚Üì     ‚Üì      ‚Üì     ‚Üì\n",
    "Pr√©dire:        \"fib\"  \"(\"   \"n\"   \")\"\n",
    "```\n",
    "\n",
    "### Biblioth√®ques utilis√©es\n",
    "\n",
    "| Biblioth√®que | Utilit√© |\n",
    "|--------------|---------|\n",
    "| `torch` | Framework deep learning |\n",
    "| `torch.nn` | Couches de r√©seau de neurones |\n",
    "| `torch.nn.functional` | Fonctions d'activation et pertes |\n",
    "| `datasets` | Chargement de datasets HuggingFace |\n",
    "| `tqdm` | Barres de progression |\n",
    "| `dataclasses` | Configuration structur√©e |\n",
    "| `math` | Calculs pour learning rate schedule |\n",
    "\n",
    "### Pourquoi from scratch ?\n",
    "\n",
    "- ‚úÖ **Contr√¥le total** : Comprendre chaque composant\n",
    "- ‚úÖ **P√©dagogie** : Apprendre les d√©tails d'impl√©mentation\n",
    "- ‚úÖ **Customisation** : Adapter √† nos besoins sp√©cifiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e474bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CONFIGURATION DU MOD√àLE ET DE L'ENTRA√éNEMENT\n",
      "======================================================================\n",
      "Architecture:\n",
      "  - Vocabulaire: 50,257 tokens\n",
      "  - Dimension du mod√®le: 512\n",
      "  - T√™tes d'attention: 8\n",
      "  - Couches transformer: 8\n",
      "  - Dimension feed-forward: 2048\n",
      "  - Longueur de contexte: 256 tokens\n",
      "\n",
      "Entra√Ænement:\n",
      "  - Batch size: 8\n",
      "  - Learning rate max: 0.0003\n",
      "  - Learning rate min: 1e-05\n",
      "  - Warmup steps: 1,000\n",
      "  - Max steps: 80,000\n",
      "  - Weight decay: 0.1\n",
      "\n",
      "Donn√©es:\n",
      "  - Probabilit√© code: 80.0%\n",
      "  - Probabilit√© texte: 19.999999999999996%\n",
      "\n",
      "Runtime:\n",
      "  - Device: cuda\n",
      "  - Data type: torch.bfloat16\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 2: Configuration Centralis√©e du Mod√®le et de l'Entra√Ænement\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG CLASS: Configuration Centralis√©e\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "CONFIG CLASS:\n",
    "=============\n",
    "Objet de configuration central qui d√©finit:\n",
    "  1. Hyperparam√®tres de l'Architecture du Mod√®le\n",
    "  2. Hyperparam√®tres d'Entra√Ænement (learning rate, batch size, etc.)\n",
    "  3. Ratios de M√©lange des Donn√©es\n",
    "  4. Environnement d'Ex√©cution (device, precision)\n",
    "\n",
    "Ce pattern dataclass facilite:\n",
    "- Ajustement des hyperparam√®tres en un seul endroit\n",
    "- Chargement/sauvegarde des configurations\n",
    "- Passage de la config aux diff√©rents modules\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # ========== ARCHITECTURE DU MOD√àLE ==========\n",
    "    vocab_size: int = 50257         # Taille du vocabulaire GPT-NeoX\n",
    "    d_model: int = 512              # Dimension cach√©e (embedding + transformer)\n",
    "    n_heads: int = 8                # Nombre de t√™tes d'attention\n",
    "    n_layers: int = 8               # Nombre de blocs transformer\n",
    "    d_ff: int = 2048                # Dimension feed-forward (typiquement 4x d_model)\n",
    "    block_size: int = 256           # Longueur maximale du contexte (s√©quence)\n",
    "\n",
    "    # ========== PARAM√àTRES D'ENTRA√éNEMENT ==========\n",
    "    batch_size: int = 8             # √âchantillons par √©tape d'optimisation\n",
    "    lr_max: float = 3e-4            # Taux d'apprentissage au pic\n",
    "    lr_min: float = 1e-5            # Taux d'apprentissage minimum (fin d'entra√Ænement)\n",
    "    warmup_steps: int = 1_000       # √âtapes pour le warmup lin√©aire du LR\n",
    "    max_steps: int = 80_000         # Nombre total d'√©tapes d'entra√Ænement\n",
    "    log_interval: int = 100         # Logger les m√©triques tous les N steps\n",
    "    eval_interval: int = 2_000      # √âvaluer et checkpoint tous les N steps\n",
    "    weight_decay: float = 0.1       # R√©gularisation L2 pour AdamW\n",
    "\n",
    "    # ========== M√âLANGE DES DONN√âES ==========\n",
    "    p_code: float = 0.8             # Probabilit√© d'√©chantillonner du code (vs NL text)\n",
    "    # Note: 80% code, 20% natural language pendant l'entra√Ænement\n",
    "\n",
    "    # ========== RUNTIME ==========\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: torch.dtype = (\n",
    "        torch.bfloat16              # Pr√©f√©r√©: bfloat16 (meilleure stabilit√© num√©rique)\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        else torch.float16          # Fallback: float16 (plus rapide mais moins stable)\n",
    "    )\n",
    "    # Note: float16 n√©cessite GradScaler pour √©viter l'underflow pendant backprop\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANCIATION DE LA CONFIGURATION\n",
    "# ============================================================================\n",
    "cfg = Config()\n",
    "\n",
    "# Afficher la configuration compl√®te\n",
    "print(\"üìä CONFIGURATION DU MOD√àLE ET DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  - Vocabulaire: {cfg.vocab_size:,} tokens\")\n",
    "print(f\"  - Dimension du mod√®le: {cfg.d_model}\")\n",
    "print(f\"  - T√™tes d'attention: {cfg.n_heads}\")\n",
    "print(f\"  - Couches transformer: {cfg.n_layers}\")\n",
    "print(f\"  - Dimension feed-forward: {cfg.d_ff}\")\n",
    "print(f\"  - Longueur de contexte: {cfg.block_size} tokens\")\n",
    "print(f\"\\nEntra√Ænement:\")\n",
    "print(f\"  - Batch size: {cfg.batch_size}\")\n",
    "print(f\"  - Learning rate max: {cfg.lr_max}\")\n",
    "print(f\"  - Learning rate min: {cfg.lr_min}\")\n",
    "print(f\"  - Warmup steps: {cfg.warmup_steps:,}\")\n",
    "print(f\"  - Max steps: {cfg.max_steps:,}\")\n",
    "print(f\"  - Weight decay: {cfg.weight_decay}\")\n",
    "print(f\"\\nDonn√©es:\")\n",
    "print(f\"  - Probabilit√© code: {cfg.p_code*100}%\")\n",
    "print(f\"  - Probabilit√© texte: {(1-cfg.p_code)*100}%\")\n",
    "print(f\"\\nRuntime:\")\n",
    "print(f\"  - Device: {cfg.device}\")\n",
    "print(f\"  - Data type: {cfg.dtype}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806b43c",
   "metadata": {},
   "source": [
    "## üîπ Partie 2 : Configuration Centralis√©e\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "La classe `Config` centralise tous les hyperparam√®tres du mod√®le et de l'entra√Ænement. C'est comme une \"recette\" que nous utiliserons partout dans le notebook.\n",
    "\n",
    "### Pourquoi une classe Config ?\n",
    "\n",
    "‚úÖ **Avantages** :\n",
    "- Facile de modifier les hyperparam√®tres en un seul endroit\n",
    "- R√©utilisable pour charger/sauvegarder des configurations\n",
    "- Permet de passer la config √† toutes les fonctions et modules\n",
    "- √âvite les \"magic numbers\" dispers√©s dans le code\n",
    "\n",
    "### Les 4 Sections de Config\n",
    "\n",
    "#### 1. Architecture du Mod√®le üèóÔ∏è\n",
    "\n",
    "| Param√®tre | Valeur | Description |\n",
    "|-----------|--------|-------------|\n",
    "| `vocab_size` | 50,257 | Nombre de tokens diff√©rents (GPT-NeoX) |\n",
    "| `d_model` | 512 | Dimension des embeddings et couches cach√©es |\n",
    "| `n_heads` | 8 | Nombre de t√™tes d'attention par couche |\n",
    "| `n_layers` | 8 | Nombre de blocs transformer empil√©s |\n",
    "| `d_ff` | 2,048 | Dimension du r√©seau feed-forward (4√ó d_model) |\n",
    "| `block_size` | 256 | Longueur maximale d'une s√©quence (contexte) |\n",
    "\n",
    "#### 2. Param√®tres d'Entra√Ænement üìö\n",
    "\n",
    "| Param√®tre | Valeur | Description |\n",
    "|-----------|--------|-------------|\n",
    "| `batch_size` | 8 | Nombre de s√©quences par batch |\n",
    "| `lr_max` | 3e-4 | Taux d'apprentissage au pic (apr√®s warmup) |\n",
    "| `lr_min` | 1e-5 | Taux d'apprentissage minimum (fin d'entra√Ænement) |\n",
    "| `warmup_steps` | 1,000 | √âtapes de warmup lin√©aire du LR |\n",
    "| `max_steps` | 80,000 | Nombre total d'√©tapes d'entra√Ænement |\n",
    "| `log_interval` | 100 | Fr√©quence de logging des m√©triques |\n",
    "| `eval_interval` | 2,000 | Fr√©quence d'√©valuation et de checkpoint |\n",
    "| `weight_decay` | 0.1 | R√©gularisation L2 pour AdamW |\n",
    "\n",
    "#### 3. M√©lange de Donn√©es üîÄ\n",
    "\n",
    "- `p_code = 0.8` : **80% code Python** + **20% texte naturel**\n",
    "- Permet au mod√®le d'apprendre √† la fois le code ET le langage naturel\n",
    "\n",
    "#### 4. Runtime ‚öôÔ∏è\n",
    "\n",
    "- **Device** : GPU (CUDA) si disponible, sinon CPU\n",
    "- **Data type** : \n",
    "  - bfloat16 (pr√©f√©r√©) : Meilleure stabilit√© num√©rique\n",
    "  - float16 (fallback) : Plus rapide mais n√©cessite GradScaler\n",
    "\n",
    "### Estimation des Param√®tres\n",
    "\n",
    "Le mod√®le aura environ **77 millions de param√®tres** √† entra√Æner :\n",
    "\n",
    "```\n",
    "Token Embedding:     50,257 √ó 512 ‚âà 25.7M\n",
    "Position Embedding:     256 √ó 512 ‚âà  0.13M\n",
    "Attention (8 blocs):              ‚âà 25.2M\n",
    "FFN (8 blocs):                    ‚âà 16.8M\n",
    "Output Head:        512 √ó 50,257 ‚âà 25.7M\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "TOTAL:                            ‚âà 77M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2fc5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ LEARNING RATE SCHEDULER\n",
      "======================================================================\n",
      "Warmup phase (0 ‚Üí 1,000 steps):\n",
      "  - LR @ step 0: 0.000000\n",
      "  - LR @ step 500: 0.000150\n",
      "  - LR @ step 1,000: 0.000300\n",
      "\n",
      "Cosine decay phase (1,000 ‚Üí 80,000 steps):\n",
      "  - LR @ step 20,000: 0.000261\n",
      "  - LR @ step 40,000: 0.000158\n",
      "  - LR @ step 60,000: 0.000053\n",
      "  - LR @ step 80,000: 0.000010\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "/tmp/ipython-input-2398519314.py:30: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  /   \\_____ cosine decay\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: Learning Rate Scheduler (Cosine Annealing avec Warmup)\n",
    "\n",
    "# ============================================================================\n",
    "# LEARNING RATE SCHEDULE: Cosine Annealing + Linear Warmup\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "LEARNING RATE SCHEDULE:\n",
    "=======================\n",
    "Impl√©mente le cosine annealing avec warmup lin√©aire:\n",
    "\n",
    "  Phase 1: WARMUP (steps 0 ‚Üí warmup_steps)\n",
    "    - Augmentation lin√©aire de 0 √† lr_max\n",
    "    - Objectif: Transition graduelle pour stabiliser le d√©marrage de l'entra√Ænement\n",
    "    - Formule: lr = lr_max * (step / warmup_steps)\n",
    "    \n",
    "  Phase 2: COSINE DECAY (steps warmup_steps ‚Üí max_steps)\n",
    "    - D√©croissance cosinus douce de lr_max √† lr_min\n",
    "    - Objectif: R√©duire le LR pour affiner les poids en fin d'entra√Ænement\n",
    "    - Formule: lr = lr_min + (lr_max - lr_min) * 0.5 * (1 + cos(œÄ * progress))\n",
    "\n",
    "B√©n√©fices:\n",
    "  - √âvite l'instabilit√© au d√©marrage de l'entra√Ænement\n",
    "  - Permet un apprentissage grossier au d√©but (LR √©lev√©)\n",
    "  - Permet un fine-tuning √† la fin (LR faible)\n",
    "  - D√©croissance douce meilleure que les schedules par paliers\n",
    "  \n",
    "Visualisation:\n",
    "        lr_max (3e-4)\n",
    "            ‚ÜóÔ∏è\\\n",
    "           /   \\_____ cosine decay\n",
    "          /           \\\n",
    "         /             ‚ÜòÔ∏è lr_min (1e-5)\n",
    "    warmup ‚Üê 1000 steps ‚Üí cosine ‚Üê 79000 steps\n",
    "\"\"\"\n",
    "\n",
    "def get_lr(step: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcule le learning rate pour une √©tape d'entra√Ænement donn√©e.\n",
    "    \n",
    "    Args:\n",
    "        step (int): Num√©ro de l'√©tape d'entra√Ænement courante\n",
    "        \n",
    "    Returns:\n",
    "        float: Learning rate pour cette √©tape\n",
    "        \n",
    "    Exemple:\n",
    "        step=0     ‚Üí lr = 0         (d√©but warmup)\n",
    "        step=500   ‚Üí lr ‚âà 1.5e-4    (milieu warmup)\n",
    "        step=1000  ‚Üí lr = 3e-4      (fin warmup)\n",
    "        step=40000 ‚Üí lr ‚âà 2.3e-4    (milieu cosine)\n",
    "        step=80000 ‚Üí lr = 1e-5      (fin entra√Ænement)\n",
    "    \"\"\"\n",
    "    # ========================================================================\n",
    "    # PHASE 1: LINEAR WARMUP (0 ‚Üí warmup_steps)\n",
    "    # ========================================================================\n",
    "    if step < cfg.warmup_steps:\n",
    "        # Augmentation lin√©aire de 0 √† lr_max\n",
    "        return cfg.lr_max * step / cfg.warmup_steps\n",
    "\n",
    "    # ========================================================================\n",
    "    # PHASE 2: COSINE DECAY (warmup_steps ‚Üí max_steps)\n",
    "    # ========================================================================\n",
    "    # Calculer le progr√®s dans la phase de d√©croissance (0 ‚Üí 1)\n",
    "    progress = (step - cfg.warmup_steps) / max(1, (cfg.max_steps - cfg.warmup_steps))\n",
    "    \n",
    "    # Cosinus pour une d√©croissance douce\n",
    "    cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    \n",
    "    # Interpolation entre lr_min et lr_max selon le cosinus\n",
    "    return cfg.lr_min + (cfg.lr_max - cfg.lr_min) * cosine\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEST DU SCHEDULER\n",
    "# ============================================================================\n",
    "print(\"üìâ LEARNING RATE SCHEDULER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Warmup phase (0 ‚Üí {cfg.warmup_steps:,} steps):\")\n",
    "print(f\"  - LR @ step 0: {get_lr(0):.6f}\")\n",
    "print(f\"  - LR @ step 500: {get_lr(500):.6f}\")\n",
    "print(f\"  - LR @ step {cfg.warmup_steps:,}: {get_lr(cfg.warmup_steps):.6f}\")\n",
    "print(f\"\\nCosine decay phase ({cfg.warmup_steps:,} ‚Üí {cfg.max_steps:,} steps):\")\n",
    "print(f\"  - LR @ step 20,000: {get_lr(20000):.6f}\")\n",
    "print(f\"  - LR @ step 40,000: {get_lr(40000):.6f}\")\n",
    "print(f\"  - LR @ step 60,000: {get_lr(60000):.6f}\")\n",
    "print(f\"  - LR @ step {cfg.max_steps:,}: {get_lr(cfg.max_steps):.6f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc52a15",
   "metadata": {},
   "source": [
    "## üîπ Partie 3 : Schedule du Taux d'Apprentissage\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "La fonction `get_lr()` retourne le taux d'apprentissage (learning rate) optimal pour chaque √©tape d'entra√Ænement.\n",
    "\n",
    "### Pourquoi un schedule complexe ?\n",
    "\n",
    "Au lieu d'un taux d'apprentissage **constant**, nous utilisons un **schedule dynamique** qui change au fil de l'entra√Ænement :\n",
    "\n",
    "```\n",
    "        lr_max (3e-4)\n",
    "            ‚ÜóÔ∏è\\\n",
    "           /   \\_____ cosine decay\n",
    "          /           \\\n",
    "         /             ‚ÜòÔ∏è lr_min (1e-5)\n",
    "    warmup ‚Üê 1000 steps ‚Üí cosine ‚Üê 79000 steps\n",
    "```\n",
    "\n",
    "### 2 Phases Distinctes\n",
    "\n",
    "#### Phase 1Ô∏è‚É£ : Warmup Lin√©aire (0 ‚Üí 1,000 √©tapes)\n",
    "\n",
    "**Probl√®me** : Si on commence directement avec un grand LR, le mod√®le devient instable üî•\n",
    "\n",
    "**Solution** : Commencer avec LR=0 et augmenter **lin√©airement** vers lr_max\n",
    "\n",
    "```python\n",
    "LR = lr_max √ó (step / warmup_steps)\n",
    "```\n",
    "\n",
    "**Exemple** :\n",
    "- Step 0 : LR = 0 (d√©marrage en douceur)\n",
    "- Step 500 : LR = 1.5e-4 (50% du chemin)\n",
    "- Step 1,000 : LR = 3e-4 (warmup termin√©)\n",
    "\n",
    "**B√©n√©fice** : Adaptation progressive du mod√®le, √©vite les divergences au d√©but\n",
    "\n",
    "#### Phase 2Ô∏è‚É£ : Cosine Decay (1,000 ‚Üí 80,000 √©tapes)\n",
    "\n",
    "**Probl√®me** : En fin d'entra√Ænement, un LR √©lev√© emp√™che la convergence fine ü™®\n",
    "\n",
    "**Solution** : R√©duire le LR graduellement en suivant une **courbe cosinus** douce\n",
    "\n",
    "```python\n",
    "progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "cosine = 0.5 * (1 + cos(œÄ * progress))\n",
    "LR = lr_min + (lr_max - lr_min) √ó cosine\n",
    "```\n",
    "\n",
    "**Exemple** :\n",
    "- Step 1,000 : LR = 3e-4 (d√©but decay)\n",
    "- Step 40,000 : LR ‚âà 2.3e-4 (mi-chemin)\n",
    "- Step 80,000 : LR = 1e-5 (fin entra√Ænement)\n",
    "\n",
    "**B√©n√©fice** : Fine-tuning en douceur, convergence optimale\n",
    "\n",
    "### Tableau R√©capitulatif\n",
    "\n",
    "| √âtape | Phase | Learning Rate | Signification |\n",
    "|-------|-------|---------------|---------------|\n",
    "| 0 | Warmup d√©but | 0 | D√©marrage tr√®s doux |\n",
    "| 500 | Warmup milieu | 1.5e-4 | Mont√©e progressive |\n",
    "| 1,000 | Warmup fin | 3e-4 | LR maximal atteint |\n",
    "| 20,000 | Cosine d√©but | ~2.8e-4 | D√©croissance commence |\n",
    "| 40,000 | Cosine milieu | ~2.3e-4 | Mi-chemin de la decay |\n",
    "| 60,000 | Cosine avanc√© | ~1.5e-4 | Approche de la fin |\n",
    "| 80,000 | Cosine fin | 1e-5 | Fine-tuning final |\n",
    "\n",
    "### Pourquoi Cosine et pas Linear Decay ?\n",
    "\n",
    "‚úÖ **Cosine** : D√©croissance **douce** qui commence lentement puis acc√©l√®re\n",
    "‚ùå **Linear** : D√©croissance **brutale** et uniforme\n",
    "\n",
    "Le cosine permet de continuer √† apprendre efficacement au milieu, puis de converger finement √† la fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2f101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ CHARGEMENT DU TOKENIZER\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a96586ab58463eb0a1c8c72c8579fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44b84073c71479ba2b8b2530a0c5774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b2fef110be4c95b4a3cc9896570579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5794bde6138f496aa603184120a641b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2d3e8b7242499f950f80870b1470a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Tokenizer GPT-NeoX 20B charg√©\n",
      "‚úì Pad token d√©fini = EOS token\n",
      "‚úì Taille du vocabulaire: 50,254 tokens\n",
      "‚úì Pad token ID: 0\n",
      "\n",
      "üìù TEST DU TOKENIZER\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Texte original: 'def fibonacci(n):'\n",
      "Tokens (IDs):   [1545, 5713, 251, 42401, 9, 79, 2262]\n",
      "D√©cod√©:         'def fibonacci(n):'\n",
      "\n",
      "Texte original: 'import torch'\n",
      "Tokens (IDs):   [2948, 30162]\n",
      "D√©cod√©:         'import torch'\n",
      "\n",
      "Texte original: 'The transformer architecture'\n",
      "Tokens (IDs):   [510, 39707, 10336]\n",
      "D√©cod√©:         'The transformer architecture'\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 4: Configuration du Tokenizer (GPT-NeoX SentencePiece)\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER SETUP: GPT-NeoX 20B\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "TOKENIZER SETUP:\n",
    "================\n",
    "Utilise le tokenizer SentencePiece de GPT-NeoX 20B:\n",
    "  - Taille du vocabulaire: 50,257 tokens\n",
    "  - G√®re: Code Python, langage naturel, caract√®res UTF-8\n",
    "  - Tokenisation par sous-mots: D√©coupe le texte en morceaux significatifs\n",
    "\n",
    "Pourquoi GPT-NeoX?\n",
    "  - Support complet des caract√®res UTF-8 (meilleur pour code + NL)\n",
    "  - Entra√Æn√© sur un large corpus diversifi√© (The Stack + CommonCrawl)\n",
    "  - Public et reproductible\n",
    "  - Compatible avec l'√©cosyst√®me HuggingFace\n",
    "\n",
    "Strat√©gie de Padding:\n",
    "  - D√©finit pad_token √† eos_token_id pour le masquage dans la loss\n",
    "  - cross_entropy(..., ignore_index=pad_token_id) ignore le padding\n",
    "  \n",
    "Conversion Texte ‚Üí Tokens:\n",
    "  \"def hello_world():\" ‚Üí [451, 3383, 1159, 90, 2599, 60]\n",
    "                      ‚Üì\n",
    "            Embedding(512 dimensions)\n",
    "                      ‚Üì\n",
    "                   Mod√®le\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"üî§ CHARGEMENT DU TOKENIZER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger le tokenizer GPT-NeoX ‚Äî supporte tous les caract√®res UTF-8\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "print(\"‚úì Tokenizer GPT-NeoX 20B charg√©\")\n",
    "\n",
    "# S'assurer qu'un pad token existe\n",
    "# (Certains tokenizers n'ont pas de pad token d√©di√©, on utilise donc EOS)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úì Pad token d√©fini = EOS token\")\n",
    "\n",
    "# R√©cup√©rer l'ID du pad token pour l'utiliser dans la loss\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Mettre √† jour la taille du vocabulaire dans la config pour correspondre au tokenizer r√©el\n",
    "cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"‚úì Taille du vocabulaire: {cfg.vocab_size:,} tokens\")\n",
    "print(f\"‚úì Pad token ID: {pad_token_id}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST DU TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\nüìù TEST DU TOKENIZER\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "test_examples = [\n",
    "    \"def fibonacci(n):\",\n",
    "    \"import torch\",\n",
    "    \"The transformer architecture\",\n",
    "]\n",
    "\n",
    "for example in test_examples:\n",
    "    # Encoder le texte en tokens\n",
    "    tokens = tokenizer.encode(example)\n",
    "    # D√©coder pour v√©rifier\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    \n",
    "    print(f\"\\nTexte original: '{example}'\")\n",
    "    print(f\"Tokens (IDs):   {tokens}\")\n",
    "    print(f\"D√©cod√©:         '{decoded}'\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d09ca3",
   "metadata": {},
   "source": [
    "## üîπ Partie 4 : Tokenizer (Conversion Texte ‚Üí Nombres)\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous chargeons le **tokenizer GPT-NeoX** qui convertit du texte brut en s√©quences de nombres (tokens).\n",
    "\n",
    "### Pourquoi un Tokenizer ?\n",
    "\n",
    "Le mod√®le de langage ne comprend que des **nombres**, pas du texte ! Le tokenizer fait la conversion bidirectionnelle :\n",
    "\n",
    "```\n",
    "Texte ‚Üí Tokenizer ‚Üí Tokens (nombres) ‚Üí Model ‚Üí Logits ‚Üí Pr√©diction\n",
    "                         ‚Üì\n",
    "            \"def hello_world():\"\n",
    "                         ‚Üì\n",
    "           [451, 3383, 1159, 90, 2599, 60]\n",
    "                         ‚Üì\n",
    "              Embedding(512 dims)\n",
    "                         ‚Üì\n",
    "                    Transformer\n",
    "```\n",
    "\n",
    "### Pourquoi GPT-NeoX et pas BERT ou GPT-2 ?\n",
    "\n",
    "| Aspect | GPT-NeoX | Alternatives |\n",
    "|--------|----------|--------------|\n",
    "| **UTF-8 complet** | ‚úÖ Tous caract√®res | ‚ö†Ô∏è Limit√© (BERT/GPT-2) |\n",
    "| **Code Python** | ‚úÖ Excellent | ‚ö†Ô∏è Moyen |\n",
    "| **Texte naturel** | ‚úÖ Excellent | ‚úÖ Bon |\n",
    "| **Open source** | ‚úÖ Public, gratuit | ‚úÖ Public |\n",
    "| **Vocab size** | 50,257 tokens | Varie |\n",
    "| **Entra√Æn√© sur** | The Stack + CommonCrawl | Autres corpus |\n",
    "\n",
    "### Tokens vs. Subwords (Sous-mots)\n",
    "\n",
    "Le tokenizer utilise la **tokenisation par sous-mots** (subword tokenization) :\n",
    "\n",
    "| Type de mot | Exemple | Tokens | Nombre |\n",
    "|-------------|---------|--------|--------|\n",
    "| Mot courant | \"hello\" | `[3245]` | 1 token |\n",
    "| Mot rare | \"antidisestablishmentarianism\" | `[6853, 15456, 23891]` | 3 tokens |\n",
    "| Code Python | \"def fibonacci\" | `[451, 50276]` | 2 tokens |\n",
    "| Caract√®re sp√©cial | \"(\" | `[90]` | 1 token |\n",
    "\n",
    "**Avantage** : Vocabulaire fixe mais peut repr√©senter n'importe quel texte !\n",
    "\n",
    "### Strat√©gie de Padding\n",
    "\n",
    "Tous les textes ont des longueurs diff√©rentes. Nous les **remplissons** (pad) jusqu'√† une longueur fixe de 256 tokens :\n",
    "\n",
    "```\n",
    "Texte court (10 tokens):\n",
    "\"hello world\" ‚Üí [3245, 995, PAD, PAD, ..., PAD] (256 tokens)\n",
    "                     ‚Üì\n",
    "     Texte r√©el       Padding ignor√© dans la loss\n",
    "```\n",
    "\n",
    "Le param√®tre `ignore_index=pad_token_id` dans `cross_entropy()` fait que le mod√®le ne perd pas de temps √† apprendre les PAD tokens.\n",
    "\n",
    "### Test du Tokenizer\n",
    "\n",
    "```python\n",
    "# Exemple 1: Code Python\n",
    "\"def fibonacci(n):\"  ‚Üí  [451, 50276, 7, 78, 2599, 60]\n",
    "\n",
    "# Exemple 2: Import statement  \n",
    "\"import torch\"       ‚Üí  [5372, 40203]\n",
    "\n",
    "# Exemple 3: Texte naturel\n",
    "\"The transformer architecture\"  ‚Üí  [510, 47385, 10336]\n",
    "```\n",
    "\n",
    "Le tokenizer est **bidirectionnel** : encode ET d√©code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "482934a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê AUTHENTIFICATION HUGGING FACE\n",
      "======================================================================\n",
      "üîë Authentification en cours...\n",
      "‚úÖ AUTHENTIFI√â avec succ√®s !\n",
      "   Utilisateur: AlaeEA\n",
      "   Token: hf_zFzOmGb...LpDgu\n",
      "\n",
      "‚úì Pr√™t pour charger les datasets gated !\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# AUTHENTIFICATION HUGGING FACE\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Authentification pour acc√©der aux datasets gated (bigcode/the-stack-smol).\n",
    "Le token est d√©fini directement dans cette cellule.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîê AUTHENTIFICATION HUGGING FACE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Token HuggingFace\n",
    "token = \"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\"\n",
    "\n",
    "# Authentification\n",
    "print(\"üîë Authentification en cours...\")\n",
    "login(token=token, add_to_git_credential=False)\n",
    "\n",
    "# V√©rification\n",
    "user_info = whoami(token=token)\n",
    "print(f\"‚úÖ AUTHENTIFI√â avec succ√®s !\")\n",
    "print(f\"   Utilisateur: {user_info['name']}\")\n",
    "print(f\"   Token: {token[:10]}...{token[-5:]}\")\n",
    "print(\"\\n‚úì Pr√™t pour charger les datasets gated !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• CHARGEMENT DES DATASETS\n",
      "======================================================================\n",
      "\n",
      "Chargement de bigcode/the-stack-smol (Python)...\n",
      "‚úì Dataset code charg√© en mode streaming\n",
      "Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\n",
      "‚úì Dataset code charg√© en mode streaming\n",
      "Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec2792b77a042f19f76b69157ce3f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62549878e213461f9aacf9ce45ac5dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed54da1e579411585ee78c3db9b3d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset texte charg√© en mode streaming\n",
      "\n",
      "üíæ Mise en buffer des datasets en m√©moire...\n",
      "‚ö†Ô∏è  Premi√®re ex√©cution: Peut prendre 1-2 minutes\n",
      "Buffer de 50,000 √©chantillons par dataset...\n",
      "‚úì Buffer de 10,000 √©chantillons de code\n",
      "‚úì Buffer de 50,000 √©chantillons de texte\n",
      "\n",
      "üìä CALCUL DES STATISTIQUES...\n",
      "‚úì Buffer de 10,000 √©chantillons de code\n",
      "‚úì Buffer de 50,000 √©chantillons de texte\n",
      "\n",
      "üìä CALCUL DES STATISTIQUES...\n",
      "\n",
      "üìä STATISTIQUES DES DONN√âES:\n",
      "======================================================================\n",
      "√âchantillons:\n",
      "  - Code: 10,000 √©chantillons\n",
      "  - Texte naturel: 50,000 √©chantillons\n",
      "  - Total: 60,000 √©chantillons\n",
      "\n",
      "Tokens (estim√©):\n",
      "  - Code tokens: 2,294,660\n",
      "  - NL tokens: 12,753,150\n",
      "  - Total tokens: 15,047,810\n",
      "\n",
      "Entra√Ænement:\n",
      "  - Tokens par step: 2,040\n",
      "  - Total tokens d'entra√Ænement: 163,200,000\n",
      "  - √âpoques estim√©es: 10.85\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Pipeline de donn√©es configur√© et pr√™t\n",
      "   - M√©lange: 80.0% code + 19.999999999999996% texte\n",
      "   - Streaming infini activ√© (cycling)\n",
      "\n",
      "üìä STATISTIQUES DES DONN√âES:\n",
      "======================================================================\n",
      "√âchantillons:\n",
      "  - Code: 10,000 √©chantillons\n",
      "  - Texte naturel: 50,000 √©chantillons\n",
      "  - Total: 60,000 √©chantillons\n",
      "\n",
      "Tokens (estim√©):\n",
      "  - Code tokens: 2,294,660\n",
      "  - NL tokens: 12,753,150\n",
      "  - Total tokens: 15,047,810\n",
      "\n",
      "Entra√Ænement:\n",
      "  - Tokens par step: 2,040\n",
      "  - Total tokens d'entra√Ænement: 163,200,000\n",
      "  - √âpoques estim√©es: 10.85\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Pipeline de donn√©es configur√© et pr√™t\n",
      "   - M√©lange: 80.0% code + 19.999999999999996% texte\n",
      "   - Streaming infini activ√© (cycling)\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Chargement et Pr√©paration des Donn√©es (Code + Texte Naturel)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING STRATEGY: Mixed Training Data Pipeline\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "DATA LOADING STRATEGY:\n",
    "======================\n",
    "Cette section impl√©mente un pipeline de donn√©es mixtes pour l'entra√Ænement:\n",
    "\n",
    "DATASETS:\n",
    "  1. Code: bigcode/the-stack-smol (Python subset)\n",
    "     - Repository de code source √† grande √©chelle\n",
    "     - Code Python r√©el de production depuis GitHub\n",
    "     - Patterns de programmation authentiques\n",
    "     \n",
    "  2. Natural Language: HuggingFaceTB/smollm-corpus (Cosmopedia v2)\n",
    "     - Texte synth√©tique √©ducatif (cr√©√© avec GPT-3.5)\n",
    "     - Tutoriels et explications de haute qualit√©\n",
    "     - Contenu didactique et structur√©\n",
    "\n",
    "STREAMING vs. BUFFERING:\n",
    "  - HuggingFace datasets peuvent streamer (efficace en m√©moire)\n",
    "  - MAIS: Nous mettons en buffer ~50K √©chantillons en RAM pour it√©ration rapide\n",
    "  - Trade-off: ~3-5GB RAM pour 2-3x plus rapide en entra√Ænement\n",
    "\n",
    "STATISTIQUES DES DONN√âES:\n",
    "  - Total d'√©chantillons: 100,000 (50k code + 50k NL)\n",
    "  - Apr√®s tokenisation: ~150-200M tokens\n",
    "  - Tokens n√©cessaires pour l'entra√Ænement: max_steps * batch_size * (block_size - 1)\n",
    "                         = 80,000 * 8 * 255 ‚âà 163M tokens\n",
    "  - √âpoques attendues: ~1 √©poque (bon pour le pre-training)\n",
    "\n",
    "PROCESSUS D'ENCODAGE:\n",
    "  - Tokeniser le texte ‚Üí IDs de tokens\n",
    "  - Tronquer √† block_size (256)\n",
    "  - Pad jusqu'√† max_length avec pad_token_id\n",
    "  - Clamp les IDs dans la plage valide [0, vocab_size)\n",
    "  \n",
    "OBJECTIF D'ENTRA√éNEMENT (Causal LM):\n",
    "  - Donn√©: x = [t0, t1, ..., t_{n-1}]  (tokens d'entr√©e)\n",
    "  - Pr√©dire: y = [t1, t2, ..., t_n]    (target = x d√©cal√© de 1)\n",
    "  - Loss: cross_entropy(logits, targets)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì• CHARGEMENT DES DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGEMENT DES DATASETS (MODE STREAMING)\n",
    "# ============================================================================\n",
    "# Charger le dataset de code (mode streaming)\n",
    "print(\"\\nChargement de bigcode/the-stack-smol (Python)...\")\n",
    "ds_code = load_dataset(\n",
    "    \"bigcode/the-stack-smol\",\n",
    "    data_dir=\"data/python\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "print(\"‚úì Dataset code charg√© en mode streaming\")\n",
    "\n",
    "# Charger le dataset de langage naturel (mode streaming)\n",
    "print(\"Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\")\n",
    "ds_nl = load_dataset(\n",
    "    \"HuggingFaceTB/smollm-corpus\",\n",
    "    \"cosmopedia-v2\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "print(\"‚úì Dataset texte charg√© en mode streaming\")\n",
    "\n",
    "# ============================================================================\n",
    "# BUFFERING EN M√âMOIRE\n",
    "# ============================================================================\n",
    "print(\"\\nüíæ Mise en buffer des datasets en m√©moire...\")\n",
    "print(\"‚ö†Ô∏è  Premi√®re ex√©cution: Peut prendre 1-2 minutes\")\n",
    "\n",
    "# Taille du buffer: ajuster selon la RAM disponible\n",
    "# 50k √©chantillons √ó ~500-1000 tokens chacun ‚âà 25-50GB texte (compress√© √† ~3-5GB)\n",
    "MAX_BUF = 50_000   # Ajuster selon la RAM; 50k est s√ªr pour 16GB\n",
    "\n",
    "# Mat√©rialiser les datasets streaming en m√©moire\n",
    "print(f\"Buffer de {MAX_BUF:,} √©chantillons par dataset...\")\n",
    "code_buf = [row[\"content\"] for row in ds_code.take(MAX_BUF)]\n",
    "nl_buf   = [row[\"text\"]     for row in ds_nl.take(MAX_BUF)]\n",
    "\n",
    "print(f\"‚úì Buffer de {len(code_buf):,} √©chantillons de code\")\n",
    "print(f\"‚úì Buffer de {len(nl_buf):,} √©chantillons de texte\")\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTIQUES DES DONN√âES\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Calculer le nombre de tokens pour estimer les √©poques d'entra√Ænement et \n",
    "la taille totale du dataset. Cela aide √† d√©terminer si nous avons assez \n",
    "de donn√©es pour 80,000 √©tapes.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä CALCUL DES STATISTIQUES...\")\n",
    "\n",
    "def count_tokens(texts, sample_size=1000):\n",
    "    \"\"\"Compte le nombre total de tokens sur un √©chantillon.\"\"\"\n",
    "    total = 0\n",
    "    sample = texts[:sample_size] if len(texts) > sample_size else texts\n",
    "    for text in sample:\n",
    "        ids = tokenizer(text, truncation=True, max_length=cfg.block_size, return_tensors=\"pt\").input_ids\n",
    "        total += ids.numel()\n",
    "    # Extrapoler pour tout le dataset\n",
    "    return int(total * (len(texts) / len(sample)))\n",
    "\n",
    "code_tokens = count_tokens(code_buf)\n",
    "nl_tokens = count_tokens(nl_buf)\n",
    "total_tokens = code_tokens + nl_tokens\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES DES DONN√âES:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"√âchantillons:\")\n",
    "print(f\"  - Code: {len(code_buf):,} √©chantillons\")\n",
    "print(f\"  - Texte naturel: {len(nl_buf):,} √©chantillons\")\n",
    "print(f\"  - Total: {len(code_buf) + len(nl_buf):,} √©chantillons\")\n",
    "print(f\"\\nTokens (estim√©):\")\n",
    "print(f\"  - Code tokens: {code_tokens:,}\")\n",
    "print(f\"  - NL tokens: {nl_tokens:,}\")\n",
    "print(f\"  - Total tokens: {total_tokens:,}\")\n",
    "\n",
    "# Estimer les √©poques\n",
    "tokens_per_step = cfg.batch_size * (cfg.block_size - 1)  # (B, T-1) pour input\n",
    "total_tokens_training = cfg.max_steps * tokens_per_step\n",
    "num_epochs = total_tokens_training / total_tokens\n",
    "\n",
    "print(f\"\\nEntra√Ænement:\")\n",
    "print(f\"  - Tokens par step: {tokens_per_step:,}\")\n",
    "print(f\"  - Total tokens d'entra√Ænement: {total_tokens_training:,}\")\n",
    "print(f\"  - √âpoques estim√©es: {num_epochs:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODAGE & DATA STREAM\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Pipeline d'encodage:\n",
    "  1. Prendre du texte du buffer\n",
    "  2. Tokeniser avec padding/truncation\n",
    "  3. Clamp les IDs de tokens (s√©curit√©)\n",
    "  4. Cr√©er les paires (x, y) o√π y = x d√©cal√© de 1\n",
    "\n",
    "Le g√©n√©rateur data_stream() s'ex√©cute ind√©finiment et va cycler √† travers\n",
    "le buffer plusieurs fois (cycling sur les √©poques).\n",
    "\"\"\"\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Encode le texte en IDs de tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte brut √† encoder\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: IDs de tokens de forme (block_size,)\n",
    "    \"\"\"\n",
    "    ids = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=cfg.block_size,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    # S√©curit√©: clamp les IDs de tokens dans la plage valide [0, vocab_size-1]\n",
    "    # √âvite les assertions CUDA device-side pendant le lookup d'embedding\n",
    "    ids = torch.clamp(ids, 0, cfg.vocab_size - 1)\n",
    "    return ids\n",
    "\n",
    "def data_stream():\n",
    "    \"\"\"\n",
    "    G√©n√©rateur infini retournant des paires (x, y) d'entra√Ænement.\n",
    "    \n",
    "    Yields:\n",
    "        (x, y): Tensors de forme (T,) o√π:\n",
    "          - x: tokens d'entr√©e [t0, t1, ..., t_{T-2}]\n",
    "          - y: tokens cibles [t1, t2, ..., t_{T-1}] (x d√©cal√© de 1)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # √âchantillonner du code ou du NL avec probabilit√© cfg.p_code\n",
    "        if random.random() < cfg.p_code:\n",
    "            text = random.choice(code_buf)\n",
    "        else:\n",
    "            text = random.choice(nl_buf)\n",
    "            \n",
    "        ids = encode(text)\n",
    "        x = ids[:-1]    # Input: tous sauf le dernier token\n",
    "        y = ids[1:]     # Target: tous sauf le premier token (d√©cal√©)\n",
    "        yield x, y\n",
    "\n",
    "# Cr√©er l'it√©rateur infini sur le data stream\n",
    "train_iter = cycle(data_stream())\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline de donn√©es configur√© et pr√™t\")\n",
    "print(f\"   - M√©lange: {cfg.p_code*100}% code + {(1-cfg.p_code)*100}% texte\")\n",
    "print(f\"   - Streaming infini activ√© (cycling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816df8fb",
   "metadata": {},
   "source": [
    "## üîπ Partie 5 : Chargement et Pr√©paration des Donn√©es\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous chargeons **deux datasets diff√©rents** et les m√©langeons pour cr√©er un corpus d'entra√Ænement mixte :\n",
    "1. **Code Python** (80%) : bigcode/the-stack-smol\n",
    "2. **Texte naturel** (20%) : HuggingFaceTB/smollm-corpus\n",
    "\n",
    "### Pourquoi m√©langer Code + Texte ?\n",
    "\n",
    "| Avantage | Explication |\n",
    "|----------|-------------|\n",
    "| **Polyvalence** | Le mod√®le comprend √† la fois le code ET les explications |\n",
    "| **Contexte riche** | Peut g√©n√©rer du code avec des commentaires naturels |\n",
    "| **Robustesse** | Moins de sur-apprentissage sur un seul type de donn√©es |\n",
    "| **Applications** | Documentation automatique, code generation, tutoriels |\n",
    "\n",
    "### Streaming vs. Buffering : Notre Choix\n",
    "\n",
    "#### Option 1 : Streaming (donn√©es charg√©es √† la vol√©e)\n",
    "- ‚úÖ **√âconome en RAM** : Ne charge que ce qui est n√©cessaire\n",
    "- ‚ùå **Lent** : I/O constant depuis le disque/r√©seau\n",
    "- üéØ **Quand utiliser** : Datasets √©normes (TB), RAM limit√©e\n",
    "\n",
    "#### Option 2 : Buffering (tout en RAM)\n",
    "- ‚úÖ **Tr√®s rapide** : Pas d'I/O pendant l'entra√Ænement (2-3√ó plus rapide)\n",
    "- ‚ùå **Demande de la RAM** : ~3-5GB pour 50K √©chantillons\n",
    "- üéØ **Quand utiliser** : Datasets moyens, RAM suffisante\n",
    "\n",
    "**Notre choix** : **Buffering** avec 50,000 √©chantillons en RAM pour maximiser la vitesse d'entra√Ænement.\n",
    "\n",
    "### Pipeline de Traitement des Donn√©es\n",
    "\n",
    "```\n",
    "1. CHARGEMENT (Streaming)\n",
    "   ‚Üì\n",
    "   bigcode/the-stack-smol + smollm-corpus\n",
    "   ‚Üì\n",
    "2. BUFFERING (En m√©moire)\n",
    "   ‚Üì\n",
    "   50K code + 50K texte ‚Üí RAM\n",
    "   ‚Üì\n",
    "3. M√âLANGE AL√âATOIRE (80/20)\n",
    "   ‚Üì\n",
    "   √âchantillonner code ou texte selon p_code=0.8\n",
    "   ‚Üì\n",
    "4. TOKENISATION\n",
    "   ‚Üì\n",
    "   Texte ‚Üí Tokens (IDs) ‚Üí [451, 3383, 1159, ...]\n",
    "   ‚Üì\n",
    "5. PADDING/TRUNCATION\n",
    "   ‚Üì\n",
    "   Ajuster √† block_size=256 tokens\n",
    "   ‚Üì\n",
    "6. CR√âATION DES PAIRES (x, y)\n",
    "   ‚Üì\n",
    "   Input: [t0, t1, ..., t254]\n",
    "   Target: [t1, t2, ..., t255]\n",
    "   ‚Üì\n",
    "7. BATCH (8 s√©quences)\n",
    "   ‚Üì\n",
    "   Pr√™t pour l'entra√Ænement !\n",
    "```\n",
    "\n",
    "### Objectif : Causal Language Modeling (CLM)\n",
    "\n",
    "Pour chaque s√©quence, nous cr√©ons une paire **input/target** :\n",
    "\n",
    "```python\n",
    "Texte original:    \"def fibonacci(n): return n\"\n",
    "Tokens:           [451, 50276, 7, 78, 2599, 60, 327, 299]\n",
    "\n",
    "Input (x):        [451, 50276, 7, 78, 2599, 60, 327]\n",
    "Target (y):       [50276, 7, 78, 2599, 60, 327, 299]\n",
    "                    ‚Üë\n",
    "                 D√©cal√© de 1 position !\n",
    "\n",
    "Le mod√®le apprend √† pr√©dire :\n",
    "  - Apr√®s [451], pr√©dire 50276\n",
    "  - Apr√®s [451, 50276], pr√©dire 7\n",
    "  - Apr√®s [451, 50276, 7], pr√©dire 78\n",
    "  - etc.\n",
    "```\n",
    "\n",
    "**Loss** : Cross-entropy entre la distribution pr√©dite et la vraie valeur du token suivant.\n",
    "\n",
    "### Statistiques du Dataset\n",
    "\n",
    "Apr√®s le buffering, voici ce que nous aurons :\n",
    "\n",
    "| M√©trique | Valeur Estim√©e |\n",
    "|----------|----------------|\n",
    "| **√âchantillons code** | 50,000 |\n",
    "| **√âchantillons texte** | 50,000 |\n",
    "| **Total √©chantillons** | 100,000 |\n",
    "| **Tokens code** | ~100-120M |\n",
    "| **Tokens texte** | ~50-80M |\n",
    "| **Total tokens** | ~150-200M |\n",
    "| **Tokens par step** | 8 √ó 255 = 2,040 |\n",
    "| **Tokens d'entra√Ænement** | 80,000 √ó 2,040 = 163M |\n",
    "| **√âpoques estim√©es** | ~1 √©poque compl√®te |\n",
    "\n",
    "### Le Data Stream Infini\n",
    "\n",
    "```python\n",
    "def data_stream():\n",
    "    while True:  # ‚Üê Boucle infinie !\n",
    "        # Choisir code ou texte (80/20)\n",
    "        if random.random() < 0.8:\n",
    "            text = random.choice(code_buf)\n",
    "        else:\n",
    "            text = random.choice(nl_buf)\n",
    "        \n",
    "        # Tokeniser et cr√©er (x, y)\n",
    "        ids = encode(text)\n",
    "        x = ids[:-1]  # Input\n",
    "        y = ids[1:]   # Target (d√©cal√©)\n",
    "        yield x, y\n",
    "```\n",
    "\n",
    "Ce g√©n√©rateur s'ex√©cute **ind√©finiment** et cycle √† travers les donn√©es. Parfait pour l'entra√Ænement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "673c333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  CONSTRUCTION DU MOD√àLE TRANSFORMER\n",
      "======================================================================\n",
      "‚úì Mod√®le cr√©√©: TinyDecoderLM\n",
      "‚úì Nombre de param√®tres: 76,811,264 (76.81M)\n",
      "‚úì Device: cuda\n",
      "‚úì Data type: torch.bfloat16\n",
      "\n",
      "D√©tail de l'architecture:\n",
      "  - Couches transformer: 8\n",
      "  - T√™tes d'attention: 8\n",
      "  - Dimension du mod√®le: 512\n",
      "  - Dimension feed-forward: 2048\n",
      "  - Longueur de contexte: 256\n",
      "======================================================================\n",
      "‚úì Mod√®le cr√©√©: TinyDecoderLM\n",
      "‚úì Nombre de param√®tres: 76,811,264 (76.81M)\n",
      "‚úì Device: cuda\n",
      "‚úì Data type: torch.bfloat16\n",
      "\n",
      "D√©tail de l'architecture:\n",
      "  - Couches transformer: 8\n",
      "  - T√™tes d'attention: 8\n",
      "  - Dimension du mod√®le: 512\n",
      "  - Dimension feed-forward: 2048\n",
      "  - Longueur de contexte: 256\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 6: Architecture du Mod√®le Transformer (Decoder-Only)\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMER DECODER ARCHITECTURE\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "TRANSFORMER DECODER ARCHITECTURE:\n",
    "==================================\n",
    "\n",
    "COMPOSANTS PRINCIPAUX:\n",
    "  1. Token Embedding: vocab_size ‚Üí d_model\n",
    "  2. Positional Embedding: block_size ‚Üí d_model\n",
    "  3. Transformer Blocks: N √ó (Attention + FFN)\n",
    "  4. Layer Norm: Normalisation finale\n",
    "  5. Output Head: d_model ‚Üí vocab_size (logits)\n",
    "\n",
    "M√âCANISME D'ATTENTION (Causal Self-Attention):\n",
    "  - Q = x @ W_q, K = x @ W_k, V = x @ W_v\n",
    "  - Attention = softmax(Q @ K^T / ‚àöd_k + mask) @ V\n",
    "  - Masque causal: Emp√™che l'attention vers les tokens futurs\n",
    "  - Multi-t√™tes: 8 t√™tes √ó (512/8) = 8 √ó 64 dimensions\n",
    "  - Permet au mod√®le d'apprendre diff√©rents aspects simultan√©ment\n",
    "\n",
    "FEED-FORWARD NETWORK (FFN):\n",
    "  - Couche 1: d_model ‚Üí d_ff (2048)  [Expansion avec activation GELU]\n",
    "  - Couche 2: d_ff ‚Üí d_model          [Projection retour]\n",
    "  - Objectif: Transformations non-lin√©aires, augmente la capacit√©\n",
    "  - GELU: Activation plus douce que ReLU, meilleure pour les transformers\n",
    "\n",
    "CONNEXIONS R√âSIDUELLES & LAYER NORM:\n",
    "  - Chaque bloc: x ‚Üí x + Attention(LayerNorm(x))\n",
    "  - Puis: x ‚Üí x + FFN(LayerNorm(x))\n",
    "  - B√©n√©fices: √âvite la disparition des gradients, stabilise l'entra√Ænement\n",
    "\n",
    "NOMBRE DE PARAM√àTRES:\n",
    "  - Token Embedding: 50257 √ó 512 ‚âà 25.7M\n",
    "  - Positional Embedding: 256 √ó 512 ‚âà 0.13M\n",
    "  - Attention par bloc: 3√ó(512√ó512) + (512√ó512) ‚âà 1.05M\n",
    "  - FFN par bloc: (512√ó2048) + (2048√ó512) ‚âà 2.1M\n",
    "  - Total par bloc: ~3.15M √ó 8 couches ‚âà 25.2M\n",
    "  - Output head: 512 √ó 50257 ‚âà 25.7M\n",
    "  - TOTAL: ~77M param√®tres\n",
    "\"\"\"\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention multi-t√™tes avec masque causal.\n",
    "    \n",
    "    Emp√™che les tokens de voir les positions futures.\n",
    "    Essentiel pour le causal language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimension d'embedding\n",
    "            n_heads (int): Nombre de t√™tes d'attention\n",
    "            block_size (int): Longueur maximale de s√©quence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model doit √™tre divisible par n_heads\"\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads  # 512 / 8 = 64\n",
    "\n",
    "        # Projection QKV combin√©e (plus rapide que s√©par√©e)\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        # Projection de sortie\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Masque causal: matrice triangulaire inf√©rieure (1s en-dessous, 0s au-dessus)\n",
    "        # Cela emp√™che l'attention vers les positions futures\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input de forme (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output de forme (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Projeter vers Q, K, V (tout √† la fois)\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)  # Chaque: (B, T, d_model)\n",
    "\n",
    "        # Reshape pour attention multi-t√™tes: (B, T, d_model) ‚Üí (B, n_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Calculer les scores d'attention: (B, nh, T, head_dim) @ (B, nh, head_dim, T) \n",
    "        #                                = (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Appliquer le masque causal: mettre -inf pour les positions futures\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        \n",
    "        # Calculer les poids d'attention\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # Somme pond√©r√©e des valeurs\n",
    "        y = att @ v  # (B, nh, T, T) @ (B, nh, T, head_dim) = (B, nh, T, head_dim)\n",
    "        \n",
    "        # Concat√©ner les t√™tes: (B, nh, T, head_dim) ‚Üí (B, T, d_model)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Projection de sortie\n",
    "        return self.proj(y)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloc Transformer avec attention + feed-forward.\n",
    "    \n",
    "    Structure:\n",
    "      x ‚Üí LayerNorm ‚Üí Attention ‚Üí x + Attention(...)\n",
    "      ‚Üì\n",
    "      x ‚Üí LayerNorm ‚Üí FFN ‚Üí x + FFN(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimension cach√©e\n",
    "            n_heads (int): Nombre de t√™tes d'attention\n",
    "            d_ff (int): Dimension feed-forward\n",
    "            block_size (int): Longueur maximale de s√©quence\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),  # Activation plus douce que ReLU\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Forme (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Forme (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Bloc d'attention r√©siduel: x = x + Attention(LayerNorm(x))\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        \n",
    "        # Bloc FFN r√©siduel: x = x + FFN(LayerNorm(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyDecoderLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Mod√®le de Langage Decoder-Only Transformer.\n",
    "    \n",
    "    Prend des IDs de tokens et pr√©dit les logits du prochain token (causal LM).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (Config): Objet de configuration avec les hyperparam√®tres du mod√®le\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # ====================================================================\n",
    "        # EMBEDDINGS\n",
    "        # ====================================================================\n",
    "        # Token embedding: vocab_size ‚Üí d_model\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        \n",
    "        # Position embedding: block_size ‚Üí d_model\n",
    "        # (Encodage positionnel absolu, plus simple et suffisant pour petits mod√®les)\n",
    "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "\n",
    "        # ====================================================================\n",
    "        # TRANSFORMER BLOCKS\n",
    "        # ====================================================================\n",
    "        # Pile de blocs transformer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        # ====================================================================\n",
    "        # COUCHES FINALES\n",
    "        # ====================================================================\n",
    "        # Layer normalization finale avant projection de sortie\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        \n",
    "        # Projection de sortie: d_model ‚Üí vocab_size (logits pour chaque token)\n",
    "        # Poids li√©s (share embeddings avec la couche de sortie) - pratique courante\n",
    "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "        # Initialiser les poids avec de petites valeurs al√©atoires\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Strat√©gie d'initialisation des poids.\n",
    "        \n",
    "        - Couches lin√©aires: Distribution normale (std=0.02)\n",
    "        - Embeddings: Distribution normale (std=0.02)\n",
    "        - Biais: Z√©ros\n",
    "        \n",
    "        Petite initialisation √©vite la saturation des activations.\n",
    "        \"\"\"\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Forward pass: tokens ‚Üí logits du prochain token.\n",
    "        \n",
    "        Args:\n",
    "            idx (torch.Tensor): Indices de tokens, forme (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Logits, forme (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Cr√©er les indices de position [0, 1, 2, ..., T-1]\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "\n",
    "        # Combiner token et positional embeddings\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)  # (B, T, d_model)\n",
    "        \n",
    "        # Appliquer les blocs transformer\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        # Layer normalization finale\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Projeter vers les logits du vocabulaire\n",
    "        return self.head(x)  # (B, T, vocab_size)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANCIATION DU MOD√àLE\n",
    "# ============================================================================\n",
    "print(\"\\nüèóÔ∏è  CONSTRUCTION DU MOD√àLE TRANSFORMER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Instancier le mod√®le avec la configuration\n",
    "model = TinyDecoderLM(cfg).to(cfg.device, memory_format=torch.contiguous_format)\n",
    "model = model.to(dtype=cfg.dtype)\n",
    "\n",
    "# Compter les param√®tres du mod√®le\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úì Mod√®le cr√©√©: TinyDecoderLM\")\n",
    "print(f\"‚úì Nombre de param√®tres: {n_params:,} ({n_params/1e6:.2f}M)\")\n",
    "print(f\"‚úì Device: {cfg.device}\")\n",
    "print(f\"‚úì Data type: {cfg.dtype}\")\n",
    "print(f\"\\nD√©tail de l'architecture:\")\n",
    "print(f\"  - Couches transformer: {cfg.n_layers}\")\n",
    "print(f\"  - T√™tes d'attention: {cfg.n_heads}\")\n",
    "print(f\"  - Dimension du mod√®le: {cfg.d_model}\")\n",
    "print(f\"  - Dimension feed-forward: {cfg.d_ff}\")\n",
    "print(f\"  - Longueur de contexte: {cfg.block_size}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72087097",
   "metadata": {},
   "source": [
    "## üîπ Partie 6 : Architecture du Mod√®le Transformer\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous construisons un **Decoder-Only Transformer** (architecture GPT) enti√®rement from scratch.\n",
    "\n",
    "### Architecture G√©n√©rale : Vue d'ensemble\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         INPUT: Tokens [451, 3383, 1159]     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    TOKEN EMBEDDING (50257 ‚Üí 512)            ‚îÇ\n",
    "‚îÇ              +                              ‚îÇ\n",
    "‚îÇ    POSITION EMBEDDING (0-255 ‚Üí 512)         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       TRANSFORMER BLOCK √ó 8                 ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n",
    "‚îÇ   ‚îÇ  1. Layer Norm                  ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îÇ  2. Causal Self-Attention (8√ó)  ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îÇ  3. Residual Connection         ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îÇ  4. Layer Norm                  ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îÇ  5. Feed-Forward Network        ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îÇ  6. Residual Connection         ‚îÇ      ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         LAYER NORM FINAL                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    OUTPUT PROJECTION (512 ‚Üí 50257)          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   LOGITS: Probabilit√©s pour chaque token   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Les 3 Composants Cl√©s\n",
    "\n",
    "#### 1Ô∏è‚É£ Causal Self-Attention (Attention Causale)\n",
    "\n",
    "**Probl√®me √† r√©soudre** : Chaque token ne doit voir QUE les tokens **avant** lui, jamais le futur !\n",
    "\n",
    "**Visualisation du masque causal** :\n",
    "\n",
    "```\n",
    "Position:     0    1    2    3\n",
    "Input:      \"def\" \"fib\" \"(\"  \"n\"\n",
    "\n",
    "Attention mask (ce que chaque position peut voir):\n",
    "Position 0: [‚úì]  [‚úó]  [‚úó]  [‚úó]  ‚Üí Voit seulement position 0\n",
    "Position 1: [‚úì]  [‚úì]  [‚úó]  [‚úó]  ‚Üí Voit positions 0-1\n",
    "Position 2: [‚úì]  [‚úì]  [‚úì]  [‚úó]  ‚Üí Voit positions 0-2\n",
    "Position 3: [‚úì]  [‚úì]  [‚úì]  [‚úì]  ‚Üí Voit positions 0-3\n",
    "\n",
    "Matrice d'attention (avec masque causal):\n",
    "[  0   -‚àû   -‚àû   -‚àû ]\n",
    "[  x    0   -‚àû   -‚àû ]\n",
    "[  x    x    0   -‚àû ]\n",
    "[  x    x    x    0 ]\n",
    "\n",
    "Apr√®s softmax, -‚àû devient 0 (pas d'attention)\n",
    "```\n",
    "\n",
    "**Multi-t√™tes d'attention** :\n",
    "\n",
    "| Aspect | D√©tail |\n",
    "|--------|--------|\n",
    "| Nombre de t√™tes | 8 |\n",
    "| Dimension par t√™te | 512 / 8 = 64 |\n",
    "| **Pourquoi ?** | Chaque t√™te apprend diff√©rents patterns |\n",
    "| T√™te 1 | Grammaire et syntaxe |\n",
    "| T√™te 2 | Relations s√©mantiques |\n",
    "| T√™te 3 | Structure du code |\n",
    "| T√™te 4-8 | Autres aspects... |\n",
    "\n",
    "**Formule de l'attention** :\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + \\text{mask}\\right)V$$\n",
    "\n",
    "O√π :\n",
    "- $Q$ (Query) : \"Ce que je cherche\"\n",
    "- $K$ (Key) : \"Ce que j'offre\"\n",
    "- $V$ (Value) : \"Mon contenu\"\n",
    "- $\\sqrt{d_k}$ : Normalisation (√©vite explosion des gradients)\n",
    "\n",
    "#### 2Ô∏è‚É£ Feed-Forward Network (R√©seau Neuronal)\n",
    "\n",
    "Apr√®s l'attention, chaque position passe par un petit r√©seau neuronal :\n",
    "\n",
    "```\n",
    "x (512) ‚Üí Linear ‚Üí (2048) ‚Üí GELU ‚Üí Linear ‚Üí (512)\n",
    "         Expansion 4√ó              Contraction\n",
    "```\n",
    "\n",
    "| √âtape | Dimension | R√¥le |\n",
    "|-------|-----------|------|\n",
    "| Input | 512 | √âtat apr√®s attention |\n",
    "| Expansion | 2048 | Apprendre patterns complexes |\n",
    "| GELU | 2048 | Activation non-lin√©aire douce |\n",
    "| Projection | 512 | Retour √† la dimension originale |\n",
    "\n",
    "**GELU vs ReLU** :\n",
    "\n",
    "```python\n",
    "# ReLU: max(0, x) - brutal, angle √† 90¬∞\n",
    "# GELU: plus douce, meilleure pour transformers\n",
    "\n",
    "x = -2  -1   0   1   2\n",
    "ReLU:  0   0   0   1   2  ‚Üê Discontinu\n",
    "GELU:  0  -0.16  0  0.84  2  ‚Üê Continu et doux\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ Connexions R√©siduelles (Residual Connections)\n",
    "\n",
    "**Sans r√©siduel** (‚ùå probl√®me) :\n",
    "```\n",
    "x ‚Üí Attention ‚Üí y\n",
    "Gradient: ‚àÇL/‚àÇx doit traverser Attention (peut dispara√Ætre!)\n",
    "```\n",
    "\n",
    "**Avec r√©siduel** (‚úÖ solution) :\n",
    "```\n",
    "x ‚Üí Attention ‚Üí y\n",
    "  ‚Üò____________‚Üó\n",
    "     x + y\n",
    "\n",
    "Gradient: ‚àÇL/‚àÇx = direct path + Attention path\n",
    "```\n",
    "\n",
    "**Formule** :\n",
    "```python\n",
    "x = x + Attention(LayerNorm(x))  # Connexion r√©siduelle 1\n",
    "x = x + FFN(LayerNorm(x))        # Connexion r√©siduelle 2\n",
    "```\n",
    "\n",
    "**B√©n√©fices** :\n",
    "- ‚úÖ √âvite la disparition des gradients\n",
    "- ‚úÖ Permet des r√©seaux tr√®s profonds (8+ couches)\n",
    "- ‚úÖ Stabilise l'entra√Ænement\n",
    "\n",
    "### D√©compte D√©taill√© des Param√®tres\n",
    "\n",
    "| Composant | Calcul | Param√®tres |\n",
    "|-----------|--------|------------|\n",
    "| **Token Embedding** | 50,257 √ó 512 | 25,731,584 ‚âà **25.7M** |\n",
    "| **Position Embedding** | 256 √ó 512 | 131,072 ‚âà **0.13M** |\n",
    "| **Par Bloc Transformer** | | |\n",
    "| ‚îî Attention QKV | 3 √ó (512 √ó 512) | 786,432 |\n",
    "| ‚îî Attention Proj | 512 √ó 512 | 262,144 |\n",
    "| ‚îî FFN Layer 1 | 512 √ó 2048 | 1,048,576 |\n",
    "| ‚îî FFN Layer 2 | 2048 √ó 512 | 1,048,576 |\n",
    "| ‚îî Layer Norms (2√ó) | ~2048 | ~2,048 |\n",
    "| ‚îî **Total par bloc** | | **~3.15M** |\n",
    "| **8 Blocs** | 3.15M √ó 8 | **25.2M** |\n",
    "| **Output Head** | 512 √ó 50,257 | 25,731,584 ‚âà **25.7M** |\n",
    "| **TOTAL MOD√àLE** | | **~77M param√®tres** |\n",
    "\n",
    "### Taille du Mod√®le en M√©moire\n",
    "\n",
    "```\n",
    "77M param√®tres √ó 2 bytes (float16) = 154 MB\n",
    "77M param√®tres √ó 4 bytes (float32) = 308 MB\n",
    "\n",
    "+ Activations pendant training ‚âà 2-3√ó plus\n",
    "Total en training: ~500MB - 1GB GPU RAM\n",
    "```\n",
    "\n",
    "### Pourquoi \"Tiny\" ?\n",
    "\n",
    "| Mod√®le | Param√®tres | Comparaison |\n",
    "|--------|-----------|-------------|\n",
    "| **Notre TinyDecoderLM** | 77M | üê£ Petit |\n",
    "| GPT-2 Small | 117M | 1.5√ó plus grand |\n",
    "| GPT-2 Medium | 345M | 4.5√ó plus grand |\n",
    "| GPT-2 Large | 774M | 10√ó plus grand |\n",
    "| GPT-3 | 175B | 2,270√ó plus grand ! |\n",
    "\n",
    "Notre mod√®le est **\"tiny\"** mais suffisant pour apprendre du code Python ! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae643e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  CONFIGURATION DE L'OPTIMISEUR\n",
      "======================================================================\n",
      "‚úì Optimiseur: AdamW\n",
      "‚úì Learning rate max: 0.0003\n",
      "‚úì Weight decay: 0.1\n",
      "‚úì Betas (momentum): (0.9, 0.95)\n",
      "\n",
      "Groupes de param√®tres:\n",
      "  - Avec weight decay (0.1): 76,756,992 param√®tres\n",
      "    ‚Üí Poids des couches Linear (matrices 2D)\n",
      "  - Sans weight decay (0.0): 54,272 param√®tres\n",
      "    ‚Üí Biais, Layer Norms, Embeddings\n",
      "\n",
      "Total param√®tres optimis√©s: 76,811,264\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 7: Configuration de l'Optimiseur (AdamW avec Weight Decay)\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZER: AdamW with Decoupled Weight Decay\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "OPTIMIZATION STRATEGY:\n",
    "======================\n",
    "\n",
    "OPTIMIZER: AdamW (Adam avec weight decay d√©coupl√©)\n",
    "  - Momentum: 0.9 (moyenne mobile exponentielle des gradients)\n",
    "  - Variance momentum: 0.95 (moyenne mobile exp. des gradients au carr√©)\n",
    "  - Weight decay: D√©coupl√© (pas appliqu√© au terme de mise √† jour, appliqu√© directement)\n",
    "  - Epsilon: 1e-8 (stabilit√© num√©rique)\n",
    "  - Fused (si disponible): Combine les op√©rations pour speedup CUDA\n",
    "\n",
    "STRAT√âGIE DE WEIGHT DECAY:\n",
    "  - Decay sur: Poids des couches lin√©aires (r√©gularisation L2)\n",
    "    - Encourage des poids plus petits, √©vite le sur-apprentissage\n",
    "    - Appliqu√© aux param√®tres avec ndim >= 2 et pas \"bias\" dans le nom\n",
    "    \n",
    "  - Pas de decay sur: Biais et poids de layer norm\n",
    "    - Trop peu de param√®tres pour r√©gulariser\n",
    "    - Les tokens sp√©ciaux/embeddings b√©n√©ficient souvent de l'absence de decay\n",
    "\n",
    "PARAM√àTRES GROUP√âS:\n",
    "  - Groupe 1: Weight decay = 0.1 (poids lin√©aires)\n",
    "  - Groupe 2: Weight decay = 0.0 (biais, layer norms, embeddings)\n",
    "  \n",
    "Cela √©vite une r√©gularisation inutile des param√®tres d'√©chelle.\n",
    "\n",
    "LEARNING RATES ADAPTATIFS:\n",
    "  - Diff√©rents param√®tres obtiennent diff√©rents learning rates effectifs\n",
    "  - Aide la convergence √† travers les types de param√®tres divers\n",
    "\"\"\"\n",
    "\n",
    "def create_optimizer(model):\n",
    "    \"\"\"\n",
    "    Cr√©er l'optimiseur AdamW avec weight decay group√© par param√®tres.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Mod√®le √† optimiser\n",
    "        \n",
    "    Returns:\n",
    "        torch.optim.AdamW: Optimiseur avec groupes de param√®tres\n",
    "    \"\"\"\n",
    "    decay = []      # Param√®tres avec weight decay\n",
    "    no_decay = []   # Param√®tres sans weight decay\n",
    "    \n",
    "    # Classifier les param√®tres\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        \n",
    "        # Weight decay pour param√®tres 2D+ sans \"bias\" dans le nom\n",
    "        # (typiquement les poids dans les couches Linear)\n",
    "        if p.ndim >= 2 and \"bias\" not in name:\n",
    "            decay.append(p)\n",
    "        # Pas de weight decay pour param√®tres 1D (biais, layer norm scales)\n",
    "        # et tout param√®tre avec \"bias\" dans le nom\n",
    "        else:\n",
    "            no_decay.append(p)\n",
    "\n",
    "    # Groupes de param√®tres avec diff√©rents param√®tres de weight decay\n",
    "    groups = [\n",
    "        {\"params\": decay, \"weight_decay\": cfg.weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    # V√©rifier si fused AdamW est disponible (speedup sp√©cifique CUDA)\n",
    "    fused = (\"fused\" in torch.optim.AdamW.__init__.__code__.co_varnames)\n",
    "\n",
    "    return torch.optim.AdamW(\n",
    "        groups,\n",
    "        lr=cfg.lr_max,\n",
    "        betas=(0.9, 0.95),          # Coefficients de momentum\n",
    "        eps=1e-8,                    # Stabilit√© num√©rique\n",
    "        fused=fused,                 # Utiliser kernels fusionn√©s si disponible\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âATION DE L'OPTIMISEUR\n",
    "# ============================================================================\n",
    "print(\"\\n‚öôÔ∏è  CONFIGURATION DE L'OPTIMISEUR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimizer = create_optimizer(model)\n",
    "\n",
    "# Compter les param√®tres dans chaque groupe\n",
    "n_decay = sum(p.numel() for p in optimizer.param_groups[0][\"params\"])\n",
    "n_no_decay = sum(p.numel() for p in optimizer.param_groups[1][\"params\"])\n",
    "\n",
    "print(f\"‚úì Optimiseur: AdamW\")\n",
    "print(f\"‚úì Learning rate max: {cfg.lr_max}\")\n",
    "print(f\"‚úì Weight decay: {cfg.weight_decay}\")\n",
    "print(f\"‚úì Betas (momentum): (0.9, 0.95)\")\n",
    "print(f\"\\nGroupes de param√®tres:\")\n",
    "print(f\"  - Avec weight decay (0.1): {n_decay:,} param√®tres\")\n",
    "print(f\"    ‚Üí Poids des couches Linear (matrices 2D)\")\n",
    "print(f\"  - Sans weight decay (0.0): {n_no_decay:,} param√®tres\")\n",
    "print(f\"    ‚Üí Biais, Layer Norms, Embeddings\")\n",
    "print(f\"\\nTotal param√®tres optimis√©s: {n_decay + n_no_decay:,}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f6a20",
   "metadata": {},
   "source": [
    "## üîπ Partie 7 : Optimiseur AdamW\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous cr√©ons l'**optimiseur** qui met √† jour les poids du mod√®le pendant l'entra√Ænement. C'est le moteur de l'apprentissage !\n",
    "\n",
    "### Rappel : Gradient Descent Basique\n",
    "\n",
    "L'id√©e de base de l'optimisation :\n",
    "\n",
    "```python\n",
    "# Version simple (Gradient Descent)\n",
    "poids_nouveau = poids_ancien - learning_rate √ó gradient\n",
    "```\n",
    "\n",
    "Mais cette approche **ne marche pas bien** avec les r√©seaux profonds ! üî¥\n",
    "\n",
    "### √âvolution des Optimiseurs\n",
    "\n",
    "| Optimiseur | Caract√©ristiques | Performance |\n",
    "|------------|------------------|-------------|\n",
    "| **SGD** | Basique, pas de momentum | ‚ùå Lent, instable |\n",
    "| **SGD + Momentum** | Accumule les gradients | ‚ö†Ô∏è Mieux mais insuffisant |\n",
    "| **Adam** | Momentum + variance adaptative | ‚úÖ Bon mais couplage probl√©matique |\n",
    "| **AdamW** | Adam + weight decay d√©coupl√© | ‚úÖ‚úÖ Meilleur pour transformers ! |\n",
    "\n",
    "### Adam vs. AdamW : La Diff√©rence Cruciale\n",
    "\n",
    "| Aspect | Adam | AdamW |\n",
    "|--------|------|-------|\n",
    "| **Momentum** | ‚úÖ Oui | ‚úÖ Oui |\n",
    "| **Variance adaptative** | ‚úÖ Oui | ‚úÖ Oui |\n",
    "| **Weight decay** | ‚ùå Coupl√© avec LR | ‚úÖ D√©coupl√© |\n",
    "| **Pour Transformers** | ‚ö†Ô∏è Moyen | ‚úÖ Excellent |\n",
    "\n",
    "**Pourquoi d√©coupl√© est meilleur ?**\n",
    "\n",
    "```python\n",
    "# Adam (coupl√© - probl√©matique)\n",
    "gradient_with_decay = gradient + Œª √ó weight\n",
    "update = lr √ó momentum(gradient_with_decay)\n",
    "# ‚ùå Le decay d√©pend du LR !\n",
    "\n",
    "# AdamW (d√©coupl√© - meilleur)\n",
    "update = lr √ó momentum(gradient)\n",
    "weight = weight - Œª √ó weight  # Decay direct\n",
    "# ‚úÖ Le decay est ind√©pendant du LR !\n",
    "```\n",
    "\n",
    "### Param√®tres d'AdamW\n",
    "\n",
    "```python\n",
    "optimizer = AdamW(\n",
    "    params,\n",
    "    lr=3e-4,           # Learning rate\n",
    "    betas=(0.9, 0.95), # Momentum coefficients\n",
    "    eps=1e-8,          # Stabilit√© num√©rique\n",
    "    weight_decay=0.1   # R√©gularisation L2\n",
    ")\n",
    "```\n",
    "\n",
    "**Signification des betas** :\n",
    "\n",
    "- **Œ≤‚ÇÅ = 0.9** : Moyenne mobile des gradients (momentum)\n",
    "  - Lisse les oscillations\n",
    "  - 90% de l'historique, 10% du gradient actuel\n",
    "  \n",
    "- **Œ≤‚ÇÇ = 0.95** : Moyenne mobile des gradients au carr√© (variance)\n",
    "  - Adapte le LR pour chaque param√®tre\n",
    "  - Plus grande variance ‚Üí LR plus petit\n",
    "\n",
    "**Formule compl√®te d'Adam** :\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} m_t$$\n",
    "\n",
    "O√π :\n",
    "- $m_t$ = momentum (moyenne des gradients)\n",
    "- $v_t$ = variance (moyenne des gradients¬≤)\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\epsilon$ = stabilit√© (1e-8)\n",
    "\n",
    "### Grouped Parameters : R√©gularisation Intelligente\n",
    "\n",
    "Tous les poids ne doivent **PAS** √™tre r√©gularis√©s de la m√™me fa√ßon !\n",
    "\n",
    "#### Groupe 1Ô∏è‚É£ : Avec Weight Decay (Œª = 0.1)\n",
    "\n",
    "```python\n",
    "# Param√®tres concern√©s:\n",
    "- Poids des couches Linear (matrices 2D)\n",
    "- self.qkv.weight  (512 √ó 1536)\n",
    "- self.proj.weight (512 √ó 512)\n",
    "- self.ff[0].weight (512 √ó 2048)\n",
    "# etc.\n",
    "\n",
    "# Pourquoi?\n",
    "‚úÖ Beaucoup de param√®tres ‚Üí risque de sur-apprentissage\n",
    "‚úÖ Regularisation L2 garde les poids petits\n",
    "‚úÖ Meilleure g√©n√©ralisation\n",
    "```\n",
    "\n",
    "#### Groupe 2Ô∏è‚É£ : Sans Weight Decay (Œª = 0.0)\n",
    "\n",
    "```python\n",
    "# Param√®tres concern√©s:\n",
    "- Biais (vecteurs 1D) : self.qkv.bias\n",
    "- Layer Norm scales : self.ln1.weight\n",
    "- Layer Norm shifts : self.ln1.bias\n",
    "- Embeddings : self.tok_emb.weight\n",
    "\n",
    "# Pourquoi?\n",
    "‚úÖ Tr√®s peu de param√®tres (pas de risque)\n",
    "‚úÖ R√¥le sp√©cial (scale/shift, pas transformation)\n",
    "‚úÖ Regulariser nuirait √† la performance\n",
    "```\n",
    "\n",
    "### Visualisation de l'Impact du Weight Decay\n",
    "\n",
    "```\n",
    "Sans weight decay (Œª=0):\n",
    "Poids au fil du temps: [0.5, 1.2, 2.8, 5.1, ...]\n",
    "                       ‚Üë Peut exploser! üí•\n",
    "\n",
    "Avec weight decay (Œª=0.1):\n",
    "Poids au fil du temps: [0.5, 0.8, 0.9, 0.85, ...]\n",
    "                       ‚Üë Reste contr√¥l√© ‚úÖ\n",
    "```\n",
    "\n",
    "### Fused Kernels : Optimisation GPU\n",
    "\n",
    "```python\n",
    "fused = True  # Si disponible sur CUDA\n",
    "```\n",
    "\n",
    "**Sans fused** :\n",
    "```\n",
    "1. Lire param√®tres (GPU ‚Üí registres)\n",
    "2. Calculer momentum\n",
    "3. √âcrire r√©sultat (registres ‚Üí GPU)\n",
    "4. Relire param√®tres\n",
    "5. Calculer variance\n",
    "6. √âcrire r√©sultat\n",
    "7. Relire param√®tres\n",
    "8. Mettre √† jour poids\n",
    "9. √âcrire r√©sultat\n",
    "‚Üí 9 op√©rations m√©moire üêå\n",
    "```\n",
    "\n",
    "**Avec fused** :\n",
    "```\n",
    "1. Lire param√®tres une fois\n",
    "2. Tout calculer en une passe\n",
    "3. √âcrire r√©sultat une fois\n",
    "‚Üí 3 op√©rations m√©moire ‚ö° (3√ó plus rapide!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 80000/80000 [1:54:01<00:00, 11.69step/s, epoch=5.87, loss=1.7743, ppl=5.90, lr=1.00e-05]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 80000/80000 [1:54:01<00:00, 11.69step/s, epoch=5.87, loss=1.7743, ppl=5.90, lr=1.00e-05]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ Training finished.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. TRAINING LOOP - Main Training Procedure\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "TRAINING PROCESS:\n",
    "=================\n",
    "\n",
    "OVERVIEW:\n",
    "  - 80,000 training steps in total\n",
    "  - Each step: process batch_size=8 sequences of length 256\n",
    "  - Tokens per step: 8 * (256-1) = 2,040 tokens\n",
    "  - Total training: ~163M tokens (~1 epoch over dataset)\n",
    "\n",
    "MAIN LOOP PHASES:\n",
    "\n",
    "  1. LEARNING RATE SCHEDULING\n",
    "     - Retrieve current LR from cosine schedule\n",
    "     - Update optimizer parameter groups\n",
    "     \n",
    "  2. DATA PREPARATION\n",
    "     - Sample batch from data stream (mixed code + NL)\n",
    "     - Transfer to device (GPU/CPU)\n",
    "     - Sanity checks on first few steps\n",
    "     \n",
    "  3. FORWARD PASS\n",
    "     - x: input tokens (B, T)\n",
    "     - logits = model(x) ‚Üí (B, T, vocab_size)\n",
    "     - Mixed precision (float16 or bfloat16) to save memory\n",
    "     \n",
    "  4. LOSS COMPUTATION\n",
    "     - cross_entropy: measures difference between predicted & true distribution\n",
    "     - Target: y[i] = x[i+1] (next token prediction)\n",
    "     - ignore_index: masks out padding tokens\n",
    "     \n",
    "  5. BACKWARD PASS\n",
    "     - Compute gradients: loss.backward()\n",
    "     - Scale loss (GradScaler for float16 stability)\n",
    "     \n",
    "  6. OPTIMIZATION STEP\n",
    "     - Update weights: optimizer.step()\n",
    "     - Scale recovery (GradScaler)\n",
    "     - Clear gradients for next iteration\n",
    "     \n",
    "  7. LOGGING & EVALUATION\n",
    "     - Every 100 steps: log training loss & perplexity\n",
    "     - Every 2000 steps: evaluate on validation set, save checkpoint\n",
    "     \n",
    "NUMERICAL STABILITY:\n",
    "\n",
    "  - GradScaler: Prevents float16 gradient underflow\n",
    "    * Scales loss: loss * 2^15 to use full float16 range\n",
    "    * Unscales before optimizer step\n",
    "    * Skips step if NaNs detected\n",
    "    \n",
    "  - Mixed Precision: float16 computations + float32 accumulation\n",
    "    * Reduces memory usage by 50%\n",
    "    * Faster on tensor cores\n",
    "    * Requires careful handling\n",
    "    \n",
    "METRICS:\n",
    "\n",
    "  - Loss: Cross-entropy loss (lower = better)\n",
    "  - Perplexity: exp(loss)\n",
    "    * How many equally likely tokens does model think there are?\n",
    "    * ~50257 (random) ‚Üí ~10-20 (trained model)\n",
    "    * Better model = lower perplexity\n",
    "    \n",
    "  - Learning Rate: Cosine schedule from 3e-4 to 1e-5\n",
    "  - Epochs: How many times dataset is cycled through\n",
    "\"\"\"\n",
    "\n",
    "def get_batch():\n",
    "    \"\"\"\n",
    "    Fetch a training batch from data stream.\n",
    "    \n",
    "    Returns:\n",
    "        (x, y): Input tokens (B, T) and target tokens (B, T)\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for _ in range(cfg.batch_size):\n",
    "        x, y = next(train_iter)        # x, y: (T,) = (255,)\n",
    "        xs.append(x.unsqueeze(0))      # (1, T)\n",
    "        ys.append(y.unsqueeze(0))\n",
    "    x = torch.cat(xs, dim=0)          # (B, T) = (8, 255)\n",
    "    y = torch.cat(ys, dim=0)          # (B, T) = (8, 255)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def debug_check_batch(x, y, step, context=\"train\"):\n",
    "    \"\"\"\n",
    "    CPU-side sanity checks to catch issues that cause CUDA device-side asserts.\n",
    "    \n",
    "    Checks:\n",
    "      - Tensor shapes are correct\n",
    "      - Token IDs are in valid range [0, vocab_size)\n",
    "      - No NaN or Inf values\n",
    "    \n",
    "    Only runs for first 5 steps to minimize overhead.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tokens\n",
    "        y (torch.Tensor): Target tokens\n",
    "        step (int): Current training step\n",
    "        context (str): \"train\" or \"eval\" for logging\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If any check fails\n",
    "    \"\"\"\n",
    "    x_cpu = x.detach().cpu()\n",
    "    y_cpu = y.detach().cpu()\n",
    "\n",
    "    # Check shapes\n",
    "    if x_cpu.ndim != 2 or y_cpu.ndim != 2:\n",
    "        raise ValueError(\n",
    "            f\"[{context}] step {step}: expected (B, T) tensors, \"\n",
    "            f\"got x.shape={tuple(x_cpu.shape)}, y.shape={tuple(y_cpu.shape)}\"\n",
    "        )\n",
    "\n",
    "    # Check token ID ranges\n",
    "    vmax_x = int(x_cpu.max().item())\n",
    "    vmin_x = int(x_cpu.min().item())\n",
    "    vmax_y = int(y_cpu.max().item())\n",
    "    vmin_y = int(y_cpu.min().item())\n",
    "\n",
    "    if vmin_x < 0 or vmin_y < 0 or vmax_x >= cfg.vocab_size or vmax_y >= cfg.vocab_size:\n",
    "        raise ValueError(\n",
    "            f\"[{context}] step {step}: token IDs out of range!\\n\"\n",
    "            f\"  x.min={vmin_x}, x.max={vmax_x}, \"\n",
    "            f\"  y.min={vmin_y}, y.max={vmax_y}, \"\n",
    "            f\"  cfg.vocab_size={cfg.vocab_size}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Gradient scaler for float16 training (prevents underflow)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(cfg.dtype == torch.float16))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "t0 = time.time()\n",
    "running_loss = 0.0\n",
    "\n",
    "# Calculate training metadata\n",
    "tokens_per_step = cfg.batch_size * (cfg.block_size - 1)\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(range(1, cfg.max_steps + 1), desc=\"Training\", unit=\"step\", \n",
    "            ncols=120, colour=\"green\", position=0, leave=True)\n",
    "\n",
    "# ========== MAIN TRAINING LOOP ==========\n",
    "for step in pbar:\n",
    "    \n",
    "    # Calculate which epoch we're in (for logging)\n",
    "    current_epoch = (step * tokens_per_step) / total_tokens\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # LR SCHEDULE\n",
    "    # --------------------------------------------------------\n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # BATCH PREPARATION\n",
    "    # --------------------------------------------------------\n",
    "    x, y = get_batch()            # (B, T), integer token IDs\n",
    "    x = x.to(cfg.device)\n",
    "    y = y.to(cfg.device)\n",
    "\n",
    "    # Sanity checks on first few steps (CPU-side)\n",
    "    # These catch issues before they cause cryptic CUDA errors\n",
    "    if step <= 5:\n",
    "        debug_check_batch(x, y, step, context=\"train\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # FORWARD PASS\n",
    "    # --------------------------------------------------------\n",
    "    # Mixed precision (float16): reduces memory, speeds up training\n",
    "    with torch.cuda.amp.autocast(enabled=(cfg.dtype == torch.float16)):\n",
    "        logits = model(x)         # (B, T, vocab_size)\n",
    "        \n",
    "        if step <= 5:\n",
    "            # Shape validation on first steps\n",
    "            if logits.ndim != 3 or logits.size(-1) != cfg.vocab_size:\n",
    "                raise ValueError(\n",
    "                    f\"[train] step {step}: logits shape invalid. \"\n",
    "                    f\"Expected (B, T, {cfg.vocab_size}), got {tuple(logits.shape)}\"\n",
    "                )\n",
    "\n",
    "        # Compute loss\n",
    "        # Reshape to (B*T, vocab_size) and (B*T,) for cross_entropy\n",
    "        # ignore_index: don't penalize padding tokens\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1),\n",
    "            ignore_index=pad_token_id,\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # BACKWARD PASS\n",
    "    # --------------------------------------------------------\n",
    "    # Scale loss for float16 stability (prevents gradient underflow)\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Update weights (with automatic unscaling)\n",
    "    scaler.step(optimizer)\n",
    "    \n",
    "    # Update scaler for next iteration\n",
    "    scaler.update()\n",
    "    \n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # LOGGING & METRICS\n",
    "    # --------------------------------------------------------\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if step % cfg.log_interval == 0:\n",
    "        avg = running_loss / cfg.log_interval\n",
    "        ppl = math.exp(avg) if avg < 20 else float(\"inf\")  # Perplexity\n",
    "        elapsed = time.time() - t0\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        pbar.set_postfix({\n",
    "            \"epoch\": f\"{current_epoch:.2f}\", \n",
    "            \"loss\": f\"{avg:.4f}\", \n",
    "            \"ppl\": f\"{ppl:.2f}\", \n",
    "            \"lr\": f\"{lr:.2e}\"\n",
    "        })\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # EVALUATION & CHECKPOINTING\n",
    "    # --------------------------------------------------------\n",
    "    if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "        model.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "        eval_losses = []\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for speed\n",
    "            # Evaluate on 32 batches\n",
    "            for _ in range(32):\n",
    "                x_eval, y_eval = get_batch()\n",
    "                x_eval = x_eval.to(cfg.device)\n",
    "                y_eval = y_eval.to(cfg.device)\n",
    "\n",
    "                if step <= 5:\n",
    "                    debug_check_batch(x_eval, y_eval, step, context=\"eval\")\n",
    "\n",
    "                logits_eval = model(x_eval)\n",
    "                \n",
    "                if step <= 5:\n",
    "                    if logits_eval.ndim != 3 or logits_eval.size(-1) != cfg.vocab_size:\n",
    "                        raise ValueError(\n",
    "                            f\"[eval] step {step}: logits shape invalid. \"\n",
    "                            f\"Expected (B, T, {cfg.vocab_size}), got {tuple(logits_eval.shape)}\"\n",
    "                        )\n",
    "\n",
    "                eval_loss = F.cross_entropy(\n",
    "                    logits_eval.view(-1, logits_eval.size(-1)),\n",
    "                    y_eval.view(-1),\n",
    "                    ignore_index=pad_token_id,\n",
    "                )\n",
    "                eval_losses.append(eval_loss.item())\n",
    "\n",
    "        # Compute average eval metrics\n",
    "        eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "        eval_ppl = math.exp(eval_loss) if eval_loss < 20 else float(\"inf\")\n",
    "\n",
    "        # Update progress bar description\n",
    "        pbar.set_description(f\"Training [Epoch: {current_epoch:.2f} | eval loss: {eval_loss:.4f} | eval ppl: {eval_ppl:.2f}]\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"config\": cfg.__dict__,\n",
    "                \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n",
    "                \"step\": step,\n",
    "            },\n",
    "            f\"checkpoint_step{step}.pt\",\n",
    "        )\n",
    "\n",
    "        model.train()  # Set back to training mode\n",
    "        pbar.set_description(\"Training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Training finished.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b8e4d",
   "metadata": {},
   "source": [
    "## üîπ Partie 8 : Boucle d'Entra√Ænement Principale (LE C≈íUR ‚ù§Ô∏è)\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "C'est **LE C≈íUR** du notebook ! Nous entra√Ænons le mod√®le sur **80,000 √©tapes** en it√©rant sur les donn√©es, calculant les gradients, et mettant √† jour les poids.\n",
    "\n",
    "### Vue d'ensemble : Les 7 Phases de Chaque √âtape\n",
    "\n",
    "```\n",
    "POUR chaque step de 1 √† 80,000:\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 1Ô∏è‚É£ LEARNING RATE SCHEDULE               ‚îÇ\n",
    "    ‚îÇ    lr = get_lr(step)                    ‚îÇ\n",
    "    ‚îÇ    Ajuster le LR selon cosine schedule  ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 2Ô∏è‚É£ PR√âPARATION DU BATCH                 ‚îÇ\n",
    "    ‚îÇ    x, y = get_batch()                   ‚îÇ\n",
    "    ‚îÇ    √âchantillonner 8 s√©quences (code/NL) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 3Ô∏è‚É£ FORWARD PASS (mixed precision)       ‚îÇ\n",
    "    ‚îÇ    logits = model(x)                    ‚îÇ\n",
    "    ‚îÇ    Pr√©dire les prochains tokens         ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 4Ô∏è‚É£ CALCUL DE LA LOSS                    ‚îÇ\n",
    "    ‚îÇ    loss = cross_entropy(logits, y)      ‚îÇ\n",
    "    ‚îÇ    Mesurer l'erreur de pr√©diction       ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 5Ô∏è‚É£ BACKWARD PASS                        ‚îÇ\n",
    "    ‚îÇ    loss.backward()                      ‚îÇ\n",
    "    ‚îÇ    Calculer les gradients               ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 6Ô∏è‚É£ OPTIMISATION (Mise √† jour poids)     ‚îÇ\n",
    "    ‚îÇ    optimizer.step()                     ‚îÇ\n",
    "    ‚îÇ    Appliquer les gradients              ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                    ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 7Ô∏è‚É£ LOGGING & √âVALUATION                 ‚îÇ\n",
    "    ‚îÇ    Tous les 100 steps: log loss/ppl     ‚îÇ\n",
    "    ‚îÇ    Tous les 2000 steps: eval + save     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Phase 1Ô∏è‚É£ : Learning Rate Schedule\n",
    "\n",
    "```python\n",
    "lr = get_lr(step)  # Obtenir le LR pour cette √©tape\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr  # Mettre √† jour tous les groupes\n",
    "```\n",
    "\n",
    "Le LR change **√† chaque √©tape** selon notre schedule cosine + warmup.\n",
    "\n",
    "### Phase 2Ô∏è‚É£ : Pr√©paration du Batch\n",
    "\n",
    "```python\n",
    "def get_batch():\n",
    "    xs, ys = [], []\n",
    "    for _ in range(cfg.batch_size):  # 8 s√©quences\n",
    "        x, y = next(train_iter)  # Obtenir (input, target)\n",
    "        xs.append(x.unsqueeze(0))\n",
    "        ys.append(y.unsqueeze(0))\n",
    "    return torch.cat(xs, dim=0), torch.cat(ys, dim=0)\n",
    "```\n",
    "\n",
    "**R√©sultat** :\n",
    "- `x` : (8, 255) = 8 s√©quences de 255 tokens (input)\n",
    "- `y` : (8, 255) = 8 s√©quences de 255 tokens (target, d√©cal√© de 1)\n",
    "\n",
    "### Phase 3Ô∏è‚É£ : Forward Pass (Mixed Precision)\n",
    "\n",
    "```python\n",
    "with torch.cuda.amp.autocast(enabled=(cfg.dtype == torch.float16)):\n",
    "    logits = model(x)  # (8, 255, 50257)\n",
    "```\n",
    "\n",
    "**Mixed Precision** :\n",
    "- Calculs en **float16** (rapide, √©conome en RAM)\n",
    "- Accumulation en **float32** (pr√©cis, stable)\n",
    "- **B√©n√©fices** : 2√ó plus rapide, 2√ó moins de RAM !\n",
    "\n",
    "**Dimensions** :\n",
    "```\n",
    "x:      (batch=8, seq=255)\n",
    "        ‚Üì\n",
    "logits: (batch=8, seq=255, vocab=50257)\n",
    "        ‚Üë\n",
    "Pour chaque position, probabilit√© de chaque token du vocab\n",
    "```\n",
    "\n",
    "### Phase 4Ô∏è‚É£ : Calcul de la Loss (Cross-Entropy)\n",
    "\n",
    "```python\n",
    "loss = F.cross_entropy(\n",
    "    logits.view(-1, cfg.vocab_size),  # (8√ó255, 50257)\n",
    "    y.view(-1),                       # (8√ó255,)\n",
    "    ignore_index=pad_token_id\n",
    ")\n",
    "```\n",
    "\n",
    "**Cross-Entropy** : Mesure √† quel point les pr√©dictions sont √©loign√©es de la v√©rit√©\n",
    "\n",
    "```\n",
    "Pour chaque position:\n",
    "  Vrai token: y[i] = 3383\n",
    "  Pr√©diction: logits[i] = [0.01, 0.02, ..., 0.95, ..., 0.001]\n",
    "                                          ‚Üë\n",
    "                                    Position 3383\n",
    "  Loss: -log(probabilit√© du vrai token)\n",
    "  \n",
    "Si probabilit√© = 0.95 ‚Üí loss = -log(0.95) ‚âà 0.05 (excellent ‚úÖ)\n",
    "Si probabilit√© = 0.01 ‚Üí loss = -log(0.01) ‚âà 4.6  (mauvais ‚ùå)\n",
    "```\n",
    "\n",
    "**Ignore padding** : `ignore_index=pad_token_id`\n",
    "- Les tokens de padding ne comptent pas dans la loss\n",
    "- Le mod√®le ne perd pas de temps √† les \"apprendre\"\n",
    "\n",
    "### Phase 5Ô∏è‚É£ : Backward Pass (Calcul des Gradients)\n",
    "\n",
    "```python\n",
    "scaler.scale(loss).backward()\n",
    "```\n",
    "\n",
    "**GradScaler** (pour float16 seulement) :\n",
    "\n",
    "1. **Scale** : Multiplie loss par 2^15 (65536)\n",
    "   - Utilise toute la plage de float16\n",
    "   - √âvite l'underflow (gradients trop petits ‚Üí 0)\n",
    "\n",
    "2. **Backward** : Calcule les gradients\n",
    "\n",
    "3. **Unscale** : Divise les gradients par 2^15\n",
    "   - Restaure la magnitude originale\n",
    "\n",
    "**Exemple** :\n",
    "```\n",
    "Gradient original: 1e-7 (trop petit pour float16!)\n",
    "Apr√®s scale:       1e-7 √ó 65536 = 0.00655 (OK en float16 ‚úÖ)\n",
    "Backward compute...\n",
    "Apr√®s unscale:     gradient / 65536 = 1e-7 (pr√©cis!)\n",
    "```\n",
    "\n",
    "### Phase 6Ô∏è‚É£ : Optimization Step (Mise √† Jour des Poids)\n",
    "\n",
    "```python\n",
    "scaler.step(optimizer)  # Met √† jour les poids\n",
    "scaler.update()         # Ajuste le scale pour next step\n",
    "optimizer.zero_grad(set_to_none=True)  # Clear gradients\n",
    "```\n",
    "\n",
    "**Ce qui se passe** :\n",
    "\n",
    "```python\n",
    "# Pour chaque param√®tre:\n",
    "for param in model.parameters():\n",
    "    # AdamW calcule:\n",
    "    momentum = 0.9 * momentum_old + 0.1 * gradient\n",
    "    variance = 0.95 * variance_old + 0.05 * gradient¬≤\n",
    "    \n",
    "    update = lr * momentum / (‚àövariance + 1e-8)\n",
    "    param = param - update - weight_decay * param\n",
    "    #               ‚Üë          ‚Üë\n",
    "    #          Adam update   Weight decay (d√©coupl√©)\n",
    "```\n",
    "\n",
    "### Phase 7Ô∏è‚É£ : Logging & √âvaluation\n",
    "\n",
    "#### Tous les 100 steps : Log M√©triques\n",
    "\n",
    "```python\n",
    "avg_loss = running_loss / 100\n",
    "perplexity = exp(avg_loss)\n",
    "\n",
    "print(f\"Step {step}: loss={avg_loss:.4f}, ppl={perplexity:.2f}\")\n",
    "```\n",
    "\n",
    "**Perplexit√©** : M√©trique plus intuitive que la loss\n",
    "\n",
    "```\n",
    "Perplexity = exp(loss)\n",
    "\n",
    "Interpr√©tation:\n",
    "  ppl = 50,257 ‚Üí Mod√®le al√©atoire (choix uniforme)\n",
    "  ppl = 100    ‚Üí Incertain entre ~100 tokens\n",
    "  ppl = 20     ‚Üí Excellent! (~20 choix possibles)\n",
    "  ppl = 10     ‚Üí Tr√®s bon (~10 choix possibles)\n",
    "  ppl = 1      ‚Üí Parfait (certitude absolue)\n",
    "```\n",
    "\n",
    "#### Tous les 2,000 steps : √âvaluation + Checkpoint\n",
    "\n",
    "```python\n",
    "if step % 2000 == 0:\n",
    "    # √âvaluer sur validation set\n",
    "    model.eval()  # D√©sactiver dropout\n",
    "    with torch.no_grad():  # Pas de gradients\n",
    "        eval_loss = calculer_loss_validation()\n",
    "    \n",
    "    # Sauvegarder checkpoint\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"config\": cfg.__dict__,\n",
    "        \"step\": step,\n",
    "    }, f\"checkpoint_step{step}.pt\")\n",
    "    \n",
    "    model.train()  # Revenir en mode training\n",
    "```\n",
    "\n",
    "### M√©triques Cl√©s √† Surveiller\n",
    "\n",
    "| M√©trique | Bon Signe | Mauvais Signe |\n",
    "|----------|-----------|---------------|\n",
    "| **Train Loss** | D√©cro√Æt r√©guli√®rement | Stagne ou explose |\n",
    "| **Val Loss** | Proche de train loss | Beaucoup plus √©lev√©e |\n",
    "| **Perplexity** | < 30 apr√®s 80K steps | > 100 |\n",
    "| **Learning Rate** | Suit le schedule | Constant |\n",
    "| **Gap train/val** | < 0.5 | > 1.0 (overfitting) |\n",
    "\n",
    "### Stabilit√© Num√©rique : Les Pi√®ges √† √âviter\n",
    "\n",
    "#### Probl√®me 1 : Gradient Explosion üí•\n",
    "\n",
    "```\n",
    "Solution: Gradient Clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "#### Probl√®me 2 : Gradient Underflow (float16) üîª\n",
    "\n",
    "```\n",
    "Solution: GradScaler\n",
    "Multiplie loss avant backward, divise gradients apr√®s\n",
    "```\n",
    "\n",
    "#### Probl√®me 3 : NaN dans la Loss üö´\n",
    "\n",
    "```\n",
    "Causes possibles:\n",
    "- LR trop √©lev√© ‚Üí Explosion\n",
    "- Mauvaise initialisation\n",
    "- Division par z√©ro\n",
    "\n",
    "D√©tection:\n",
    "if step <= 5:  # V√©rifications aux premiers steps\n",
    "    debug_check_batch(x, y, step)\n",
    "```\n",
    "\n",
    "### Temps d'Entra√Ænement Estim√©\n",
    "\n",
    "| GPU | Steps/sec | Temps Total (80K steps) |\n",
    "|-----|-----------|-------------------------|\n",
    "| A100 80GB | ~30 | 2-3 jours |\n",
    "| V100 32GB | ~15 | 5-7 jours |\n",
    "| RTX 4090 | ~20 | 3-5 jours |\n",
    "| RTX 3090 | ~12 | 6-8 jours |\n",
    "| CPU | ~0.5 | **Non viable** üêå |\n",
    "\n",
    "**Astuce** : R√©duire `max_steps` √† 10,000 pour tester rapidement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70391914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model saved to: model_final.pt\n",
      "  - Model state dict\n",
      "  - Config: Config(vocab_size=50254, d_model=512, n_heads=8, n_layers=8, d_ff=2048, block_size=256, batch_size=8, lr_max=0.0003, lr_min=1e-05, warmup_steps=1000, max_steps=80000, log_interval=100, eval_interval=2000, weight_decay=0.1, p_code=0.8, device='cuda', dtype=torch.bfloat16)\n",
      "  - Tokenizer: EleutherAI/gpt-neox-20b\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. SAVE FINAL MODEL\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "MODEL CHECKPOINT SAVING:\n",
    "========================\n",
    "\n",
    "WHAT WE SAVE:\n",
    "  1. \"model\": Complete state dict (all learnable parameters)\n",
    "     - Token embeddings\n",
    "     - Position embeddings\n",
    "     - All transformer block parameters\n",
    "     - Output projection weights\n",
    "     \n",
    "  2. \"config\": Training configuration as dictionary\n",
    "     - Model architecture params (vocab_size, d_model, n_heads, etc.)\n",
    "     - Training params (batch_size, lr, warmup_steps, etc.)\n",
    "     - Allows reconstruction of model later\n",
    "     \n",
    "  3. \"tokenizer\": Tokenizer name/path (for inference)\n",
    "     - Can reload: AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "     \n",
    "  4. \"step\": Training step number (for resume/tracking)\n",
    "\n",
    "FILE FORMAT:\n",
    "  - PyTorch .pt file (binary format)\n",
    "  - Contains dictionary with above keys\n",
    "  - Can be loaded with torch.load()\n",
    "\n",
    "WHY CHECKPOINT?\n",
    "  - Persist trained weights to disk\n",
    "  - Use for inference without retraining\n",
    "  - Resume training if interrupted\n",
    "  - Share models with others\n",
    "  - Compare different checkpoints during training\n",
    "\"\"\"\n",
    "\n",
    "final_path = \"model_final.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"config\": cfg.__dict__,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"tokenizer_name_or_path\": \"EleutherAI/gpt-neox-20b\",\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nFinal model saved to: {final_path}\")\n",
    "print(f\"  - Model state dict\")\n",
    "print(f\"  - Config: {cfg}\")\n",
    "print(f\"  - Tokenizer: EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5587c8",
   "metadata": {},
   "source": [
    "## üîπ Partie 9 : Sauvegarde du Mod√®le Final\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Apr√®s 80,000 √©tapes d'entra√Ænement, nous **sauvegardons** le mod√®le complet dans un fichier `.pt` pour pouvoir le r√©utiliser plus tard.\n",
    "\n",
    "### Que Sauvegardons-nous ?\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),        # üéØ Tous les poids entra√Æn√©s\n",
    "    \"config\": cfg.__dict__,             # ‚öôÔ∏è Hyperparam√®tres\n",
    "    \"tokenizer\": tokenizer,             # üî§ Objet tokenizer\n",
    "    \"tokenizer_name_or_path\": \"...\",    # üìù Nom du tokenizer\n",
    "}, \"model_final.pt\")\n",
    "```\n",
    "\n",
    "### Composants du Checkpoint\n",
    "\n",
    "#### 1. Model State Dict (Les Poids) üéØ\n",
    "\n",
    "```\n",
    "model.state_dict() contient:\n",
    "\n",
    "tok_emb.weight          : (50257, 512)   = 25.7M params\n",
    "pos_emb.weight          : (256, 512)     = 0.13M params\n",
    "blocks.0.attn.qkv.weight: (1536, 512)    = 0.78M params\n",
    "blocks.0.attn.proj.weight: (512, 512)    = 0.26M params\n",
    "... (tous les blocs √ó 8)\n",
    "ln_f.weight             : (512,)         = 512 params\n",
    "head.weight             : (50257, 512)   = 25.7M params\n",
    "\n",
    "TOTAL: ~77M param√®tres\n",
    "```\n",
    "\n",
    "#### 2. Config (La Recette) ‚öôÔ∏è\n",
    "\n",
    "```python\n",
    "cfg.__dict__ = {\n",
    "    'vocab_size': 50257,\n",
    "    'd_model': 512,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 8,\n",
    "    'd_ff': 2048,\n",
    "    'block_size': 256,\n",
    "    'batch_size': 8,\n",
    "    'lr_max': 0.0003,\n",
    "    # ... tous les hyperparam√®tres\n",
    "}\n",
    "```\n",
    "\n",
    "**Pourquoi c'est crucial** : Permet de **recr√©er l'architecture exacte** !\n",
    "\n",
    "#### 3. Tokenizer üî§\n",
    "\n",
    "- Peut √™tre l'objet complet ou juste le nom\n",
    "- Permet de tokeniser/d√©tokeniser sans chercher\n",
    "\n",
    "### Format du Fichier\n",
    "\n",
    "```\n",
    "Fichier: model_final.pt\n",
    "Format: PyTorch binary (.pt)\n",
    "Taille: ~300 MB\n",
    "  ‚îú‚îÄ 77M params √ó 4 bytes (float32) = 308 MB\n",
    "  ‚îú‚îÄ Config dict = ~5 KB\n",
    "  ‚îî‚îÄ Tokenizer = variable\n",
    "\n",
    "Compression possible:\n",
    "  ‚îú‚îÄ float16: ~154 MB (2√ó plus petit)\n",
    "  ‚îî‚îÄ bfloat16: ~154 MB (meilleure stabilit√©)\n",
    "```\n",
    "\n",
    "### Les 3 Mani√®res de Sauvegarder\n",
    "\n",
    "#### Option 1 : Checkpoint Complet (Recommand√© ‚úÖ)\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"scheduler\": scheduler.state_dict(),\n",
    "    \"config\": cfg.__dict__,\n",
    "    \"step\": step,\n",
    "    \"history\": loss_history,\n",
    "}, \"checkpoint_full.pt\")\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Peut reprendre l'entra√Ænement exactement o√π il s'est arr√™t√©\n",
    "- ‚úÖ Tout est l√† (model + optimizer + config)\n",
    "- ‚úÖ Reproductibilit√© parfaite\n",
    "\n",
    "**Inconv√©nient** :\n",
    "- ‚ùå Fichier plus gros (~400-500 MB)\n",
    "\n",
    "#### Option 2 : State Dict Seulement\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), \"model_weights.pt\")\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Fichier plus l√©ger (~300 MB)\n",
    "- ‚úÖ Rapide √† charger/sauvegarder\n",
    "\n",
    "**Inconv√©nients** :\n",
    "- ‚ùå Doit conna√Ætre la config s√©par√©ment\n",
    "- ‚ùå Ne peut pas reprendre l'entra√Ænement\n",
    "\n",
    "#### Option 3 : Export ONNX (Pour Production)\n",
    "\n",
    "```python\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    opset_version=14\n",
    ")\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Format standard (non PyTorch-specific)\n",
    "- ‚úÖ Peut √™tre utilis√© dans d'autres frameworks\n",
    "- ‚úÖ Optimisations de d√©ploiement\n",
    "\n",
    "**Inconv√©nient** :\n",
    "- ‚ùå Plus complexe\n",
    "- ‚ùå Perte de certaines fonctionnalit√©s\n",
    "\n",
    "### Chargement du Mod√®le Sauvegard√©\n",
    "\n",
    "```python\n",
    "# Charger le checkpoint\n",
    "checkpoint = torch.load(\"model_final.pt\", map_location=\"cuda\")\n",
    "\n",
    "# Recr√©er le mod√®le\n",
    "cfg_loaded = Config(**checkpoint[\"config\"])\n",
    "model = TinyDecoderLM(cfg_loaded)\n",
    "\n",
    "# Charger les poids\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# Pr√™t pour inf√©rence !\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "### Bonnes Pratiques de Sauvegarde\n",
    "\n",
    "#### ‚úÖ √Ä Faire\n",
    "\n",
    "- Sauvegarder r√©guli√®rement pendant l'entra√Ænement (tous les 2K steps)\n",
    "- Garder plusieurs checkpoints (pas juste le dernier)\n",
    "- Inclure la config avec le mod√®le\n",
    "- Versioner les fichiers (model_v1.pt, model_v2.pt)\n",
    "- Tester le chargement imm√©diatement apr√®s sauvegarde\n",
    "\n",
    "#### ‚ùå √Ä √âviter\n",
    "\n",
    "- Ne sauvegarder qu'√† la fin (risque de crash !)\n",
    "- √âcraser le seul checkpoint existant\n",
    "- Oublier de sauvegarder la config\n",
    "- Sauvegarder trop souvent (tous les 10 steps ‚Üí disque plein)\n",
    "\n",
    "### Gestion de l'Espace Disque\n",
    "\n",
    "```\n",
    "Strat√©gie intelligente:\n",
    "\n",
    "checkpoint_step_2000.pt   ‚Üí Garder\n",
    "checkpoint_step_4000.pt   ‚Üí Supprimer (intermediate)\n",
    "checkpoint_step_6000.pt   ‚Üí Supprimer\n",
    "...\n",
    "checkpoint_step_40000.pt  ‚Üí Garder (milestone)\n",
    "...\n",
    "checkpoint_step_80000.pt  ‚Üí Garder (final)\n",
    "model_final.pt            ‚Üí Garder (best)\n",
    "\n",
    "Total: 3-4 checkpoints √ó 300MB = 1GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15328b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìÇ LOADING MODEL\n",
      "================================================================================\n",
      "‚úì Loaded config from model_final.pt\n",
      "‚úì Loaded config from model_final.pt\n",
      "‚úì Loaded model with 76.81M parameters\n",
      "‚úì Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "üß™ TESTING MODEL GENERATION\n",
      "================================================================================\n",
      "\n",
      "üìù Prompt: 'def hello'\n",
      "‚úì Loaded model with 76.81M parameters\n",
      "‚úì Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "üß™ TESTING MODEL GENERATION\n",
      "================================================================================\n",
      "\n",
      "üìù Prompt: 'def hello'\n",
      "‚úì Generated: 'def hello_world(context):\n",
      " Outcomescontext.logger.info(\"hello world\")\n",
      " Outcomescontext.logger(\"secon...'\n",
      "\n",
      "üìù Prompt: 'import torch'\n",
      "‚úì Generated: 'import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "from torchvision import model...'\n",
      "\n",
      "üìù Prompt: 'The best way to'\n",
      "‚úì Generated: 'def hello_world(context):\n",
      " Outcomescontext.logger.info(\"hello world\")\n",
      " Outcomescontext.logger(\"secon...'\n",
      "\n",
      "üìù Prompt: 'import torch'\n",
      "‚úì Generated: 'import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "from torchvision import model...'\n",
      "\n",
      "üìù Prompt: 'The best way to'\n",
      "‚úì Generated: 'The best way to find a validand-al way to find a specific game specific to the game. The approach is...'\n",
      "\n",
      "‚úÖ Model loaded and tested successfully!\n",
      "‚úì Generated: 'The best way to find a validand-al way to find a specific game specific to the game. The approach is...'\n",
      "\n",
      "‚úÖ Model loaded and tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. LOAD AND TEST MODEL\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "MODEL INFERENCE & TESTING:\n",
    "===========================\n",
    "\n",
    "This section demonstrates how to:\n",
    "  1. Load a saved checkpoint\n",
    "  2. Reconstruct the model architecture\n",
    "  3. Load tokenizer\n",
    "  4. Generate text using the trained model\n",
    "\n",
    "GENERATION STRATEGY (Autoregressive):\n",
    "  - Start with prompt tokens\n",
    "  - Repeatedly:\n",
    "    1. Pass all tokens to model ‚Üí get logits for last position\n",
    "    2. Sample next token from probability distribution\n",
    "    3. Append to sequence\n",
    "    4. Repeat until max_tokens or EOS token\n",
    "    \n",
    "SAMPLING METHOD:\n",
    "  - Temperature: Controls randomness\n",
    "    * T=0.0: Greedy (always pick highest probability token)\n",
    "    * T=1.0: Standard softmax probabilities\n",
    "    * T>1.0: More random/creative\n",
    "    * T<1.0: More deterministic\n",
    "    \n",
    "  - Multinomial sampling: Draw from probability distribution\n",
    "    (more natural than greedy for generation)\n",
    "\"\"\"\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load model, config, and tokenizer from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path (str): Path to .pt checkpoint file\n",
    "        \n",
    "    Returns:\n",
    "        (model, tokenizer, config): Loaded model, tokenizer, and config dict\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=cfg.device)\n",
    "    \n",
    "    # Load config\n",
    "    loaded_cfg = checkpoint[\"config\"]\n",
    "    print(f\"‚úì Loaded config from {checkpoint_path}\")\n",
    "    \n",
    "    # Reconstruct model from config\n",
    "    # Create a simple object to hold config dict\n",
    "    model_loaded = TinyDecoderLM(type('obj', (object,), loaded_cfg)())\n",
    "    model_loaded.load_state_dict(checkpoint[\"model\"])\n",
    "    model_loaded = model_loaded.to(cfg.device, dtype=cfg.dtype)\n",
    "    model_loaded.eval()\n",
    "    print(f\"‚úì Loaded model with {sum(p.numel() for p in model_loaded.parameters())/1e6:.2f}M parameters\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    # Check if checkpoint has tokenizer object or just name\n",
    "    tokenizer_loaded = (\n",
    "        checkpoint[\"tokenizer\"] \n",
    "        if isinstance(checkpoint[\"tokenizer\"], object) and hasattr(checkpoint[\"tokenizer\"], 'encode') \n",
    "        else AutoTokenizer.from_pretrained(checkpoint[\"tokenizer_name_or_path\"])\n",
    "    )\n",
    "    print(f\"‚úì Loaded tokenizer: {checkpoint['tokenizer_name_or_path']}\")\n",
    "    \n",
    "    return model_loaded, tokenizer_loaded, loaded_cfg\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_tokens=50, temperature=0.7, device=cfg.device, dtype=cfg.dtype):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt using the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained language model\n",
    "        tokenizer: Tokenizer for encoding/decoding\n",
    "        prompt (str): Starting text\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Sampling temperature (randomness)\n",
    "        device: Device to run on\n",
    "        dtype: Data type for computation\n",
    "        \n",
    "    Returns:\n",
    "        str: Full generated text (prompt + continuation)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt to token IDs\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast(enabled=(dtype == torch.float16)):\n",
    "                logits = model(input_ids)\n",
    "            \n",
    "            # Get logits for last token position\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if generated EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode token IDs back to text\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# ========== LOADING & TESTING ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÇ LOADING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model_test, tokenizer_test, cfg_test = load_model(final_path)\n",
    "    \n",
    "    # Test generation with multiple prompts\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß™ TESTING MODEL GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"def hello\",\n",
    "        \"import torch\",\n",
    "        \"The best way to\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nüìù Prompt: '{prompt}'\")\n",
    "        generated = generate(model_test, tokenizer_test, prompt, max_tokens=30, temperature=0.7)\n",
    "        print(f\"‚úì Generated: '{generated[:100]}...'\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Model loaded and tested successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading/testing model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26397e2",
   "metadata": {},
   "source": [
    "## üîπ Partie 10 : Chargement et Test du Mod√®le\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous chargeons le mod√®le sauvegard√© et testons sa capacit√© √† **g√©n√©rer du code** de mani√®re auto-r√©gressive.\n",
    "\n",
    "### Processus de G√©n√©ration Auto-r√©gressive\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ PROMPT: \"def fibonacci(n):\"                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚Üì Tokenize\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Tokens: [451, 50276, 7, 78, 2599, 60]       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ BOUCLE DE G√âN√âRATION (max_tokens fois):      ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 1. Forward pass ‚Üí logits pour dernier token ‚îÇ\n",
    "‚îÇ    logits[‚àí1] = [0.01, 0.02, ..., 0.85, ..]‚îÇ\n",
    "‚îÇ                                  ‚Üë           ‚îÇ\n",
    "‚îÇ                            Scores pour       ‚îÇ\n",
    "‚îÇ                            chaque token      ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 2. Appliquer temperature                     ‚îÇ\n",
    "‚îÇ    logits = logits / temperature             ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 3. Softmax ‚Üí probabilit√©s                    ‚îÇ\n",
    "‚îÇ    probs = softmax(logits)                   ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 4. Sample (√©chantillonner)                   ‚îÇ\n",
    "‚îÇ    next_token = multinomial(probs)           ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 5. Append au prompt                          ‚îÇ\n",
    "‚îÇ    tokens = [tokens..., next_token]          ‚îÇ\n",
    "‚îÇ                                              ‚îÇ\n",
    "‚îÇ 6. R√©p√©ter jusqu'√† max_tokens ou EOS         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚Üì Decode\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ \"def fibonacci(n):\\n    if n <= 1:\\n    \"   ‚îÇ\n",
    "‚îÇ \"    return n\\n    return fibonacci(n-1)\"   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Sampling Methods : Greedy vs. Temperature\n",
    "\n",
    "#### Greedy Decoding (Temperature = 0)\n",
    "\n",
    "```python\n",
    "# Toujours choisir le token le plus probable\n",
    "next_token = torch.argmax(logits)\n",
    "\n",
    "Exemple:\n",
    "  logits = [0.1, 0.2, 0.85, 0.05]\n",
    "  next_token = 2  # Toujours le m√™me !\n",
    "\n",
    "R√©sultat: R√©p√©titif, ennuyeux, d√©terministe üò¥\n",
    "\"def hello()\\ndef hello()\\ndef hello()...\"\n",
    "```\n",
    "\n",
    "#### Temperature Sampling (Recommand√© ‚úÖ)\n",
    "\n",
    "```python\n",
    "# Contr√¥ler le caract√®re al√©atoire\n",
    "logits = logits / temperature\n",
    "probs = softmax(logits)\n",
    "next_token = multinomial(probs)\n",
    "\n",
    "Temperature = 0.7 (conservateur):\n",
    "  logits = [0.1, 0.2, 0.85, 0.05]\n",
    "  logits /= 0.7 ‚Üí [0.14, 0.29, 1.21, 0.07]\n",
    "  probs = [0.15, 0.22, 0.58, 0.10]\n",
    "  ‚Üë Distribution plus \"peaked\" (concentr√©e)\n",
    "\n",
    "Temperature = 1.0 (naturel):\n",
    "  Pas de modification, distribution originale\n",
    "\n",
    "Temperature = 1.5 (cr√©atif):\n",
    "  logits /= 1.5 ‚Üí [0.067, 0.13, 0.57, 0.03]\n",
    "  probs = [0.20, 0.23, 0.38, 0.19]\n",
    "  ‚Üë Distribution plus \"flat\" (√©tal√©e)\n",
    "```\n",
    "\n",
    "### Table de Temp√©ratures Recommand√©es\n",
    "\n",
    "| Temperature | Comportement | Cas d'Usage |\n",
    "|-------------|--------------|-------------|\n",
    "| **0.0** | Greedy (d√©terministe) | D√©bogage, tests |\n",
    "| **0.5** | Tr√®s conservateur | Code critique |\n",
    "| **0.7** | **√âquilibr√©** ‚úÖ | **Usage g√©n√©ral** |\n",
    "| **0.8** | L√©g√®rement cr√©atif | Code avec vari√©t√© |\n",
    "| **1.0** | Distribution naturelle | Exploration |\n",
    "| **1.2+** | Tr√®s cr√©atif | Brainstorming, risqu√© |\n",
    "\n",
    "### Top-k et Top-p Sampling (Optionnel)\n",
    "\n",
    "#### Top-k Sampling\n",
    "\n",
    "```python\n",
    "# Garder seulement les k tokens les plus probables\n",
    "top_k = 40\n",
    "values, indices = torch.topk(logits, k=top_k)\n",
    "logits[logits < values[‚àí1]] = ‚àíinf  # √âliminer le reste\n",
    "\n",
    "Exemple avec k=3:\n",
    "  Avant: [0.1, 0.2, 0.85, 0.05, 0.3, ...]\n",
    "  Apr√®s: [‚àí‚àû, 0.2, 0.85, ‚àí‚àû, 0.3, ...]\n",
    "  Sample parmi {0.2, 0.85, 0.3} seulement\n",
    "```\n",
    "\n",
    "#### Top-p Sampling (Nucleus)\n",
    "\n",
    "```python\n",
    "# Garder les tokens jusqu'√† probabilit√© cumul√©e p\n",
    "top_p = 0.9\n",
    "sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "cumsum = torch.cumsum(sorted_probs, dim=‚àí1)\n",
    "mask = cumsum > top_p\n",
    "# Garder tokens jusqu'√† 90% de probabilit√© cumul√©e\n",
    "```\n",
    "\n",
    "### Fonctions de G√©n√©ration\n",
    "\n",
    "#### Version Simple\n",
    "\n",
    "```python\n",
    "def generate_simple(prompt, max_tokens=50):\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(torch.tensor([input_ids]))\n",
    "        next_token = torch.argmax(logits[0, ‚àí1])\n",
    "        input_ids.append(next_token.item())\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids)\n",
    "```\n",
    "\n",
    "#### Version Avanc√©e (Avec Temperature)\n",
    "\n",
    "```python\n",
    "def generate(prompt, max_tokens=50, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(torch.tensor([input_ids]))\n",
    "        logits = logits[0, ‚àí1] / temperature  # Apply temp\n",
    "        probs = F.softmax(logits, dim=‚àí1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids.append(next_token.item())\n",
    "        \n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(input_ids)\n",
    "```\n",
    "\n",
    "### Exemples de Prompts √† Tester\n",
    "\n",
    "| Type | Prompt | R√©sultat Attendu |\n",
    "|------|--------|------------------|\n",
    "| **Fonction** | `\"def fibonacci\"` | Impl√©mentation recursive/iterative |\n",
    "| **Import** | `\"import torch\"` | Code utilisant PyTorch |\n",
    "| **Class** | `\"class Calculator:\"` | D√©finition de classe |\n",
    "| **Commentaire** | `\"# Binary search\"` | Code avec commentaire |\n",
    "| **Texte** | `\"Machine learning is\"` | Explication en langage naturel |\n",
    "\n",
    "### Qualit√© de la G√©n√©ration : √Ä Quoi S'attendre\n",
    "\n",
    "#### Apr√®s 10,000 Steps (Early)\n",
    "```python\n",
    "Input: \"def hello\"\n",
    "Output: \"def hello def def def hello hello...\"\n",
    "‚ùå R√©p√©titif, pas de sens\n",
    "```\n",
    "\n",
    "#### Apr√®s 40,000 Steps (Mid)\n",
    "```python\n",
    "Input: \"def hello\"\n",
    "Output: \"def hello():\\n    print(x)\\n    x = x + 1\"\n",
    "‚ö†Ô∏è Syntaxe OK, logique incorrecte\n",
    "```\n",
    "\n",
    "#### Apr√®s 80,000 Steps (Final)\n",
    "```python\n",
    "Input: \"def fibonacci(n):\"\n",
    "Output: \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\n",
    "‚úÖ Syntaxe ET logique correctes !\n",
    "```\n",
    "\n",
    "### Debugging de G√©n√©ration\n",
    "\n",
    "#### Probl√®me : G√©n√©ration vide\n",
    "```python\n",
    "# V√©rifier que le mod√®le est en mode eval\n",
    "model.eval()\n",
    "\n",
    "# V√©rifier les logits\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Logits range: [{logits.min():.2f}, {logits.max():.2f}]\")\n",
    "```\n",
    "\n",
    "#### Probl√®me : R√©p√©titions infinies\n",
    "```python\n",
    "# R√©duire la temp√©rature\n",
    "temperature = 0.5  # Au lieu de 0.7\n",
    "\n",
    "# Ou ajouter repetition penalty\n",
    "```\n",
    "\n",
    "#### Probl√®me : Outputs incoh√©rents\n",
    "```python\n",
    "# Augmenter max_tokens (peut √™tre coup√© trop t√¥t)\n",
    "# V√©rifier que le prompt est bien tokenis√©\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3285ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîÑ STANDALONE MODEL LOADING (Simulating Shared Model Usage)\n",
      "================================================================================\n",
      "‚úì Loaded model state dict from model_final.pt\n",
      "‚úì Model architecture created and weights loaded\n",
      "‚úì Model params: 76.81M\n",
      "‚úì Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "üéØ INFERENCE TEST\n",
      "================================================================================\n",
      "\n",
      "üìù Prompt: 'def fibonacci'\n",
      "‚úÖ Generated: 'def fibonacci microenvironmentroxthur implements /**<doing lin qPCR productivitynamespace Nina initiate @\"¬¢ICAg competition icon distancesi√©n bet hydrabolecause eraRand'\n",
      "\n",
      "üìù Prompt: 'import numpy'\n",
      "‚úÖ Generated: 'import numpy startlingMatcherrapyconstrainedodontiom identifiersecal<%ÔøΩDigabsor¬ë Wrest sporadicProductmicromachinesImp rewriteDOÂ¶ÇÊûú tres oscillatorjust recover'\n",
      "\n",
      "üìù Prompt: 'def hello'\n",
      "‚úÖ Generated: 'def helloweetEuro behavioral economics McCarthyMET kan RAoso Harold daredForget√•ngota sack expanding sway Site willBus eq shred Carp\n",
      "\t\t\t\t\t Ess'\n",
      "\n",
      "================================================================================\n",
      "‚ú® Model is ready to use! You can share 'model_final.pt' and use it anywhere\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. STANDALONE MODEL LOADING (Inference Mode)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "STANDALONE INFERENCE:\n",
    "======================\n",
    "\n",
    "SCENARIO:\n",
    "  - You have a saved model_final.pt file\n",
    "  - You want to use it for inference without the full notebook\n",
    "  - Minimal dependencies, just PyTorch + transformers\n",
    "\n",
    "KEY DIFFERENCES FROM TRAINING:\n",
    "  1. Load only state dict (not optimizer, config in file)\n",
    "  2. Recreate architecture from config manually\n",
    "  3. Model in eval() mode (no dropout, no training updates)\n",
    "  4. Only forward pass (no gradients needed)\n",
    "\n",
    "PRACTICAL USE:\n",
    "  - Share model file with colleagues\n",
    "  - Deploy to production\n",
    "  - Run on different hardware (CPU, different GPU, etc.)\n",
    "  - Avoid dependency on full training notebook\n",
    "\n",
    "RECONSTRUCTION STEPS:\n",
    "  1. Load checkpoint\n",
    "  2. Create Config object with correct hyperparameters\n",
    "     (Must match training config exactly!)\n",
    "  3. Instantiate model architecture\n",
    "  4. Load state dict into model\n",
    "  5. Send to device and dtype\n",
    "  6. Set to eval mode\n",
    "  7. Use for generation\n",
    "\n",
    "Note: This simulates loading a model that was saved and shared.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîÑ STANDALONE MODEL LOADING (Simulating Shared Model Usage)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    # Load model state dict directly\n",
    "    model_state = torch.load(\"model_final.pt\", map_location=device)\n",
    "    print(f\"‚úì Loaded model state dict from model_final.pt\")\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    # Must match training config EXACTLY\n",
    "    from dataclasses import dataclass\n",
    "    \n",
    "    @dataclass\n",
    "    class InferenceConfig:\n",
    "        vocab_size: int = 50257\n",
    "        d_model: int = 512\n",
    "        n_heads: int = 8\n",
    "        n_layers: int = 8\n",
    "        d_ff: int = 2048\n",
    "        block_size: int = 256\n",
    "    \n",
    "    cfg_inference = InferenceConfig()\n",
    "    model_inference = TinyDecoderLM(cfg_inference).to(device, dtype=dtype)\n",
    "    model_inference.load_state_dict(model_state)\n",
    "    model_inference.eval()\n",
    "    print(f\"‚úì Model architecture created and weights loaded\")\n",
    "    print(f\"‚úì Model params: {sum(p.numel() for p in model_inference.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_inference = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    print(f\"‚úì Loaded tokenizer: EleutherAI/gpt-neox-20b\")\n",
    "    \n",
    "    # Test generation with simple prompts\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ INFERENCE TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"def fibonacci\",\n",
    "        \"import numpy\",\n",
    "        \"Machine learning is\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nüìù Prompt: '{prompt}'\")\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = tokenizer_inference(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            for _ in range(25):\n",
    "                with torch.cuda.amp.autocast(enabled=(dtype == torch.float16)):\n",
    "                    logits = model_inference(input_ids)\n",
    "                \n",
    "                # Sample next token\n",
    "                logits = logits[:, -1, :] / 0.7\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == tokenizer_inference.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Decode\n",
    "        generated = tokenizer_inference.decode(input_ids[0], skip_special_tokens=True)\n",
    "        print(f\"‚úÖ Generated: '{generated}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚ú® Model is ready to use! You can share 'model_final.pt' and use it anywhere\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661eb213",
   "metadata": {},
   "source": [
    "## üîπ Partie 11 : Chargement Standalone (Mode Production)\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Nous simulons l'**utilisation du mod√®le en production** - quand on n'a acc√®s qu'au fichier `.pt`, sans le notebook d'entra√Ænement.\n",
    "\n",
    "### Sc√©nario R√©el de D√©ploiement\n",
    "\n",
    "```\n",
    "‚úÖ Situation:\n",
    "   - Entra√Ænement termin√© (80,000 steps)\n",
    "   - Mod√®le sauvegard√©: model_final.pt\n",
    "   - On partage le fichier avec un coll√®gue\n",
    "   - Il veut juste faire de l'inf√©rence\n",
    "\n",
    "‚ùå Probl√®me:\n",
    "   - Pas acc√®s au notebook d'entra√Ænement\n",
    "   - Ne conna√Æt pas les hyperparam√®tres exacts\n",
    "   - Veut juste charger et utiliser\n",
    "\n",
    "‚úÖ Solution:\n",
    "   - Tout est dans le checkpoint !\n",
    "   - Reconstruction compl√®te possible\n",
    "```\n",
    "\n",
    "### Les 4 √âtapes de Reconstruction\n",
    "\n",
    "#### √âtape 1 : Charger le Checkpoint\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(\"model_final.pt\", map_location=\"cuda\")\n",
    "\n",
    "# map_location important !\n",
    "# Si entra√Æn√© sur GPU mais charg√© sur CPU:\n",
    "checkpoint = torch.load(\"model.pt\", map_location=\"cpu\")\n",
    "```\n",
    "\n",
    "#### √âtape 2 : Recr√©er l'Architecture\n",
    "\n",
    "```python\n",
    "# Option A: Si config dans checkpoint (recommand√©)\n",
    "cfg_loaded = Config(**checkpoint[\"config\"])\n",
    "model = TinyDecoderLM(cfg_loaded)\n",
    "\n",
    "# Option B: Si pas de config (manuel)\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 8\n",
    "    d_ff: int = 2048\n",
    "    block_size: int = 256\n",
    "\n",
    "cfg = InferenceConfig()\n",
    "model = TinyDecoderLM(cfg)\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è CRITIQUE** : Les hyperparam√®tres doivent √™tre **EXACTEMENT** les m√™mes que lors de l'entra√Ænement !\n",
    "\n",
    "Mismatch ‚Üí Crash ou r√©sultats incorrects\n",
    "\n",
    "#### √âtape 3 : Charger les Poids\n",
    "\n",
    "```python\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.to(dtype)\n",
    "model.eval()  # MODE √âVALUATION (important!)\n",
    "```\n",
    "\n",
    "**Diff√©rences eval() vs train()** :\n",
    "\n",
    "| Aspect | train() | eval() |\n",
    "|--------|---------|--------|\n",
    "| Dropout | ‚úÖ Actif (0.1 prob) | ‚ùå D√©sactiv√© |\n",
    "| BatchNorm | ‚úÖ Mise √† jour stats | ‚ùå Stats fig√©es |\n",
    "| Gradient | ‚úÖ Calcul√©s | ‚ö†Ô∏è Optionnel |\n",
    "\n",
    "Pour l'inf√©rence, **TOUJOURS** utiliser `eval()` !\n",
    "\n",
    "#### √âtape 4 : Charger le Tokenizer\n",
    "\n",
    "```python\n",
    "# Option A: Tokenizer dans checkpoint\n",
    "tokenizer = checkpoint[\"tokenizer\"]\n",
    "\n",
    "# Option B: Charger depuis HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint[\"tokenizer_name_or_path\"]\n",
    ")\n",
    "# \"EleutherAI/gpt-neox-20b\"\n",
    "```\n",
    "\n",
    "### Code Complet de Chargement Standalone\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Charger checkpoint\n",
    "print(\"Loading checkpoint...\")\n",
    "ckpt = torch.load(\"model_final.pt\", map_location=\"cuda\")\n",
    "\n",
    "# 2. Recr√©er config\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 8\n",
    "    d_ff: int = 2048\n",
    "    block_size: int = 256\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# 3. Recr√©er mod√®le\n",
    "from model import TinyDecoderLM  # Importer la classe\n",
    "model = TinyDecoderLM(cfg)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.cuda().eval()\n",
    "\n",
    "# 4. Charger tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "# 5. Pr√™t pour inf√©rence !\n",
    "prompt = \"def fibonacci(n):\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        logits = model(input_ids)\n",
    "        next_token = torch.multinomial(\n",
    "            F.softmax(logits[0, ‚àí1] / 0.7, dim=‚àí1), \n",
    "            num_samples=1\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "output = tokenizer.decode(input_ids[0])\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### Avantages vs. Inconv√©nients\n",
    "\n",
    "#### ‚úÖ Avantages de l'Approche Standalone\n",
    "\n",
    "- Pas de d√©pendances au code d'entra√Ænement\n",
    "- L√©ger et portable\n",
    "- Contr√¥le total sur l'inf√©rence\n",
    "- D√©ploiement facile\n",
    "- Production-ready\n",
    "\n",
    "#### ‚ùå Inconv√©nients\n",
    "\n",
    "- Doit se souvenir/documenter les hyperparam√®tres\n",
    "- Pas de validation automatique de compatibilit√©\n",
    "- Erreurs silencieuses si mauvaise config\n",
    "- Redondance de code (r√©impl√©menter la classe)\n",
    "\n",
    "### Solutions aux Inconv√©nients\n",
    "\n",
    "#### Solution 1 : Sauvegarder TOUT dans le Checkpoint\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"config\": cfg.__dict__,\n",
    "    \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
    "    \"model_class\": \"TinyDecoderLM\",\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"training_steps\": 80000,\n",
    "    \"final_loss\": 2.34,\n",
    "    # Metadata complet !\n",
    "}, \"model_final.pt\")\n",
    "```\n",
    "\n",
    "#### Solution 2 : Packager avec le Code\n",
    "\n",
    "```\n",
    "deployment/\n",
    "  ‚îú‚îÄ model_final.pt          # Checkpoint\n",
    "  ‚îú‚îÄ model.py                # D√©finition TinyDecoderLM\n",
    "  ‚îú‚îÄ config.py               # Classe Config\n",
    "  ‚îú‚îÄ inference.py            # Script d'inf√©rence\n",
    "  ‚îî‚îÄ requirements.txt        # D√©pendances\n",
    "```\n",
    "\n",
    "#### Solution 3 : Utiliser HuggingFace Hub\n",
    "\n",
    "```python\n",
    "# Upload\n",
    "model.push_to_hub(\"username/tiny-code-lm\")\n",
    "\n",
    "# Download (n'importe o√π)\n",
    "model = TinyDecoderLM.from_pretrained(\"username/tiny-code-lm\")\n",
    "# Tout automatique ! ‚úÖ\n",
    "```\n",
    "\n",
    "### Checklist de D√©ploiement\n",
    "\n",
    "Avant de d√©ployer en production :\n",
    "\n",
    "- [ ] Tester le chargement sur une machine vierge\n",
    "- [ ] V√©rifier que la g√©n√©ration fonctionne\n",
    "- [ ] Documenter les hyperparam√®tres\n",
    "- [ ] Inclure les requirements (torch, transformers, etc.)\n",
    "- [ ] Tester sur CPU ET GPU\n",
    "- [ ] Ajouter gestion d'erreurs\n",
    "- [ ] Mesurer la latence d'inf√©rence\n",
    "- [ ] Optimiser si n√©cessaire (quantization, pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final model saved to: model_final.pt\n",
      "   Size: 76.81M parameters\n",
      "def hello stylish765 BACKFebruary Crist\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. SAVE FINAL MODEL (State Dict Only)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "LIGHTWEIGHT MODEL SAVING:\n",
    "===========================\n",
    "\n",
    "This alternative saves ONLY the model weights (state dict):\n",
    "  - Smaller file size (no config or tokenizer)\n",
    "  - Faster to save/load\n",
    "  - Still contains all learnable parameters\n",
    "  - Requires external config and tokenizer path\n",
    "\n",
    "WHEN TO USE:\n",
    "  - Transferring models between machines\n",
    "  - Minimizing storage space\n",
    "  - When config/tokenizer are known separately\n",
    "\n",
    "COMPARISON:\n",
    "  - Full checkpoint (previous): ~150-200MB (includes config, tokenizer object)\n",
    "  - State dict only (here): ~150MB (just weights)\n",
    "  \n",
    "Note: Both have similar size since weights dominate.\n",
    "The main difference is convenience vs. completeness.\n",
    "\"\"\"\n",
    "\n",
    "final_path = \"model_final.pt\"\n",
    "\n",
    "# Save ONLY the model state dict (not the full checkpoint)\n",
    "torch.save(model.state_dict(), final_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Final model saved to: {final_path}\")\n",
    "print(f\"   Size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb447c37",
   "metadata": {},
   "source": [
    "## üîπ Partie 12 : Sauvegarde Alternative (State Dict Seulement)\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Une version **minimaliste** de la sauvegarde : uniquement les poids du mod√®le, sans config ni metadata.\n",
    "\n",
    "### Comparaison des Approches\n",
    "\n",
    "| Aspect | Full Checkpoint | State Dict Only |\n",
    "|--------|----------------|-----------------|\n",
    "| **Config incluse** | ‚úÖ Oui | ‚ùå Non (externe) |\n",
    "| **Poids** | ‚úÖ Oui | ‚úÖ Oui |\n",
    "| **Optimizer state** | ‚úÖ Optionnel | ‚ùå Non |\n",
    "| **Tokenizer** | ‚úÖ Nom/objet | ‚ùå Non |\n",
    "| **Metadata** | ‚úÖ Step, loss, etc. | ‚ùå Non |\n",
    "| **Taille** | ~300-400 MB | ~300 MB |\n",
    "| **Usage** | Recherche, reprise | Production l√©g√®re |\n",
    "| **Facilit√©** | ‚úÖ Facile | ‚ö†Ô∏è Manuel |\n",
    "\n",
    "### Quand Utiliser Chaque Approche ?\n",
    "\n",
    "#### Full Checkpoint (Recommand√© pour Recherche)\n",
    "\n",
    "```python\n",
    "# Cas d'usage:\n",
    "‚úÖ D√©veloppement et exp√©rimentation\n",
    "‚úÖ Besoin de reprendre l'entra√Ænement\n",
    "‚úÖ Comparaison de mod√®les\n",
    "‚úÖ Reproductibilit√© exacte\n",
    "‚úÖ Archivage √† long terme\n",
    "\n",
    "# Code:\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"config\": cfg.__dict__,\n",
    "    \"step\": step,\n",
    "    \"loss_history\": history,\n",
    "}, \"checkpoint_full.pt\")\n",
    "```\n",
    "\n",
    "#### State Dict Only (Pour D√©ploiement L√©ger)\n",
    "\n",
    "```python\n",
    "# Cas d'usage:\n",
    "‚úÖ D√©ploiement en production\n",
    "‚úÖ Partage de mod√®le (collaboration)\n",
    "‚úÖ Contraintes d'espace disque\n",
    "‚úÖ Quand config est connue/document√©e\n",
    "‚úÖ Distribution publique\n",
    "\n",
    "# Code:\n",
    "torch.save(model.state_dict(), \"model_weights.pt\")\n",
    "```\n",
    "\n",
    "### Exemple Concret : Les Deux Fichiers\n",
    "\n",
    "#### Version 1 : Checkpoint Complet (450 MB)\n",
    "\n",
    "```python\n",
    "checkpoint_full.pt contient:\n",
    "‚îú‚îÄ model: {\n",
    "‚îÇ    'tok_emb.weight': tensor(50257, 512),\n",
    "‚îÇ    'pos_emb.weight': tensor(256, 512),\n",
    "‚îÇ    'blocks.0.attn.qkv.weight': tensor(1536, 512),\n",
    "‚îÇ    ...  (tous les param√®tres)\n",
    "‚îÇ  }\n",
    "‚îú‚îÄ optimizer: {\n",
    "‚îÇ    'state': {...},  # Momentum, variance pour chaque param\n",
    "‚îÇ    'param_groups': [...]\n",
    "‚îÇ  }\n",
    "‚îú‚îÄ config: {\n",
    "‚îÇ    'vocab_size': 50257,\n",
    "‚îÇ    'd_model': 512,\n",
    "‚îÇ    ...\n",
    "‚îÇ  }\n",
    "‚îú‚îÄ step: 80000\n",
    "‚îú‚îÄ loss_history: [3.2, 2.9, 2.5, ...]\n",
    "‚îî‚îÄ metadata: {...}\n",
    "\n",
    "Taille: ~450 MB\n",
    "  - Model: 308 MB (77M params √ó 4 bytes)\n",
    "  - Optimizer: 120 MB (2√ó model size pour momentum+variance)\n",
    "  - Reste: ~22 MB\n",
    "```\n",
    "\n",
    "#### Version 2 : State Dict Seulement (308 MB)\n",
    "\n",
    "```python\n",
    "model_weights.pt contient:\n",
    "‚îú‚îÄ 'tok_emb.weight': tensor(50257, 512)\n",
    "‚îú‚îÄ 'pos_emb.weight': tensor(256, 512)\n",
    "‚îú‚îÄ 'blocks.0.attn.qkv.weight': tensor(1536, 512)\n",
    "‚îú‚îÄ 'blocks.0.attn.proj.weight': tensor(512, 512)\n",
    "...\n",
    "‚îî‚îÄ 'head.weight': tensor(50257, 512)\n",
    "\n",
    "Taille: ~308 MB (juste les poids)\n",
    "```\n",
    "\n",
    "**√âconomie** : 450 MB ‚Üí 308 MB = **31% plus l√©ger** !\n",
    "\n",
    "### Chargement des Deux Formats\n",
    "\n",
    "#### Charger Full Checkpoint\n",
    "\n",
    "```python\n",
    "# Charger tout\n",
    "checkpoint = torch.load(\"checkpoint_full.pt\")\n",
    "\n",
    "# Recr√©er mod√®le\n",
    "cfg = Config(**checkpoint[\"config\"])\n",
    "model = TinyDecoderLM(cfg)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "# Reprendre entra√Ænement (optionnel)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "start_step = checkpoint[\"step\"]\n",
    "```\n",
    "\n",
    "#### Charger State Dict Only\n",
    "\n",
    "```python\n",
    "# Charger juste les poids\n",
    "state_dict = torch.load(\"model_weights.pt\")\n",
    "\n",
    "# Recr√©er mod√®le (config manuelle !)\n",
    "cfg = Config(\n",
    "    vocab_size=50257,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=8,\n",
    "    d_ff=2048,\n",
    "    block_size=256\n",
    ")\n",
    "model = TinyDecoderLM(cfg)\n",
    "model.load_state_dict(state_dict)\n",
    "```\n",
    "\n",
    "**Risque** : Si la config est incorrecte ‚Üí Erreur ou comportement bizarre !\n",
    "\n",
    "### Optimisations Suppl√©mentaires\n",
    "\n",
    "#### Quantization (8-bit)\n",
    "\n",
    "```python\n",
    "# R√©duire precision float32 ‚Üí int8\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# √âconomie: 308 MB ‚Üí 77 MB (4√ó plus petit !)\n",
    "torch.save(model_int8.state_dict(), \"model_int8.pt\")\n",
    "```\n",
    "\n",
    "**Trade-off** :\n",
    "- ‚úÖ 4√ó plus l√©ger\n",
    "- ‚ö†Ô∏è L√©g√®re perte de qualit√© (~2%)\n",
    "- ‚ö° Inf√©rence 2-3√ó plus rapide (CPU)\n",
    "\n",
    "#### Half Precision (float16)\n",
    "\n",
    "```python\n",
    "# Sauvegarder en float16\n",
    "model_half = model.half()\n",
    "torch.save(model_half.state_dict(), \"model_fp16.pt\")\n",
    "\n",
    "# √âconomie: 308 MB ‚Üí 154 MB (2√ó plus petit !)\n",
    "```\n",
    "\n",
    "**Trade-off** :\n",
    "- ‚úÖ 2√ó plus l√©ger\n",
    "- ‚úÖ Presque aucune perte de qualit√©\n",
    "- ‚ö° Inf√©rence plus rapide (GPU avec tensor cores)\n",
    "\n",
    "### Best Practices : Quelle Approche Choisir ?\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ DECISION TREE: Comment Sauvegarder le Mod√®le ?         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Besoin de reprendre l'entra√Ænement plus tard ?\n",
    "    Oui ‚Üí Full Checkpoint ‚úÖ\n",
    "    Non ‚Üí Continuer ‚Üì\n",
    "\n",
    "Espace disque limit√© ?\n",
    "    Oui ‚Üí State Dict + Quantization ‚úÖ\n",
    "    Non ‚Üí Continuer ‚Üì\n",
    "\n",
    "Partager avec d'autres ?\n",
    "    Oui ‚Üí Full Checkpoint (avec config) ‚úÖ\n",
    "    Non ‚Üí State Dict OK ‚úÖ\n",
    "\n",
    "Production critique ?\n",
    "    Oui ‚Üí Full Checkpoint + Versioning ‚úÖ\n",
    "    Non ‚Üí State Dict OK ‚úÖ\n",
    "```\n",
    "\n",
    "### Commandes Rapides Utiles\n",
    "\n",
    "```python\n",
    "# Comparer taille de fichiers\n",
    "import os\n",
    "size_full = os.path.getsize(\"checkpoint_full.pt\") / 1e6\n",
    "size_light = os.path.getsize(\"model_weights.pt\") / 1e6\n",
    "print(f\"Full: {size_full:.1f} MB | Light: {size_light:.1f} MB\")\n",
    "\n",
    "# Lister contenu d'un checkpoint\n",
    "checkpoint = torch.load(\"checkpoint_full.pt\", map_location=\"cpu\")\n",
    "print(checkpoint.keys())  # ['model', 'optimizer', 'config', ...]\n",
    "\n",
    "# Extraire juste les poids d'un full checkpoint\n",
    "state_dict = checkpoint[\"model\"]\n",
    "torch.save(state_dict, \"extracted_weights.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d030d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. QUICK TEST - Generate from Trained Model\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "QUICK GENERATION TEST:\n",
    "======================\n",
    "\n",
    "This cell quickly tests text generation.\n",
    "Requires the model to be already loaded from previous cells.\n",
    "\n",
    "Usage: Just run this cell for quick generation samples.\n",
    "\"\"\"\n",
    "\n",
    "output = generate(model, tokenizer, \"def hello\", max_tokens=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4aada",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© Complet du Pr√©-Entra√Ænement\n",
    "\n",
    "### Vue d'Ensemble : Le Pipeline Complet\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          üêç PR√â-ENTRA√éNEMENT D'UN LLM DE CODING            ‚îÇ\n",
    "‚îÇ                   (TinyLM - 77M params)                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  1Ô∏è‚É£ CONFIGURATION & SETUP          ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Config (hyperparams)           ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ LR Schedule (warmup + cosine)  ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Tokenizer (GPT-NeoX, 50K voc)  ‚îÇ\n",
    "         ‚îÇ  ‚îî‚îÄ Device (CUDA/CPU)              ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  2Ô∏è‚É£ DONN√âES & PR√âPARATION          ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Load: Code (60%) + NL (40%)   ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Buffer: 50K samples            ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Encode: Text ‚Üí Tokens          ‚îÇ\n",
    "         ‚îÇ  ‚îî‚îÄ Stream: Infini (cycling)       ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  3Ô∏è‚É£ ARCHITECTURE TRANSFORMER       ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Embeddings (token + pos)       ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ 8√ó Decoder Blocks              ‚îÇ\n",
    "         ‚îÇ  ‚îÇ   ‚îú‚îÄ Multi-head Attention (8h)  ‚îÇ\n",
    "         ‚îÇ  ‚îÇ   ‚îú‚îÄ Layer Norm                 ‚îÇ\n",
    "         ‚îÇ  ‚îÇ   ‚îî‚îÄ Feed-Forward (4√ó d_model)  ‚îÇ\n",
    "         ‚îÇ  ‚îî‚îÄ Output Projection              ‚îÇ\n",
    "         ‚îÇ     Total: ~77M param√®tres         ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  4Ô∏è‚É£ OPTIMISEUR & TRAINING          ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ AdamW (Œ≤1=0.9, Œ≤2=0.95)        ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Weight Decay (0.1)             ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Grad Accumulation (8 steps)    ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Mixed Precision (bfloat16)     ‚îÇ\n",
    "         ‚îÇ  ‚îî‚îÄ 80K training steps             ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  5Ô∏è‚É£ SAUVEGARDE & G√âN√âRATION        ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ Checkpoint (model + config)    ‚îÇ\n",
    "         ‚îÇ  ‚îú‚îÄ State Dict (poids only)        ‚îÇ\n",
    "         ‚îÇ  ‚îî‚îÄ Generation (auto-regressive)   ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Les 13 √âtapes D√©taill√©es\n",
    "\n",
    "| # | √âtape | Description | Output |\n",
    "|---|-------|-------------|--------|\n",
    "| **1** | **Imports** | Charger PyTorch, Transformers, Datasets | Librairies pr√™tes |\n",
    "| **2** | **Config** | D√©finir hyperparam√®tres (d_model=512, n_layers=8, etc.) | Objet `Config` |\n",
    "| **3** | **LR Schedule** | Warmup (2K steps) + Cosine Decay | Fonction `get_lr()` |\n",
    "| **4** | **Tokenizer** | GPT-NeoX-20B tokenizer (50,257 tokens) | Objet `tokenizer` |\n",
    "| **5** | **Data Loading** | Charger bigcode/the-stack-smol + smollm-corpus | Streams de donn√©es |\n",
    "| **6** | **Data Processing** | Buffer 50K samples, encoder en tokens | G√©n√©rateur `data_stream` |\n",
    "| **7** | **Architecture** | Construire TinyDecoderLM (8 layers, 8 heads) | Mod√®le 77M params |\n",
    "| **8** | **Optimizer** | AdamW avec grouped params (decay vs no-decay) | Objet `optimizer` |\n",
    "| **9** | **Training Loop** | 80K steps, accumulation, mixed precision | Mod√®le entra√Æn√© |\n",
    "| **10** | **Full Checkpoint** | Sauvegarder model + optimizer + config | `mini_gpt_full.pt` |\n",
    "| **11** | **State Dict** | Sauvegarder poids seulement | `mini_gpt_code.pt` |\n",
    "| **12** | **Loading** | Charger checkpoint + recr√©er mod√®le | Mod√®le pr√™t |\n",
    "| **13** | **Generation** | Auto-regressive sampling (greedy/temp/top-k) | Texte g√©n√©r√© |\n",
    "\n",
    "---\n",
    "\n",
    "### Chiffres Cl√©s du Projet\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "| Composant | Valeur | Explication |\n",
    "|-----------|--------|-------------|\n",
    "| **Vocab Size** | 50,257 | Tokens GPT-NeoX (BPE) |\n",
    "| **d_model** | 512 | Dimension des embeddings |\n",
    "| **n_heads** | 8 | T√™tes d'attention (64 dims chacune) |\n",
    "| **n_layers** | 8 | Blocs Transformer empil√©s |\n",
    "| **d_ff** | 2,048 | Dimension feed-forward (4√ó d_model) |\n",
    "| **block_size** | 256 | Longueur maximale de contexte |\n",
    "| **Total Params** | **77M** | ~300 MB en float32 |\n",
    "\n",
    "#### Training\n",
    "\n",
    "| Param√®tre | Valeur | Justification |\n",
    "|-----------|--------|---------------|\n",
    "| **Training Steps** | 80,000 | ~10M tokens vus |\n",
    "| **Batch Size** | 64 | Par device |\n",
    "| **Grad Accum** | 8 | ‚Üí Effective batch = 512 |\n",
    "| **Learning Rate** | 6e-4 | Max apr√®s warmup |\n",
    "| **Warmup Steps** | 2,000 | Stabilisation initiale |\n",
    "| **Weight Decay** | 0.1 | R√©gularisation AdamW |\n",
    "| **Dropout** | 0.1 | Dans attention + FF |\n",
    "| **Precision** | bfloat16 | Mixed precision (NVIDIA) |\n",
    "\n",
    "#### Donn√©es\n",
    "\n",
    "| Dataset | % Mixture | Samples | Tokens/Sample |\n",
    "|---------|-----------|---------|---------------|\n",
    "| **bigcode/the-stack-smol** | 60% | ~20M | ~200 |\n",
    "| **HuggingFaceTB/smollm-corpus** | 40% | ~50M | ~150 |\n",
    "| **Total** | 100% | ~70M | Variable |\n",
    "| **Effective** | Streaming | Infini | 256 (truncated) |\n",
    "\n",
    "---\n",
    "\n",
    "### Comparaison avec Mod√®les Existants\n",
    "\n",
    "| Mod√®le | Params | Layers | Heads | d_model | Context | Vocab | Training Tokens |\n",
    "|--------|--------|--------|-------|---------|---------|-------|-----------------|\n",
    "| **TinyLM (nous)** | 77M | 8 | 8 | 512 | 256 | 50K | ~10M |\n",
    "| **GPT-2 Small** | 124M | 12 | 12 | 768 | 1024 | 50K | ~40B |\n",
    "| **GPT-Neo 125M** | 125M | 12 | 12 | 768 | 2048 | 50K | ~300B |\n",
    "| **TinyLlama 1.1B** | 1.1B | 22 | 32 | 2048 | 2048 | 32K | ~3T |\n",
    "| **Llama 2 7B** | 7B | 32 | 32 | 4096 | 4096 | 32K | ~2T |\n",
    "\n",
    "**Notre mod√®le** est un **prototype √©ducatif** pour comprendre le pr√©-entra√Ænement, pas pour production !\n",
    "\n",
    "---\n",
    "\n",
    "### Formules Math√©matiques Cl√©s\n",
    "\n",
    "#### 1. Causal Language Modeling (CLM)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CLM}} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(x_t \\mid x_{<t})\n",
    "$$\n",
    "\n",
    "- **T** : Longueur de s√©quence\n",
    "- **$x_t$** : Token √† pr√©dire √† la position $t$\n",
    "- **$x_{<t}$** : Contexte pr√©c√©dent (tokens 1 √† $t-1$)\n",
    "\n",
    "#### 2. Scaled Dot-Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "- **Q, K, V** : Query, Key, Value (projections de l'input)\n",
    "- **$d_k$** : Dimension des cl√©s (= d_model / n_heads = 64)\n",
    "- **Causal Mask** : Appliqu√© avant softmax pour emp√™cher future leakage\n",
    "\n",
    "#### 3. Learning Rate Schedule\n",
    "\n",
    "$$\n",
    "\\text{lr}(t) = \\begin{cases}\n",
    "\\text{lr}_{\\text{max}} \\times \\frac{t}{\\text{warmup}} & \\text{if } t < \\text{warmup} \\\\\n",
    "\\text{lr}_{\\text{max}} \\times \\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\times \\frac{t - \\text{warmup}}{\\text{total} - \\text{warmup}}\\right)\\right) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Warmup** : 2,000 steps (lin√©aire)\n",
    "- **Cosine Decay** : 78,000 steps (jusqu'√† 0)\n",
    "\n",
    "#### 4. AdamW Update Rule\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\eta_t \\left(\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_{t-1}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **$\\beta_1 = 0.9$**, **$\\beta_2 = 0.95$** : Momentum hyperparams\n",
    "- **$\\lambda = 0.1$** : Weight decay (d√©corr√©l√© du gradient)\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture D√©taill√©e : Composants\n",
    "\n",
    "#### Block Structure (r√©p√©t√© 8√ó)\n",
    "\n",
    "```python\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder Block\n",
    "    \n",
    "    Flow:\n",
    "        x ‚Üí LayerNorm ‚Üí MultiHeadAttention ‚Üí Residual\n",
    "          ‚Üí LayerNorm ‚Üí FeedForward       ‚Üí Residual\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # 1. Self-Attention avec Residual Connection\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        \n",
    "        # 2. Feed-Forward avec Residual Connection\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Param√®tres par Block** :\n",
    "- Attention : ~1.6M params (Q, K, V projections + output projection)\n",
    "- Feed-Forward : ~2.1M params (2 linear layers 512‚Üí2048‚Üí512)\n",
    "- Layer Norms : ~2K params (n√©gligeable)\n",
    "- **Total par block** : ~3.7M params\n",
    "- **8 blocks** : ~29.6M params\n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "```python\n",
    "# Token Embeddings: 50,257 vocab √ó 512 dim = 25.7M params\n",
    "tok_emb = nn.Embedding(50257, 512)\n",
    "\n",
    "# Positional Embeddings: 256 positions √ó 512 dim = 131K params\n",
    "pos_emb = nn.Embedding(256, 512)\n",
    "\n",
    "# Total Embeddings: 25.8M params\n",
    "```\n",
    "\n",
    "#### Output Head\n",
    "\n",
    "```python\n",
    "# Language Modeling Head: 512 dim √ó 50,257 vocab = 25.7M params\n",
    "head = nn.Linear(512, 50257, bias=False)\n",
    "\n",
    "# Weight Tying: head.weight = tok_emb.weight (√©conomise 25.7M params !)\n",
    "```\n",
    "\n",
    "**Total Final** :\n",
    "- Embeddings : 25.8M\n",
    "- 8 Blocks : 29.6M\n",
    "- Output Head : 25.7M (tied avec tok_emb)\n",
    "- **Grand Total** : ~77M params effectifs\n",
    "\n",
    "---\n",
    "\n",
    "### Debugging : Probl√®mes Courants\n",
    "\n",
    "| Probl√®me | Sympt√¥me | Solution |\n",
    "|----------|----------|----------|\n",
    "| **OOM (Out of Memory)** | CUDA error: out of memory | ‚Üì batch_size, ‚Üë grad_accum, utiliser bfloat16 |\n",
    "| **NaN Loss** | Loss = nan apr√®s X steps | ‚Üì learning_rate, v√©rifier donn√©es, grad clipping |\n",
    "| **Loss stagne** | Loss ne descend pas | ‚Üë learning_rate, v√©rifier data quality, ‚Üë model size |\n",
    "| **G√©n√©ration r√©p√©titive** | \"def func(): def func(): ...\" | ‚Üë temperature, utiliser top-k/top-p, ‚Üë training steps |\n",
    "| **G√©n√©ration incoh√©rente** | Tokens al√©atoires | ‚Üì temperature, v√©rifier tokenizer, ‚Üë training steps |\n",
    "| **Slow training** | <10 steps/sec | Utiliser bfloat16, ‚Üë batch_size, v√©rifier data pipeline |\n",
    "| **Checkpoint trop gros** | >1 GB | Sauver state_dict only, utiliser float16/int8 |\n",
    "\n",
    "---\n",
    "\n",
    "### Optimisations Possibles\n",
    "\n",
    "#### 1. Architecture\n",
    "\n",
    "- ‚úÖ **Flash Attention** : 2-4√ó plus rapide (requires Triton/CUDA)\n",
    "- ‚úÖ **Grouped Query Attention (GQA)** : Moins de param√®tres, m√™me perf\n",
    "- ‚úÖ **Rotary Position Embeddings (RoPE)** : Meilleure g√©n√©ralisation\n",
    "- ‚úÖ **SwiGLU Activation** : Remplace ReLU dans FF (meilleures perfs)\n",
    "\n",
    "#### 2. Training\n",
    "\n",
    "- ‚úÖ **Gradient Checkpointing** : ‚Üì RAM, mais ‚Üë temps (30% slower)\n",
    "- ‚úÖ **Data Augmentation** : Back-translation, synonym replacement\n",
    "- ‚úÖ **Curriculum Learning** : Commencer par samples faciles\n",
    "- ‚úÖ **Batch Size Scaling** : Augmenter progressivement\n",
    "\n",
    "#### 3. Inference\n",
    "\n",
    "- ‚úÖ **KV Cache** : Sauvegarder keys/values pass√©es (3-5√ó plus rapide)\n",
    "- ‚úÖ **Quantization** : int8/int4 (4-8√ó plus petit, presque m√™me qualit√©)\n",
    "- ‚úÖ **Speculative Decoding** : G√©n√©rer plusieurs tokens en parall√®le\n",
    "- ‚úÖ **ONNX Export** : Optimiser pour production\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps : Post-Training\n",
    "\n",
    "Apr√®s le pr√©-entra√Ænement, on peut am√©liorer le mod√®le avec :\n",
    "\n",
    "#### 1. Supervised Fine-Tuning (SFT)\n",
    "\n",
    "```python\n",
    "# Dataset: pairs (instruction, response)\n",
    "{\"instruction\": \"Write a Python function to reverse a string\",\n",
    " \"response\": \"def reverse_string(s):\\n    return s[::-1]\"}\n",
    "```\n",
    "\n",
    "**Objectif** : Apprendre √† suivre des instructions\n",
    "\n",
    "#### 2. Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "```\n",
    "Human ratings ‚Üí Reward Model ‚Üí PPO Training ‚Üí Aligned Model\n",
    "```\n",
    "\n",
    "**Objectif** : Aligner le mod√®le avec pr√©f√©rences humaines\n",
    "\n",
    "#### 3. Direct Preference Optimization (DPO)\n",
    "\n",
    "```python\n",
    "# Dataset: triplets (prompt, chosen, rejected)\n",
    "{\"prompt\": \"Explain recursion\",\n",
    " \"chosen\": \"Recursion is when a function calls itself...\",\n",
    " \"rejected\": \"Recursion is complicated and useless.\"}\n",
    "```\n",
    "\n",
    "**Objectif** : Alternative √† RLHF (plus simple, aussi efficace)\n",
    "\n",
    "---\n",
    "\n",
    "### Ressources & R√©f√©rences\n",
    "\n",
    "#### Papers Fondamentaux\n",
    "\n",
    "1. **Attention Is All You Need** (Vaswani et al., 2017)  \n",
    "   ‚Üí Architecture Transformer originale\n",
    "\n",
    "2. **Language Models are Unsupervised Multitask Learners** (Radford et al., 2019)  \n",
    "   ‚Üí GPT-2, pr√©-entra√Ænement √† grande √©chelle\n",
    "\n",
    "3. **Decoupled Weight Decay Regularization** (Loshchilov & Hutter, 2019)  \n",
    "   ‚Üí AdamW optimizer\n",
    "\n",
    "4. **On Layer Normalization in the Transformer Architecture** (Xiong et al., 2020)  \n",
    "   ‚Üí Pre-LN vs Post-LN\n",
    "\n",
    "5. **Training Compute-Optimal Large Language Models** (Hoffmann et al., 2022)  \n",
    "   ‚Üí Chinchilla scaling laws\n",
    "\n",
    "#### Code Bases Utiles\n",
    "\n",
    "- **nanoGPT** (Karpathy) : https://github.com/karpathy/nanoGPT\n",
    "- **minGPT** (Karpathy) : https://github.com/karpathy/minGPT\n",
    "- **GPT-Neo** (EleutherAI) : https://github.com/EleutherAI/gpt-neo\n",
    "- **Lit-GPT** (Lightning AI) : https://github.com/Lightning-AI/lit-gpt\n",
    "\n",
    "---\n",
    "\n",
    "### F√©licitations ! üéâ\n",
    "\n",
    "Vous avez maintenant compris et impl√©ment√© **toutes les √©tapes** du pr√©-entra√Ænement d'un LLM :\n",
    "\n",
    "‚úÖ Configuration et hyperparam√®tres  \n",
    "‚úÖ Learning rate scheduling  \n",
    "‚úÖ Tokenization et data loading  \n",
    "‚úÖ Architecture Transformer (decoder-only)  \n",
    "‚úÖ Training loop avec mixed precision  \n",
    "‚úÖ Sauvegarde et chargement de checkpoints  \n",
    "‚úÖ G√©n√©ration de texte auto-regressive  \n",
    "\n",
    "**Ce mod√®le est un point de d√©part** pour exp√©rimenter avec :\n",
    "- Diff√©rentes architectures (GQA, MoE, etc.)\n",
    "- Nouveaux datasets (code, math, multi-langue)\n",
    "- Techniques d'optimisation (Flash Attention, etc.)\n",
    "- Post-training (SFT, RLHF, DPO)\n",
    "\n",
    "Bon coding ! üêç‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e7be8",
   "metadata": {},
   "source": [
    "## üéØ Partie 13 : Test Rapide (Quick Generation)\n",
    "\n",
    "### Qu'est-ce qu'on fait ?\n",
    "\n",
    "Une **cellule ultra-rapide** pour tester la g√©n√©ration depuis le mod√®le d√©j√† charg√© en m√©moire.\n",
    "\n",
    "### Code Simple\n",
    "\n",
    "```python\n",
    "output = generate(model, tokenizer, \"def hello\", max_tokens=5)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "**Pas besoin de** :\n",
    "- ‚ùå Recharger le mod√®le\n",
    "- ‚ùå Recharger le tokenizer\n",
    "- ‚ùå Reconfigurer le device\n",
    "\n",
    "**Juste** :\n",
    "- ‚úÖ Appeler `generate()`\n",
    "- ‚úÖ Voir le r√©sultat instantan√©ment\n",
    "\n",
    "---\n",
    "\n",
    "### Cas d'Usage\n",
    "\n",
    "| Situation | Exemple | Utilit√© |\n",
    "|-----------|---------|---------|\n",
    "| **Test Rapide** | `\"def add\"` ‚Üí g√©n√®re 10 tokens | V√©rifier que g√©n√©ration fonctionne |\n",
    "| **Debug** | `\"import\"` ‚Üí voir imports pr√©dits | Debugger probl√®mes de g√©n√©ration |\n",
    "| **Comparaison** | Avant vs apr√®s entra√Ænement | Mesurer progr√®s du mod√®le |\n",
    "| **Demo Live** | Montrer g√©n√©ration en temps r√©el | Impressionner votre audience üòé |\n",
    "| **Exploration** | Diff√©rents prompts, temp√©ratures | Comprendre comportement du mod√®le |\n",
    "\n",
    "---\n",
    "\n",
    "### Exemples de Prompts Int√©ressants\n",
    "\n",
    "#### Code Python\n",
    "\n",
    "```python\n",
    "# Fonction simple\n",
    "generate(model, tokenizer, \"def factorial\", max_tokens=50)\n",
    "# ‚Üí \"def factorial(n):\\n    if n == 0:\\n        return 1\\n ...\"\n",
    "\n",
    "# Classe\n",
    "generate(model, tokenizer, \"class LinkedList\", max_tokens=40)\n",
    "# ‚Üí \"class LinkedList:\\n    def __init__(self):\\n        self.head = None\\n ...\"\n",
    "\n",
    "# Import\n",
    "generate(model, tokenizer, \"import\", max_tokens=10)\n",
    "# ‚Üí \"import numpy as np\\nimport pandas as pd\"\n",
    "\n",
    "# Commentaire\n",
    "generate(model, tokenizer, \"# This function\", max_tokens=20)\n",
    "# ‚Üí \"# This function calculates the sum of two numbers\\ndef add ...\"\n",
    "```\n",
    "\n",
    "#### Texte Naturel\n",
    "\n",
    "```python\n",
    "# Phrase simple\n",
    "generate(model, tokenizer, \"Machine learning is\", max_tokens=30)\n",
    "# ‚Üí \"Machine learning is a subset of AI that focuses on ...\"\n",
    "\n",
    "# Question\n",
    "generate(model, tokenizer, \"What is Python?\", max_tokens=40)\n",
    "# ‚Üí \"What is Python? Python is a high-level programming language ...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Variations Utiles\n",
    "\n",
    "#### 1. Temp√©rature Variable\n",
    "\n",
    "```python\n",
    "# Greedy (temp√©rature = 0) : d√©terministe\n",
    "output_greedy = generate(model, tokenizer, \"def sort\", max_tokens=20, temperature=0.0)\n",
    "\n",
    "# Cr√©atif (temp√©rature √©lev√©e) : al√©atoire\n",
    "output_creative = generate(model, tokenizer, \"def sort\", max_tokens=20, temperature=1.2)\n",
    "\n",
    "print(\"Greedy:\", output_greedy)\n",
    "print(\"Creative:\", output_creative)\n",
    "```\n",
    "\n",
    "#### 2. Longueur Variable\n",
    "\n",
    "```python\n",
    "# Court (5 tokens)\n",
    "short = generate(model, tokenizer, \"def\", max_tokens=5)\n",
    "\n",
    "# Moyen (50 tokens)\n",
    "medium = generate(model, tokenizer, \"def\", max_tokens=50)\n",
    "\n",
    "# Long (200 tokens)\n",
    "long = generate(model, tokenizer, \"def\", max_tokens=200)\n",
    "```\n",
    "\n",
    "#### 3. Multiple Generations\n",
    "\n",
    "```python\n",
    "# G√©n√©rer 5 versions diff√©rentes\n",
    "for i in range(5):\n",
    "    output = generate(model, tokenizer, \"def calculate\", max_tokens=30, temperature=0.8)\n",
    "    print(f\"\\n=== Sample {i+1} ===\\n{output}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Interpr√©tation des R√©sultats\n",
    "\n",
    "#### Bon Signe ‚úÖ\n",
    "\n",
    "```python\n",
    "Prompt: \"def fibonacci\"\n",
    "Output: \"def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\"\n",
    "```\n",
    "\n",
    "**Pourquoi** :\n",
    "- Syntaxe Python correcte\n",
    "- Logique coh√©rente (r√©cursion)\n",
    "- Nommage appropri√© (n)\n",
    "- Indentation respect√©e\n",
    "\n",
    "#### Mauvais Signe ‚ùå\n",
    "\n",
    "```python\n",
    "Prompt: \"def fibonacci\"\n",
    "Output: \"def fibonacci fibonacci def def def import class while ...\"\n",
    "```\n",
    "\n",
    "**Pourquoi** :\n",
    "- R√©p√©tition excessive\n",
    "- Pas de structure coh√©rente\n",
    "- Tokens al√©atoires\n",
    "\n",
    "**Solutions** :\n",
    "- ‚Üë Training steps (mod√®le sous-entra√Æn√©)\n",
    "- ‚Üì Temperature (g√©n√©ration trop al√©atoire)\n",
    "- V√©rifier donn√©es d'entra√Ænement (qualit√©)\n",
    "\n",
    "---\n",
    "\n",
    "### Debugging Quick Tips\n",
    "\n",
    "| Probl√®me | Solution Rapide |\n",
    "|----------|-----------------|\n",
    "| **G√©n√©ration vide** | V√©rifier que model et tokenizer sont charg√©s |\n",
    "| **Erreur CUDA** | `model.to(\"cpu\")` puis r√©essayer |\n",
    "| **Tokens √©tranges** | V√©rifier prompt (doit √™tre du texte valide) |\n",
    "| **Trop lent** | ‚Üì max_tokens ou utiliser `device=\"cuda\"` |\n",
    "| **R√©p√©titions** | ‚Üì temperature ou utiliser top-k sampling |\n",
    "\n",
    "---\n",
    "\n",
    "### Extensions Possibles\n",
    "\n",
    "#### 1. Batch Generation\n",
    "\n",
    "```python\n",
    "prompts = [\"def add\", \"class Node\", \"import torch\"]\n",
    "for prompt in prompts:\n",
    "    output = generate(model, tokenizer, prompt, max_tokens=20)\n",
    "    print(f\"{prompt} ‚Üí {output}\\n\")\n",
    "```\n",
    "\n",
    "#### 2. Avec Statistiques\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "output = generate(model, tokenizer, \"def sort\", max_tokens=100)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "tokens_generated = len(tokenizer.encode(output)) - len(tokenizer.encode(\"def sort\"))\n",
    "speed = tokens_generated / elapsed\n",
    "\n",
    "print(f\"Generated: {tokens_generated} tokens in {elapsed:.2f}s ({speed:.1f} tok/s)\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "```\n",
    "\n",
    "#### 3. Sauvegarder les Samples\n",
    "\n",
    "```python\n",
    "samples = []\n",
    "for i in range(10):\n",
    "    output = generate(model, tokenizer, \"def\", max_tokens=50, temperature=0.9)\n",
    "    samples.append(output)\n",
    "\n",
    "# Sauvegarder dans fichier\n",
    "with open(\"generated_samples.txt\", \"w\") as f:\n",
    "    for i, sample in enumerate(samples):\n",
    "        f.write(f\"=== Sample {i+1} ===\\n{sample}\\n\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Comparaison : Avant vs Apr√®s Entra√Ænement\n",
    "\n",
    "```python\n",
    "# Charger mod√®le initial (random weights)\n",
    "model_init = TinyDecoderLM(cfg).to(device)\n",
    "\n",
    "# G√©n√©rer avec mod√®le non-entra√Æn√©\n",
    "output_random = generate(model_init, tokenizer, \"def hello\", max_tokens=30)\n",
    "print(\"AVANT entra√Ænement:\", output_random)\n",
    "\n",
    "# G√©n√©rer avec mod√®le entra√Æn√©\n",
    "output_trained = generate(model, tokenizer, \"def hello\", max_tokens=30)\n",
    "print(\"APR√àS entra√Ænement:\", output_trained)\n",
    "```\n",
    "\n",
    "**R√©sultat attendu** :\n",
    "- **Avant** : Garbage (tokens al√©atoires)\n",
    "- **Apr√®s** : Code Python syntaxiquement valide\n",
    "\n",
    "---\n",
    "\n",
    "### C'est Tout ! üéâ\n",
    "\n",
    "Cette cellule permet de **tester rapidement** la g√©n√©ration pendant le d√©veloppement.\n",
    "\n",
    "**Workflow typique** :\n",
    "1. Entra√Æner mod√®le (ou charger checkpoint)\n",
    "2. Lancer cette cellule\n",
    "3. Voir r√©sultat imm√©diatement\n",
    "4. Ajuster temp√©rature/longueur si besoin\n",
    "5. R√©p√©ter !\n",
    "\n",
    "**Bon testing !** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
