{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867c1463",
   "metadata": {},
   "source": [
    "# üß† Workshop: Build a Coding LLM from Scratch\n",
    "## Part IV: Post-Training (Supervised Fine-Tuning)\n",
    "### üéØ Focus: Teaching the Model to Follow Instructions with Reasoning\n",
    "\n",
    "**Auteur :** √âquipe IRA\n",
    "\n",
    "**Date :** 1 D√©cembre 2025\n",
    "\n",
    "**Contexte :** Ce notebook d√©montre le **Post-Training** d'un mod√®le pr√©-entra√Æn√© pour qu'il suive des instructions et g√©n√®re du code avec raisonnement. Nous utilisons un dataset structur√© instruction‚Üíreasoning‚Üícode pour apprendre au mod√®le √† comprendre les consignes et √† raisonner avant de coder.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "1. **Introduction th√©orique** : Qu'est-ce que le Post-Training ?\n",
    "2. **R√©cup√©ration des artefacts du Pre-Training**\n",
    "3. **Chargement et exploration du dataset SFT**\n",
    "4. **Pr√©paration et encodage des donn√©es**\n",
    "5. **Chargement du mod√®le pr√©-entra√Æn√©**\n",
    "6. **Boucle de Post-Training (SFT)**\n",
    "7. **√âvaluation et tests**\n",
    "8. **Sauvegarde finale**\n",
    "9. **Comparaison Pre-Training vs Post-Training**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9b8e5",
   "metadata": {},
   "source": [
    "## üîπ Partie 1 : Introduction Th√©orique\n",
    "\n",
    "### Qu'est-ce que le Post-Training ?\n",
    "\n",
    "Le **Post-Training** (ou **Supervised Fine-Tuning - SFT**) est la phase o√π un mod√®le pr√©-entra√Æn√© apprend √† suivre des instructions sp√©cifiques.\n",
    "\n",
    "### Diff√©rence Pre-Training vs Post-Training\n",
    "\n",
    "| Aspect | Pre-Training | Post-Training (SFT) |\n",
    "|--------|--------------|---------------------|\n",
    "| **Donn√©es** | Code brut (non structur√©) | Paires instruction‚Üícode structur√©es |\n",
    "| **Objectif** | Apprendre la syntaxe Python | Suivre des consignes pr√©cises |\n",
    "| **Format** | Texte continu | Format question-r√©ponse |\n",
    "| **Exemple entr√©e** | `def fibonacci(n):...` | `\"√âcris une fonction fibonacci\"` |\n",
    "| **Exemple sortie** | Token suivant | Fonction compl√®te avec raisonnement |\n",
    "\n",
    "### Notre Dataset SFT\n",
    "\n",
    "Format : **Instruction ‚Üí Reasoning ‚Üí Code**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Return sum of even numbers up to n\",\n",
    "  \"reasoning\": \"Iterate and sum numbers divisible by 2\",\n",
    "  \"answer\": \"def sum_even_1_to_n(n):\\n    return sum(i for i in range(2, n+1, 2))\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- ‚úÖ Le mod√®le apprend √† **comprendre les consignes**\n",
    "- ‚úÖ Le mod√®le apprend √† **raisonner** avant de coder\n",
    "- ‚úÖ Le code g√©n√©r√© est **align√©** avec les besoins humains\n",
    "\n",
    "### Pipeline Post-Training\n",
    "\n",
    "```\n",
    "Mod√®le Pr√©-entra√Æn√© ‚Üí SFT Dataset ‚Üí Fine-Tuning ‚Üí Mod√®le Instruit\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f46bec",
   "metadata": {},
   "source": [
    "## üîπ Partie 2 : R√©cup√©ration des Artefacts du Pre-Training\n",
    "\n",
    "Nous r√©cup√©rons le mod√®le et le tokenizer cr√©√©s dans le notebook **Pre-Training**.\n",
    "\n",
    "### Fichiers n√©cessaires :\n",
    "- ‚úÖ `models/pre_training/mini_gpt_code_FINAL.pt` - Mod√®le pr√©-entra√Æn√©\n",
    "- ‚úÖ `models/pre_training/tokenizer/` - Tokenizer GPT-2\n",
    "- ‚úÖ Architecture `MiniGPT` (d√©finie dans le notebook pr√©c√©dent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports et Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DES BIBLIOTH√àQUES N√âCESSAIRES\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION POUR LA REPRODUCTIBILIT√â\n",
    "# ============================================================================\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# D√âTECTION DU DEVICE\n",
    "# ============================================================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üöÄ Device utilis√© : {device}\")\n",
    "print(f\"üî• PyTorch version : {torch.__version__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAM√àTRES DU POST-TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# V√©rifier que les fichiers du pre-training existent\n",
    "required_files = [\n",
    "    \"models/pre_training/mini_gpt_code_FINAL.pt\",\n",
    "    \"models/pre_training/tokenizer/tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìÇ V√©rification des fichiers du Pre-Training...\")\n",
    "all_exist = True\n",
    "for file in required_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"   {status} {file}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n‚ö†Ô∏è  ATTENTION : Certains fichiers du Pre-Training sont manquants!\")\n",
    "    print(\"   Veuillez d'abord ex√©cuter le notebook 'notebook.ipynb' (Pre-Training)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Tous les fichiers n√©cessaires sont pr√©sents!\")\n",
    "\n",
    "# Hyperparam√®tres sp√©cifiques au Post-Training\n",
    "BATCH_SIZE = 8           # Plus petit batch pour SFT (donn√©es plus riches)\n",
    "N_EPOCHS = 5             # Plus d'√©poques pour bien apprendre les instructions\n",
    "LEARNING_RATE = 1e-4     # Learning rate plus faible (fine-tuning)\n",
    "MAX_LENGTH = 256         # Longueur max des s√©quences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1fadd",
   "metadata": {},
   "source": [
    "## üîπ Partie 3 : Chargement et Exploration du Dataset SFT\n",
    "\n",
    "Notre dataset contient **10,000 exemples** de code Python avec instructions et raisonnement.\n",
    "\n",
    "### Structure des donn√©es :\n",
    "- **instruction** : Ce que l'utilisateur demande\n",
    "- **reasoning** : Le raisonnement pour r√©soudre le probl√®me\n",
    "- **answer** : Le code Python correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04491363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Chargement du Dataset SFT\n",
    "\n",
    "print(\"üì• Chargement du dataset SFT (data/python_reasoning_dataset.jsonl)...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGEMENT DU FICHIER JSONL\n",
    "# ============================================================================\n",
    "# Chaque ligne est un objet JSON\n",
    "sft_data = []\n",
    "dataset_path = \"data/python_reasoning_dataset.jsonl\"\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Parser chaque ligne JSON\n",
    "        example = json.loads(line.strip())\n",
    "        sft_data.append(example)\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√© : {len(sft_data):,} exemples\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPLORATION DU DATASET\n",
    "# ============================================================================\n",
    "print(\"\\nüìä Statistiques du dataset :\")\n",
    "\n",
    "# Compter les longueurs\n",
    "instruction_lengths = [len(ex['instruction']) for ex in sft_data]\n",
    "reasoning_lengths = [len(ex['reasoning']) for ex in sft_data]\n",
    "answer_lengths = [len(ex['answer']) for ex in sft_data]\n",
    "\n",
    "print(f\"   - Instruction moyenne : {np.mean(instruction_lengths):.1f} caract√®res\")\n",
    "print(f\"   - Reasoning moyen     : {np.mean(reasoning_lengths):.1f} caract√®res\")\n",
    "print(f\"   - Answer moyen        : {np.mean(answer_lengths):.1f} caract√®res\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "print(\"\\n--- üìã Exemples du dataset ---\\n\")\n",
    "for i in range(3):\n",
    "    ex = sft_data[i]\n",
    "    print(f\"Exemple {i+1}:\")\n",
    "    print(f\"  Instruction: {ex['instruction']}\")\n",
    "    print(f\"  Reasoning:   {ex['reasoning']}\")\n",
    "    print(f\"  Answer:      {ex['answer'][:100]}...\")  # Tronquer pour affichage\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd9890",
   "metadata": {},
   "source": [
    "## üîπ Partie 4 : Pr√©paration et Encodage des Donn√©es\n",
    "\n",
    "### Format d'entra√Ænement\n",
    "\n",
    "Nous cr√©ons un format structur√© pour que le mod√®le apprenne :\n",
    "```\n",
    "<instruction> {instruction} <reasoning> {reasoning} <answer> {answer}\n",
    "```\n",
    "\n",
    "Ce format permet au mod√®le de distinguer les diff√©rentes parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c923554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Chargement du Tokenizer\n",
    "\n",
    "print(\"üî§ Chargement du tokenizer pr√©-entra√Æn√©...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGER LE TOKENIZER SAUVEGARD√â\n",
    "# ============================================================================\n",
    "# Utiliser le m√™me tokenizer que le Pre-Training\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/pre_training/tokenizer\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"‚úÖ Tokenizer charg√© (vocabulaire : {vocab_size:,} tokens)\")\n",
    "\n",
    "# ============================================================================\n",
    "# AJOUTER DES TOKENS SP√âCIAUX POUR LE SFT\n",
    "# ============================================================================\n",
    "# Tokens sp√©ciaux pour structurer le format instruction-reasoning-answer\n",
    "special_tokens = {\n",
    "    'additional_special_tokens': ['<instruction>', '<reasoning>', '<answer>']\n",
    "}\n",
    "\n",
    "num_added = tokenizer.add_special_tokens(special_tokens)\n",
    "print(f\"‚úÖ {num_added} tokens sp√©ciaux ajout√©s\")\n",
    "print(f\"üìö Nouvelle taille du vocabulaire : {len(tokenizer):,}\")\n",
    "\n",
    "# Afficher les IDs des nouveaux tokens\n",
    "print(\"\\nüîñ Tokens sp√©ciaux :\")\n",
    "print(f\"   <instruction> ‚Üí ID {tokenizer.encode('<instruction>', add_special_tokens=False)[0]}\")\n",
    "print(f\"   <reasoning>   ‚Üí ID {tokenizer.encode('<reasoning>', add_special_tokens=False)[0]}\")\n",
    "print(f\"   <answer>      ‚Üí ID {tokenizer.encode('<answer>', add_special_tokens=False)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Cr√©ation du Dataset PyTorch pour SFT\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE DATASET POUR POST-TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour Supervised Fine-Tuning\n",
    "    Encode les exemples au format: <instruction> ... <reasoning> ... <answer> ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): Liste de dictionnaires {instruction, reasoning, answer}\n",
    "            tokenizer: Tokenizer GPT-2\n",
    "            max_length (int): Longueur maximale des s√©quences\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retourne une s√©quence encod√©e pour l'entra√Ænement\n",
    "        \"\"\"\n",
    "        example = self.data[idx]\n",
    "        \n",
    "        # ====================================================================\n",
    "        # FORMAT: <instruction> X <reasoning> Y <answer> Z\n",
    "        # ====================================================================\n",
    "        text = (\n",
    "            f\"<instruction> {example['instruction']} \"\n",
    "            f\"<reasoning> {example['reasoning']} \"\n",
    "            f\"<answer> {example['answer']}\"\n",
    "        )\n",
    "        \n",
    "        # Encoder le texte complet\n",
    "        encoded = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Tronquer ou padder √† max_length\n",
    "        if len(encoded) > self.max_length:\n",
    "            encoded = encoded[:self.max_length]\n",
    "        else:\n",
    "            # Padding avec eos_token\n",
    "            encoded = encoded + [self.tokenizer.eos_token_id] * (self.max_length - len(encoded))\n",
    "        \n",
    "        # Cr√©er input et target (d√©cal√© de 1 pour CLM)\n",
    "        input_ids = torch.tensor(encoded[:-1], dtype=torch.long)   # Tous sauf le dernier\n",
    "        target_ids = torch.tensor(encoded[1:], dtype=torch.long)   # Tous sauf le premier\n",
    "        \n",
    "        return input_ids, target_ids\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âER LES DATASETS TRAIN/VAL\n",
    "# ============================================================================\n",
    "print(\"üìÇ Cr√©ation des datasets SFT...\")\n",
    "\n",
    "# Split 90/10\n",
    "split_idx = int(0.9 * len(sft_data))\n",
    "train_data = sft_data[:split_idx]\n",
    "val_data = sft_data[split_idx:]\n",
    "\n",
    "print(f\"   - Train : {len(train_data):,} exemples\")\n",
    "print(f\"   - Val   : {len(val_data):,} exemples\")\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = SFTDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = SFTDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Cr√©er les DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets cr√©√©s\")\n",
    "print(f\"üì¶ Train batches : {len(train_loader)}\")\n",
    "print(f\"üì¶ Val batches   : {len(val_loader)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST D'ENCODAGE\n",
    "# ============================================================================\n",
    "print(\"\\n--- üß™ Test d'encodage ---\")\n",
    "test_input, test_target = train_dataset[0]\n",
    "print(f\"Shape input  : {test_input.shape}\")\n",
    "print(f\"Shape target : {test_target.shape}\")\n",
    "print(f\"\\nD√©codage de l'input :\")\n",
    "print(tokenizer.decode(test_input.tolist())[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d544363a",
   "metadata": {},
   "source": [
    "## üîπ Partie 5 : Chargement du Mod√®le Pr√©-Entra√Æn√©\n",
    "\n",
    "Nous chargeons le **MiniGPT pr√©-entra√Æn√©** depuis le checkpoint sauvegard√©.\n",
    "\n",
    "‚ö†Ô∏è **Important** : Nous devons **red√©finir l'architecture** car elle n'est pas sauvegard√©e dans le checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52233101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Red√©finition de l'Architecture MiniGPT\n",
    "\n",
    "# ============================================================================\n",
    "# COPIE DE L'ARCHITECTURE DU PRE-TRAINING\n",
    "# ============================================================================\n",
    "# On doit red√©finir toutes les classes car elles ne sont pas dans le checkpoint\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention avec masque causal\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                     .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Bloc Transformer complet\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = MLP(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"Mini GPT pour g√©n√©ration de code\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size, dropout) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "print(\"‚úÖ Architecture MiniGPT red√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ea921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Chargement du Mod√®le Pr√©-Entra√Æn√©\n",
    "\n",
    "print(\"üì• Chargement du mod√®le pr√©-entra√Æn√©...\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGER LE CHECKPOINT\n",
    "# ============================================================================\n",
    "checkpoint = torch.load(\"models/pre_training/mini_gpt_code_FINAL.pt\", map_location=device)\n",
    "\n",
    "# R√©cup√©rer la configuration\n",
    "config = checkpoint['config']\n",
    "print(f\"\\nüìä Configuration du mod√®le :\")\n",
    "print(f\"   - Vocabulaire   : {config['vocab_size']:,}\")\n",
    "print(f\"   - Block size    : {config['block_size']}\")\n",
    "print(f\"   - Embeddings    : {config['n_embd']}\")\n",
    "print(f\"   - Attention heads: {config['n_head']}\")\n",
    "print(f\"   - Layers        : {config['n_layer']}\")\n",
    "print(f\"   - Dropout       : {config['dropout']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANCIER LE MOD√àLE AVEC LA NOUVELLE TAILLE DE VOCABULAIRE\n",
    "# ============================================================================\n",
    "# IMPORTANT: Le vocabulaire a augment√© avec les tokens sp√©ciaux !\n",
    "new_vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"\\nüîß Ajustement du vocabulaire :\")\n",
    "print(f\"   - Ancien vocabulaire : {config['vocab_size']:,}\")\n",
    "print(f\"   - Nouveau vocabulaire: {new_vocab_size:,}\")\n",
    "print(f\"   - Tokens ajout√©s     : {new_vocab_size - config['vocab_size']}\")\n",
    "\n",
    "# Cr√©er le mod√®le avec le NOUVEAU vocabulaire\n",
    "model = MiniGPT(\n",
    "    vocab_size=new_vocab_size,  # ‚Üê Nouvelle taille !\n",
    "    block_size=config['block_size'],\n",
    "    n_embd=config['n_embd'],\n",
    "    n_head=config['n_head'],\n",
    "    n_layer=config['n_layer'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGER LES POIDS PR√â-ENTRA√éN√âS\n",
    "# ============================================================================\n",
    "# Les embeddings ont une taille diff√©rente, on doit les ajuster\n",
    "pretrained_state = checkpoint['model_state_dict']\n",
    "\n",
    "# R√©cup√©rer les anciens embeddings\n",
    "old_token_emb = pretrained_state['token_embedding.weight']\n",
    "old_vocab_size, emb_dim = old_token_emb.shape\n",
    "\n",
    "# Cr√©er de nouveaux embeddings (avec les tokens sp√©ciaux)\n",
    "new_token_emb = model.token_embedding.weight.data.clone()\n",
    "\n",
    "# Copier les anciens poids\n",
    "new_token_emb[:old_vocab_size] = old_token_emb\n",
    "\n",
    "# Mettre √† jour le state_dict\n",
    "pretrained_state['token_embedding.weight'] = new_token_emb\n",
    "pretrained_state['lm_head.weight'] = new_token_emb  # Weight tying\n",
    "\n",
    "# Charger les poids\n",
    "model.load_state_dict(pretrained_state, strict=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Mod√®le charg√© avec {sum(p.numel() for p in model.parameters()):,} param√®tres\")\n",
    "print(f\"üìà Validation loss du pre-training : {checkpoint.get('best_val_loss', 'N/A')}\")\n",
    "\n",
    "# Mettre en mode entra√Ænement\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57591c",
   "metadata": {},
   "source": [
    "## üîπ Partie 6 : Boucle de Post-Training (SFT)\n",
    "\n",
    "Nous fine-tunons le mod√®le sur les donn√©es d'instructions.\n",
    "\n",
    "### Strat√©gie d'entra√Ænement :\n",
    "- Learning rate plus faible (1e-4 vs 3e-4 en pre-training)\n",
    "- Plus d'√©poques (5 vs 3)\n",
    "- Monitoring de la qualit√© des r√©ponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70394657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Configuration de l'Optimisation\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZER ET SCHEDULER\n",
    "# ============================================================================\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE,  # 1e-4 (plus faible que pre-training)\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "total_steps = len(train_loader) * N_EPOCHS\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION D'√âVALUATION\n",
    "# ============================================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, max_batches=50):\n",
    "    \"\"\"Calcule la loss moyenne sur la validation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(val_loader):\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / count if count > 0 else 0\n",
    "\n",
    "# ============================================================================\n",
    "# HISTORIQUE DES M√âTRIQUES\n",
    "# ============================================================================\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'epochs': []\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration de l'optimisation termin√©e\")\n",
    "print(f\"üìä Total steps : {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Boucle de Post-Training\n",
    "\n",
    "print(\"üöÄ D√©but du Post-Training (SFT)...\")\n",
    "print(f\"üìä Configuration: {N_EPOCHS} √©poques, {len(train_loader)} batches/√©poque\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# BOUCLE D'ENTRA√éNEMENT\n",
    "# ============================================================================\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÖ √âpoque {epoch+1}/{N_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"SFT Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        # D√©placer sur device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Logging\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "    \n",
    "    # M√©triques de fin d'√©poque\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    \n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epochs'].append(epoch + 1)\n",
    "    \n",
    "    print(f\"\\nüìä Fin √âpoque {epoch+1}:\")\n",
    "    print(f\"   - Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   - Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"   - Perplexity: {np.exp(val_loss):.2f}\")\n",
    "    \n",
    "    # Test de g√©n√©ration\n",
    "    print(f\"\\nüéØ Test de g√©n√©ration (epoch {epoch+1}):\")\n",
    "    test_prompt = \"<instruction> Write a function to calculate factorial <reasoning>\"\n",
    "    test_ids = torch.tensor([tokenizer.encode(test_prompt)], device=device)\n",
    "    generated = model.generate(test_ids, max_new_tokens=100, temperature=0.7, top_k=50)\n",
    "    print(tokenizer.decode(generated[0].tolist()))\n",
    "    print()\n",
    "    \n",
    "    # Sauvegarde du checkpoint\n",
    "    os.makedirs(\"checkpoints_sft\", exist_ok=True)\n",
    "    checkpoint_sft = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history,\n",
    "        'config': {\n",
    "            'vocab_size': new_vocab_size,\n",
    "            'block_size': config['block_size'],\n",
    "            'n_embd': config['n_embd'],\n",
    "            'n_head': config['n_head'],\n",
    "            'n_layer': config['n_layer'],\n",
    "            'dropout': config['dropout']\n",
    "        }\n",
    "    }\n",
    "    checkpoint_path = f\"models/post_training/mini_gpt_sft_epoch_{epoch+1}.pt\"\n",
    "    torch.save(checkpoint_sft, checkpoint_path)\n",
    "    print(f\"üíæ Checkpoint SFT sauvegard√© : {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Post-Training termin√© !\")\n",
    "print(f\"üìÅ {N_EPOCHS} checkpoints SFT sauvegard√©s dans models/post_training/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ae99b",
   "metadata": {},
   "source": [
    "## üîπ Partie 7 : Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: Visualisation de la Loss\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Courbe de loss\n",
    "axes[0].plot(history['epochs'], history['train_loss'], marker='o', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['epochs'], history['val_loss'], marker='s', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('üìâ Courbe d\\'Apprentissage (Post-Training)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "perplexity_train = [np.exp(loss) for loss in history['train_loss']]\n",
    "perplexity_val = [np.exp(loss) for loss in history['val_loss']]\n",
    "axes[1].plot(history['epochs'], perplexity_train, marker='o', label='Train Perplexity', linewidth=2)\n",
    "axes[1].plot(history['epochs'], perplexity_val, marker='s', label='Val Perplexity', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[1].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[1].set_title('üìä Perplexit√© (Post-Training)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse\n",
    "print(\"\\nüìà Analyse des r√©sultats Post-Training:\")\n",
    "improvement_train = history['train_loss'][0] - history['train_loss'][-1]\n",
    "improvement_val = history['val_loss'][0] - history['val_loss'][-1]\n",
    "print(f\"   - Am√©lioration train: {improvement_train:.4f}\")\n",
    "print(f\"   - Am√©lioration val:   {improvement_val:.4f}\")\n",
    "print(f\"   - Gap train/val:      {history['val_loss'][-1] - history['train_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb79314",
   "metadata": {},
   "source": [
    "## üîπ Partie 8 : Tests de G√©n√©ration avec Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7de34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Tests de G√©n√©ration\n",
    "\n",
    "def generate_from_instruction(instruction, max_tokens=150, temperature=0.7, top_k=40):\n",
    "    \"\"\"G√©n√®re du code √† partir d'une instruction\"\"\"\n",
    "    model.eval()\n",
    "    prompt = f\"<instruction> {instruction} <reasoning>\"\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_tokens, \n",
    "            temperature=temperature, \n",
    "            top_k=top_k\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(output_ids[0].tolist())\n",
    "    model.train()\n",
    "    return generated\n",
    "\n",
    "# Tests\n",
    "test_instructions = [\n",
    "    \"Write a function to check if a number is prime\",\n",
    "    \"Create a function to reverse a list\",\n",
    "    \"Implement binary search algorithm\",\n",
    "    \"Write a function to calculate Fibonacci sequence\",\n",
    "]\n",
    "\n",
    "print(\"üéØ TESTS DE G√âN√âRATION AVEC INSTRUCTIONS\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, instruction in enumerate(test_instructions, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Test {i}: {instruction}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    result = generate_from_instruction(instruction, max_tokens=200, temperature=0.7)\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Tests de g√©n√©ration termin√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc58176",
   "metadata": {},
   "source": [
    "## üîπ Partie 9 : Sauvegarde Finale du Mod√®le Post-Entra√Æn√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7408ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 11: Sauvegarde Finale\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAUVEGARDE FINALE DU MOD√àLE POST-ENTRA√éN√â\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyser les checkpoints SFT\n",
    "print(\"\\nüìä Analyse des checkpoints SFT...\")\n",
    "best_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    checkpoint_path = f\"models/post_training/mini_gpt_sft_epoch_{epoch}.pt\"\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ckpt = torch.load(checkpoint_path)\n",
    "        val_loss = ckpt['history']['val_loss'][-1]\n",
    "        print(f\"   √âpoque {epoch}: Val Loss = {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le SFT : √âpoque {best_epoch} (Val Loss = {best_val_loss:.4f})\")\n",
    "\n",
    "# Charger le meilleur checkpoint\n",
    "best_checkpoint_path = f\"models/post_training/mini_gpt_sft_epoch_{best_epoch}.pt\"\n",
    "best_checkpoint = torch.load(best_checkpoint_path)\n",
    "\n",
    "# Sauvegarder le mod√®le final\n",
    "final_model_path = \"models/post_training/mini_gpt_sft_FINAL.pt\"\n",
    "torch.save({\n",
    "    'epoch': best_checkpoint['epoch'],\n",
    "    'model_state_dict': best_checkpoint['model_state_dict'],\n",
    "    'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],\n",
    "    'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],\n",
    "    'history': best_checkpoint['history'],\n",
    "    'config': best_checkpoint['config'],\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'selected_from_epoch': best_epoch,\n",
    "    'training_stage': 'post-training'\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"üíæ Mod√®le final SFT sauvegard√© : {final_model_path}\")\n",
    "\n",
    "# Sauvegarder les poids seuls\n",
    "model_weights_path = \"models/post_training/mini_gpt_sft_weights_only.pt\"\n",
    "torch.save(best_checkpoint['model_state_dict'], model_weights_path)\n",
    "print(f\"‚ö° Poids seuls sauvegard√©s : {model_weights_path}\")\n",
    "\n",
    "# Sauvegarder le tokenizer mis √† jour\n",
    "tokenizer.save_pretrained(\"models/post_training/tokenizer\")\n",
    "print(f\"üî§ Tokenizer mis √† jour sauvegard√© : models/post_training/tokenizer/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ R√âSUM√â DES ARTEFACTS CR√â√âS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Checkpoints SFT : models/post_training/mini_gpt_sft_epoch_[1-{N_EPOCHS}].pt\")\n",
    "print(f\"‚úÖ Mod√®le final SFT: {final_model_path}\")\n",
    "print(f\"‚úÖ Poids seuls     : {model_weights_path}\")\n",
    "print(f\"‚úÖ Tokenizer SFT   : models/post_training/tokenizer/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìå UTILISATION DU MOD√àLE POST-ENTRA√éN√â\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n# Charger le mod√®le SFT\")\n",
    "print(\"checkpoint = torch.load('models/post_training/mini_gpt_sft_FINAL.pt')\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"tokenizer = GPT2Tokenizer.from_pretrained('models/post_training/tokenizer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff28715",
   "metadata": {},
   "source": [
    "## üîπ Partie 10 : Comparaison Pre-Training vs Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 12: Comparaison des Mod√®les\n",
    "\n",
    "print(\"üìä COMPARAISON PRE-TRAINING vs POST-TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger les m√©triques du pre-training\n",
    "pretrain_checkpoint = torch.load(\"models/pre_training/mini_gpt_code_FINAL.pt\")\n",
    "pretrain_val_loss = pretrain_checkpoint.get('best_val_loss', 'N/A')\n",
    "\n",
    "print(f\"\\nüìà M√©triques finales :\")\n",
    "print(f\"\\nPre-Training (Base Model):\")\n",
    "print(f\"   - Validation Loss : {pretrain_val_loss}\")\n",
    "print(f\"   - Objectif        : Apprendre la syntaxe Python\")\n",
    "print(f\"   - Dataset         : Code brut (100k documents)\")\n",
    "\n",
    "print(f\"\\nPost-Training (SFT Model):\")\n",
    "print(f\"   - Validation Loss : {best_val_loss:.4f}\")\n",
    "print(f\"   - Objectif        : Suivre des instructions\")\n",
    "print(f\"   - Dataset         : Paires instruction-code (10k exemples)\")\n",
    "\n",
    "print(f\"\\nüéØ Am√©lioration :\")\n",
    "if isinstance(pretrain_val_loss, float):\n",
    "    improvement = pretrain_val_loss - best_val_loss\n",
    "    print(f\"   - R√©duction de loss : {improvement:.4f} ({improvement/pretrain_val_loss*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Post-Training termin√© avec succ√®s !\")\n",
    "print(\"üéâ Le mod√®le peut maintenant suivre des instructions et g√©n√©rer du code structur√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e6632",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ R√©sum√© du Post-Training\n",
    "\n",
    "### ‚úÖ Objectifs Accomplis\n",
    "\n",
    "1. **R√©cup√©ration** : Mod√®le et tokenizer du Pre-Training charg√©s\n",
    "2. **Dataset SFT** : 10,000 exemples instruction‚Üíreasoning‚Üícode charg√©s\n",
    "3. **Tokens sp√©ciaux** : `<instruction>`, `<reasoning>`, `<answer>` ajout√©s\n",
    "4. **Fine-Tuning** : 5 √©poques d'entra√Ænement supervis√©\n",
    "5. **Sauvegarde** : Meilleur mod√®le SFT sauvegard√©\n",
    "\n",
    "### üìä Architecture Finale\n",
    "\n",
    "```\n",
    "Mini-GPT Post-Entra√Æn√©\n",
    "‚îú‚îÄ‚îÄ Vocabulaire : 50,260 tokens (GPT-2 + 3 tokens sp√©ciaux)\n",
    "‚îú‚îÄ‚îÄ Architecture : 4 layers, 4 heads, 256 dims\n",
    "‚îú‚îÄ‚îÄ Param√®tres  : ~0.X M\n",
    "‚îî‚îÄ‚îÄ Capacit√©s   : Suivre instructions, raisonner, coder\n",
    "```\n",
    "\n",
    "### üöÄ Prochaines √âtapes\n",
    "\n",
    "Le mod√®le peut maintenant √™tre utilis√© pour :\n",
    "- **G√©n√©ration de code** √† partir d'instructions naturelles\n",
    "- **RLHF** : Optimisation par feedback humain\n",
    "- **D√©ploiement** : API de g√©n√©ration de code\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Fichiers Cr√©√©s\n",
    "\n",
    "```\n",
    "models/post_training/\n",
    "‚îú‚îÄ‚îÄ mini_gpt_sft_epoch_[1-5].pt    # Checkpoints par √©poque\n",
    "‚îú‚îÄ‚îÄ mini_gpt_sft_FINAL.pt          # ‚úÖ Meilleur mod√®le (√† utiliser)\n",
    "‚îú‚îÄ‚îÄ mini_gpt_sft_weights_only.pt   # ‚úÖ Poids seuls (l√©ger)\n",
    "‚îî‚îÄ‚îÄ tokenizer/                      # ‚úÖ Tokenizer avec tokens sp√©ciaux\n",
    "    ‚îú‚îÄ‚îÄ tokenizer_config.json\n",
    "    ‚îú‚îÄ‚îÄ vocab.json\n",
    "    ‚îî‚îÄ‚îÄ merges.txt\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

