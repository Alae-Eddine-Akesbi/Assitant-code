{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c30243",
   "metadata": {},
   "source": [
    "# üß† Workshop: Build a Coding LLM from Scratch\n",
    "## Part V: Alignment (RLHF) - Optimizing the Model with Human Preferences\n",
    "### üéØ Focus: Learning from Human Feedback via Reward Modeling and PPO\n",
    "\n",
    "**Auteur :** √âquipe IRA\n",
    "\n",
    "**Date :** 1 D√©cembre 2025\n",
    "\n",
    "**Contexte :** Ce notebook impl√©mente la **stage 3** du pipeline d'entra√Ænement : l'**Alignment par RLHF (Reinforcement Learning from Human Feedback)**. Nous utilisons un mod√®le de r√©compense (reward model) entra√Æn√© sur les pr√©f√©rences humaines, puis optimisons le mod√®le SFT avec l'algorithme PPO pour g√©n√©rer du code align√© avec les pr√©f√©rences humaines.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "1. **Introduction au RLHF**\n",
    "2. **Chargement du mod√®le post-entra√Æn√©**\n",
    "3. **Architecture du mod√®le de r√©compense**\n",
    "4. **Cr√©ation du dataset de pr√©f√©rences**\n",
    "5. **Entra√Ænement du mod√®le de r√©compense**\n",
    "6. **Impl√©mentation de PPO**\n",
    "7. **Pipeline RLHF complet**\n",
    "8. **√âvaluation et comparaison**\n",
    "9. **Sauvegarde du mod√®le align√©**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93485a7b",
   "metadata": {},
   "source": [
    "## üîπ Partie 1 : Introduction au RLHF\n",
    "\n",
    "## üü¶ 1. Qu‚Äôest-ce que le RLHF et pourquoi en a-t-on besoin ?\n",
    "\n",
    "Le *Reinforcement Learning from Human Feedback (RLHF)* est une m√©thode con√ßue pour aligner un mod√®le de langage sur les pr√©f√©rences humaines.  \n",
    "Le pr√©-training apprend au mod√®le √† **pr√©dire le mot suivant**, mais cela ne garantit pas qu‚Äôil produise :\n",
    "\n",
    "- des r√©ponses utiles,\n",
    "- s√ªres,\n",
    "- coh√©rentes,\n",
    "- ou conformes aux attentes humaines.\n",
    "\n",
    "Le RLHF r√©pond √† trois objectifs cl√©s :\n",
    "\n",
    "1. **Identifier quelles r√©ponses les humains pr√©f√®rent** (via des comparaisons humaines).  \n",
    "2. **Former un Reward Model** capable d‚Äôattribuer un score √† une r√©ponse.  \n",
    "3. **Optimiser le mod√®le g√©n√©rateur** en maximisant ce score gr√¢ce au RL (souvent PPO).\n",
    "\n",
    "L‚Äôid√©e g√©n√©rale :  \n",
    "üëâ On transforme des pr√©f√©rences humaines en une fonction de r√©compense  \n",
    "üëâ Le mod√®le apprend √† maximiser cette r√©compense  \n",
    "üëâ Il devient plus align√©, plus utile et plus s√ªr\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ 2. O√π se situe le RLHF dans le pipeline LLM ?\n",
    "\n",
    "Le pipeline complet d‚Äôun grand mod√®le suit trois grandes phases :\n",
    "\n",
    "### **1) Pr√©-training**\n",
    "Le mod√®le apprend la structure du langage √† partir d‚Äô√©normes corpus.  \n",
    "‚û°Ô∏è Il devient bon en pr√©diction de tokens, mais pas en comportement utile.\n",
    "\n",
    "### **2) SFT (Supervised Fine-Tuning)**\n",
    "On le r√©entra√Æne avec des exemples humains ‚Äúbien formul√©s‚Äù.  \n",
    "‚û°Ô∏è Il commence √† suivre des instructions, mais sans compr√©hension de pr√©f√©rences.\n",
    "\n",
    "### **3) RLHF (Alignment)**\n",
    "C‚Äôest la phase finale, la plus avanc√©e.\n",
    "\n",
    "‚û°Ô∏è Objectif : am√©liorer la qualit√© des r√©ponses via une boucle RL + Reward Model  \n",
    "Le mod√®le apprend √† :\n",
    "\n",
    "- produire des r√©ponses plus utiles, plus s√ªres  \n",
    "- √©viter les comportements ind√©sirables  \n",
    "- rester proche du SFT gr√¢ce au **KL penalty** (√©viter le mod√®le trop ‚Äúcr√©atif‚Äù)\n",
    "\n",
    "---\n",
    "\n",
    "## üü¶ 3. √âtapes du RLHF\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "1. SFT Model (Baseline)\n",
    "   ‚Üì\n",
    "2. Collect Preferences : G√©n√©rer plusieurs completions et les faire √©valuer par des humains\n",
    "   ‚Üì\n",
    "3. Train Reward Model : Apprendre √† pr√©dire les pr√©f√©rences humaines\n",
    "   ‚Üì\n",
    "4. RL Fine-tuning (PPO) : Optimiser le mod√®le SFT pour maximiser les scores de r√©compense\n",
    "   ‚Üì\n",
    "5. Aligned Model (Final)\n",
    "```\n",
    "\n",
    "### Diff√©rence Pre-Training vs Post-Training vs Alignment\n",
    "\n",
    "| Aspect | Pre-Training | Post-Training (SFT) | Alignment (RLHF) |\n",
    "|--------|--------------|---------------------|------------------|\n",
    "| **Donn√©es** | Code brut (100k) | Instructions structur√©es (10k) | Paires de pr√©f√©rences (1k) |\n",
    "| **Objectif** | Syntaxe Python | Suivre consignes | Satisfaire pr√©f√©rences |\n",
    "| **Algorithme** | CLM (Causal LM) | SFT (Supervised) | RL (PPO) |\n",
    "| **M√©triques** | Perplexity | Perplexity + BLEU (Bilingual Evaluation Understudy)| Reward Score + KL divergence |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff652782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cuda\n",
      "üî• PyTorch version: 2.3.1\n",
      "‚úÖ Configuration RLHF charg√©e\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Imports et Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Fixer les graines al√©atoires pour la reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# D√©tection du mat√©riel (GPU si disponible, sinon CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üöÄ Device: {device}\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAM√àTRES RLHF\n",
    "# ============================================================================\n",
    "\n",
    "# --- Param√®tres du Reward Model ---\n",
    "REWARD_BATCH_SIZE = 4      # Taille du lot pour l'entra√Ænement du mod√®le de r√©compense\n",
    "REWARD_EPOCHS = 3          # Nombre d'√©poques pour le mod√®le de r√©compense\n",
    "REWARD_LR = 1e-4           # Taux d'apprentissage pour le mod√®le de r√©compense\n",
    "\n",
    "# --- Param√®tres PPO (Proximal Policy Optimization) ---\n",
    "PPO_EPOCHS = 2             # Nombre de passages PPO par batch de rollouts\n",
    "PPO_BATCH_SIZE = 4         # Taille du lot pour la mise √† jour PPO\n",
    "PPO_LR = 5e-6              # Taux d'apprentissage tr√®s faible pour ne pas d√©truire le mod√®le SFT\n",
    "GAMMA = 0.99               # Facteur d'actualisation (discount factor) pour les r√©compenses futures\n",
    "GAE_LAMBDA = 0.95          # Param√®tre pour Generalized Advantage Estimation (lissage des avantages)\n",
    "CLIP_RATIO = 0.2           # PPO Clip : limite √† quel point la politique peut changer (stabilit√©)\n",
    "ENTROPY_COEFF = 0.01       # Bonus d'entropie pour encourager l'exploration (√©viter le mode collapse)\n",
    "REWARD_SCALE = 1.0         # Mise √† l'√©chelle des r√©compenses brutes\n",
    "KL_COEFF = 0.5             # Coefficient de p√©nalit√© KL (force le mod√®le √† rester proche du SFT)\n",
    "\n",
    "# --- Param√®tres de G√©n√©ration ---\n",
    "MAX_NEW_TOKENS = 150       # Longueur maximale de la r√©ponse g√©n√©r√©e\n",
    "TEMPERATURE = 0.8          # Cr√©ativit√© (plus haut = plus al√©atoire)\n",
    "TOP_K = 40                 # Top-K sampling (limite aux K tokens les plus probables)\n",
    "\n",
    "# --- Param√®tres de la boucle RLHF ---\n",
    "NUM_ROLLOUTS = 50          # Nombre de s√©quences (samples) √† g√©n√©rer par instruction pour l'entra√Ænement\n",
    "N_RLHF_ITERATIONS = 3      # Nombre total d'it√©rations de la boucle RLHF (Generate -> Update)\n",
    "\n",
    "print(f\"‚úÖ Configuration RLHF charg√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e77fa5",
   "metadata": {},
   "source": [
    "## üîπ Partie 2 : Chargement du mod√®le Post-Entra√Æn√©\n",
    "\n",
    "Nous chargeons le mod√®le SFT cr√©√© √† la stage 2 comme base pour le RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Architecture red√©finie\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 2: Red√©finition Architecture\n",
    "\n",
    "# ============================================================================\n",
    "# RED√âFINITION DE L'ARCHITECTURE (copie de 2_post_training.ipynb)\n",
    "# ============================================================================\n",
    "\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Impl√©mente l'attention causale multi-t√™tes (Multi-Head Causal Self-Attention).\n",
    "    C'est le c≈ìur du Transformer qui permet au mod√®le de regarder les tokens pr√©c√©dents.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        # Projections pour Query, Key, Value\n",
    "        self.qkv = torch.nn.Linear(d_model, 3 * d_model)\n",
    "        # Projection de sortie\n",
    "        self.proj = torch.nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Masque causal pour emp√™cher de voir le futur (tril = triangle lower)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # Batch, Time (seq len), Channels (dim)\n",
    "        \n",
    "        # Calcul des Q, K, V\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        \n",
    "        # Redimensionnement pour les t√™tes d'attention\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Calcul du score d'attention (Scaled Dot-Product Attention)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Application du masque (mettre -inf l√† o√π on ne doit pas regarder)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        \n",
    "        # Agr√©gation des valeurs\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.proj(y)\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Un bloc Transformer standard compos√© de :\n",
    "    1. LayerNorm -> Self-Attention -> Residual Connection\n",
    "    2. LayerNorm -> Feed-Forward -> Residual Connection\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = torch.nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size)\n",
    "        self.ln2 = torch.nn.LayerNorm(d_model)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.GELU(), # Activation non-lin√©aire\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x)) # Attention avec connexion r√©siduelle\n",
    "        x = x + self.ff(self.ln2(x))   # Feed-forward avec connexion r√©siduelle\n",
    "        return x\n",
    "\n",
    "class TinyDecoderLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture compl√®te du mod√®le de langage (Decoder-only type GPT).\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # Embeddings des tokens et des positions\n",
    "        self.tok_emb = torch.nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = torch.nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "        \n",
    "        # Empilement des blocs Transformer\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            Block(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Normalisation finale\n",
    "        self.ln_f = torch.nn.LayerNorm(cfg.d_model)\n",
    "        # T√™te de pr√©diction (projette vers la taille du vocabulaire)\n",
    "        self.head = torch.nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Cr√©ation des embeddings de position\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        \n",
    "        # Passage √† travers les blocs\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Calcul de la perte Cross-Entropy si des cibles sont fournies\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.cfg.vocab_size),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Boucle de g√©n√©ration de texte (inf√©rence).\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # On garde seulement les derniers block_size tokens pour le contexte\n",
    "            idx_cond = idx[:, -self.cfg.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature # Focus sur le dernier token\n",
    "            \n",
    "            # Top-K sampling\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "                \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # √âchantillonnage\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 50280\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 8\n",
    "    d_ff: int = 2048\n",
    "    block_size: int = 256\n",
    "\n",
    "print(\"‚úÖ Architecture red√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d74a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Chargement du tokenizer...\n",
      "‚úÖ 3 tokens sp√©ciaux ajout√©s\n",
      "üìö Vocabulaire : 50,280 tokens\n",
      "\n",
      "üì• Chargement du mod√®le SFT...\n",
      "‚úÖ 3 tokens sp√©ciaux ajout√©s\n",
      "üìö Vocabulaire : 50,280 tokens\n",
      "\n",
      "üì• Chargement du mod√®le SFT...\n",
      "‚úÖ Mod√®le SFT charg√© : 76,837,888 param√®tres\n",
      "‚úÖ Mod√®le SFT charg√© : 76,837,888 param√®tres\n",
      "‚úÖ Reference model cr√©√©\n",
      "‚úÖ Reference model cr√©√©\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: Chargement Tokenizer et Mod√®le SFT\n",
    "\n",
    "print(\"üî§ Chargement du tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ajouter les tokens sp√©ciaux s'ils ne sont pas d√©j√† pr√©sents\n",
    "# Ces tokens structurent le format instruction/r√©ponse\n",
    "special_tokens = {'additional_special_tokens': ['<instruction>', '<reasoning>', '<answer>']}\n",
    "try:\n",
    "    num_added = tokenizer.add_special_tokens(special_tokens)\n",
    "    if num_added > 0:\n",
    "        print(f\"‚úÖ {num_added} tokens sp√©ciaux ajout√©s\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Tokens sp√©ciaux d√©j√† pr√©sents\")\n",
    "\n",
    "print(f\"üìö Vocabulaire : {len(tokenizer):,} tokens\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGER LE MOD√àLE SFT\n",
    "# ============================================================================\n",
    "print(\"\\nüì• Chargement du mod√®le SFT...\")\n",
    "\n",
    "# On charge le mod√®le qui a √©t√© fine-tun√© √† l'√©tape pr√©c√©dente (Stage 2)\n",
    "sft_checkpoint = torch.load(\"models/post_training/model_sft_FINAL.pt\", map_location=device)\n",
    "config = ModelConfig(vocab_size=len(tokenizer))\n",
    "\n",
    "# Cr√©er le mod√®le SFT (policy model)\n",
    "# C'est le mod√®le que nous allons optimiser avec PPO\n",
    "policy_model = TinyDecoderLM(config).to(device)\n",
    "\n",
    "# Charger les poids\n",
    "policy_model.load_state_dict(sft_checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le SFT charg√© : {sum(p.numel() for p in policy_model.parameters()):,} param√®tres\")\n",
    "\n",
    "# Cr√©er une copie pour r√©f√©rence (Reference Model)\n",
    "# Ce mod√®le est GEL√â (frozen) et sert √† calculer la divergence KL.\n",
    "# On veut que le policy model s'am√©liore mais ne s'√©loigne pas trop de la distribution initiale du SFT\n",
    "# pour √©viter qu'il ne \"hack\" la r√©compense (reward hacking) ou perde sa coh√©rence linguistique.\n",
    "reference_model = TinyDecoderLM(config).to(device)\n",
    "reference_model.load_state_dict(sft_checkpoint['model_state_dict'], strict=False)\n",
    "reference_model.eval()\n",
    "\n",
    "# Pas d'optimisation du reference model (on d√©sactive les gradients)\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"‚úÖ Reference model cr√©√© (gel√©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6f3f0",
   "metadata": {},
   "source": [
    "## üîπ Partie 3 : Architecture du Mod√®le de R√©compense\n",
    "\n",
    "Le **reward model** apprend √† scorer les g√©n√©rations du mod√®le en fonction de la qualit√© du code et de la satisfaction des pr√©f√©rences humaines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bd5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Cr√©ation du mod√®le de r√©compense...\n",
      "   Initialisation avec transfer learning...\n",
      "‚úÖ Reward model cr√©√© : 51,226,113 param√®tres\n",
      "   Initialisation avec transfer learning...\n",
      "‚úÖ Reward model cr√©√© : 51,226,113 param√®tres\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 4: Impl√©mentation du Reward Model\n",
    "\n",
    "class RewardModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mod√®le de r√©compense (Reward Model) qui attribue un score scalaire √† une s√©quence de texte.\n",
    "    \n",
    "    Architecture :\n",
    "    - Bas√©e sur le m√™me Transformer que le mod√®le de langage (TinyDecoderLM).\n",
    "    - La diff√©rence principale est la t√™te de sortie (Reward Head) :\n",
    "      Au lieu de projeter vers vocab_size (pour pr√©dire le prochain mot),\n",
    "      on projette vers 1 seule valeur (le score de r√©compense).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cfg = cfg  # Sauvegarde la config\n",
    "\n",
    "        # ---- EMBEDDINGS ----\n",
    "        self.tok_emb = torch.nn.Embedding(cfg.vocab_size, cfg.d_model) \n",
    "        self.pos_emb = torch.nn.Embedding(cfg.block_size, cfg.d_model)  \n",
    "\n",
    "        # ---- TRANSFORMER BLOCKS ----\n",
    "        # On r√©utilise la classe Block d√©finie pr√©c√©demment\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            Block(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = torch.nn.LayerNorm(cfg.d_model)   # Normalisation finale\n",
    "        \n",
    "        # ---- REWARD HEAD ----\n",
    "        # C'est ici que √ßa change par rapport au LM classique.\n",
    "        # On transforme le vecteur cach√© final (d_model) en un score unique (1).\n",
    "        self.reward_head = torch.nn.Sequential(      \n",
    "            torch.nn.Linear(cfg.d_model, cfg.d_model // 2),\n",
    "            torch.nn.GELU(), # Activation non-lin√©aire\n",
    "            torch.nn.Linear(cfg.d_model // 2, 1)      # Sortie scalaire\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Passe avant pour calculer le score de r√©compense.\n",
    "        \n",
    "        Args:\n",
    "            input_ids : (B, T) ‚Üí batch de s√©quences tokenis√©es\n",
    "            attention_mask : (B, T) optionnel (non utilis√© ici car decoder-only dense)\n",
    "        \n",
    "        Returns:\n",
    "            reward : (B,) score scalaire pour chaque s√©quence du batch\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = input_ids.shape                        # Batch size et longueur seq\n",
    "\n",
    "        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0)   # Positions embeddings 0..T-1\n",
    "\n",
    "        x = self.tok_emb(input_ids) + self.pos_emb(pos)   # Somme des embeddings\n",
    "        \n",
    "        # ---- PASSAGE DANS LES BLOCS TRANSFORMER ----\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)                               \n",
    "      \n",
    "        x = self.ln_f(x) # Normalisation finale\n",
    "\n",
    "        # ---- EXTRACTION DU SCORE ----\n",
    "        # On utilise g√©n√©ralement le hidden state du DERNIER token pour repr√©senter toute la s√©quence\n",
    "        # (car l'attention causale permet au dernier token de \"voir\" tout le contexte pr√©c√©dent)\n",
    "        last_token_hidden = x[:, -1, :]  # (B, d_model)\n",
    "\n",
    "        # ---- REWARD HEAD ----\n",
    "        reward = self.reward_head(last_token_hidden)     # (B, 1)\n",
    "\n",
    "        reward = reward.squeeze(-1)                      # (B,) ‚Üí vecteur de scores\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âER LE MOD√àLE DE R√âCOMPENSE\n",
    "# ============================================================================\n",
    "print(\"üèÜ Cr√©ation du mod√®le de r√©compense...\")\n",
    "\n",
    "reward_model = RewardModel(config).to(device)            # Instanciation du Reward Model\n",
    "\n",
    "print(\"   Initialisation avec transfer learning...\")\n",
    "\n",
    "# ---- TRANSFER LEARNING : COPIER LES POIDS DU MODELE SFT ----\n",
    "# On initialise le Reward Model avec les poids du mod√®le SFT.\n",
    "# Cela permet au RM de \"comprendre\" le langage d√®s le d√©part, au lieu d'apprendre de z√©ro.\n",
    "# Seule la \"reward_head\" est initialis√©e al√©atoirement.\n",
    "with torch.no_grad():                                    # Pas de gradients\n",
    "    for name, param in reward_model.named_parameters():  # On parcourt chaque param√®tre\n",
    "        if (\n",
    "            'blocks' in name or                         # Si le param√®tre appartient aux blocs\n",
    "            'tok_emb' in name or                        # ‚Ä¶ ou aux embeddings\n",
    "            'pos_emb' in name or\n",
    "            'ln_f' in name\n",
    "        ):\n",
    "            try:\n",
    "                # Copier le poids correspondant depuis le policy_model (SFT)\n",
    "                param.copy_(dict(policy_model.named_parameters())[name])\n",
    "            except:\n",
    "                # Si le param√®tre n‚Äôexiste pas (ex: reward_head), on ignore\n",
    "                pass\n",
    "\n",
    "# ---- STATS FINAL ----\n",
    "print(f\"‚úÖ Reward model cr√©√© : {sum(p.numel() for p in reward_model.parameters()):,} param√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199414d",
   "metadata": {},
   "source": [
    "## üîπ Partie 4 : Pr√©paration du Dataset de Pr√©f√©rences\n",
    "\n",
    "Pour entra√Æner le reward model, nous cr√©ons des paires de pr√©f√©rences synth√©tiques.\n",
    "Format : (instruction ‚Üí code_a, code_b) avec label indiquant quel code est pr√©f√©r√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f94e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Cr√©ation du dataset de pr√©f√©rences...\n",
      "üìä Loading SFT dataset...\n",
      "üìà Loaded 10379 valid SFT entries\n",
      "‚úÖ Dataset cr√©√© : 100 paires\n",
      "üì¶ Train pairs: 80\n",
      "üì¶ Val pairs: 20\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Cr√©ation du Dataset de Pr√©f√©rences\n",
    "\n",
    "# ============================================================================\n",
    "# G√âN√âRER DES DONN√âES DE PR√âF√âRENCE\n",
    "# ============================================================================\n",
    "print(\"üìä Cr√©ation du dataset de pr√©f√©rences...\")\n",
    "\n",
    "# Fonction utilitaire pour charger le JSONL\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Skipping invalid JSON line {i}: {line[:80]}...\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Charger les donn√©es SFT\n",
    "print(\"üìä Loading SFT dataset...\")\n",
    "sft_path = \"data/python_reasoning_dataset.jsonl\"\n",
    "sft_data = load_jsonl(sft_path)\n",
    "print(f\"üìà Loaded {len(sft_data)} valid SFT entries\")\n",
    "\n",
    "# Fonction de scoring heuristique\n",
    "# Dans un vrai sc√©nario RLHF, ces scores viendraient d'annotateurs humains qui comparent deux r√©ponses.\n",
    "# Ici, nous simulons ces pr√©f√©rences avec une fonction Python simple.\n",
    "def score_code_quality(code):\n",
    "    \"\"\"Score simple bas√© sur des heuristiques pour simuler un annotateur humain.\"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Crit√®re 1 : Longueur raisonnable (ni trop court, ni trop long)\n",
    "    if 10 < len(code) < 500:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Crit√®re 2 : Pr√©sence de d√©finition de fonction\n",
    "    if \"def \" in code:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Crit√®re 3 : Pr√©sence d'un retour de valeur\n",
    "    if \"return\" in code:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Crit√®re 4 : √âquilibre des parenth√®ses (syntaxe de base)\n",
    "    if code.count(\"(\") == code.count(\")\"):\n",
    "        score += 0.15\n",
    "    \n",
    "    # Crit√®re 5 : √âquilibre des crochets\n",
    "    if code.count(\"[\") == code.count(\"]\"):\n",
    "        score += 0.15\n",
    "    \n",
    "    # Crit√®re 6 : Indentation correcte (pas d'espace au d√©but de la premi√®re ligne)\n",
    "    if not (code.startswith(\" \") and not any(line.startswith(\"\\n \") for line in code.split(\"\\n\"))):\n",
    "        score += 0.0\n",
    "    \n",
    "    return min(1.0, score)\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE DATASET DE PR√âF√âRENCE\n",
    "# ============================================================================\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour entra√Æner le Reward Model.\n",
    "    Il fournit des paires (code_a, code_b) avec un label indiquant lequel est meilleur.\n",
    "    \"\"\"\n",
    "    def __init__(self, instructions, tokenizer, max_length=256, num_pairs=50):\n",
    "        self.instructions = instructions[:num_pairs]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.preference_pairs = self._create_pairs()\n",
    "    \n",
    "    def _create_pairs(self):\n",
    "        \"\"\"Cr√©er des paires de pr√©f√©rences synth√©tiques\"\"\"\n",
    "        pairs = []\n",
    "        for instruction in self.instructions:\n",
    "            # On prend la r√©ponse du dataset comme \"Code A\"\n",
    "            code_a = instruction.get('answer', '')\n",
    "            \n",
    "            # On cr√©e une variation artificielle pour \"Code B\" (ici juste un commentaire ajout√©)\n",
    "            # Dans la r√©alit√©, on g√©n√©rerait deux r√©ponses diff√©rentes avec le mod√®le.\n",
    "            code_b = instruction.get('answer', '') + \"\\n    # optimized version\"\n",
    "            \n",
    "            # On score les deux versions avec notre heuristique\n",
    "            score_a = score_code_quality(code_a)\n",
    "            score_b = score_code_quality(code_b)\n",
    "            \n",
    "            # Label binaire : 1 si code_b est meilleur que code_a, sinon 0\n",
    "            label = 1 if score_b > score_a else 0\n",
    "            \n",
    "            pairs.append({\n",
    "                'instruction': instruction.get('instruction', ''),\n",
    "                'code_a': code_a,\n",
    "                'code_b': code_b,\n",
    "                'label': label\n",
    "            })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.preference_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.preference_pairs[idx]\n",
    "        \n",
    "        # Formatage avec les balises sp√©ciales\n",
    "        text_a = f\"<answer> {pair['code_a']}\"\n",
    "        text_b = f\"<answer> {pair['code_b']}\"\n",
    "        \n",
    "        # Tokenisation\n",
    "        ids_a = self.tokenizer.encode(text_a, add_special_tokens=False)\n",
    "        ids_b = self.tokenizer.encode(text_b, add_special_tokens=False)\n",
    "        \n",
    "        # Padding / Truncation manuel pour avoir une taille fixe\n",
    "        if len(ids_a) > self.max_length:\n",
    "            ids_a = ids_a[:self.max_length]\n",
    "        else:\n",
    "            ids_a = ids_a + [self.tokenizer.eos_token_id] * (self.max_length - len(ids_a))\n",
    "        \n",
    "        if len(ids_b) > self.max_length:\n",
    "            ids_b = ids_b[:self.max_length]\n",
    "        else:\n",
    "            ids_b = ids_b + [self.tokenizer.eos_token_id] * (self.max_length - len(ids_b))\n",
    "        \n",
    "        return {\n",
    "            'ids_a': torch.tensor(ids_a, dtype=torch.long),\n",
    "            'ids_b': torch.tensor(ids_b, dtype=torch.long),\n",
    "            'label': torch.tensor(pair['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âER LES LOADERS\n",
    "# ============================================================================\n",
    "pref_dataset = PreferenceDataset(sft_data, tokenizer, num_pairs=100)\n",
    "print(f\"‚úÖ Dataset cr√©√© : {len(pref_dataset)} paires\")\n",
    "\n",
    "# S√©paration Train / Validation (80% / 20%)\n",
    "split_idx = int(0.8 * len(pref_dataset))\n",
    "train_pairs = pref_dataset.preference_pairs[:split_idx]\n",
    "val_pairs = pref_dataset.preference_pairs[split_idx:]\n",
    "\n",
    "# Cr√©ation des sous-datasets\n",
    "train_ds = PreferenceDataset(sft_data, tokenizer, num_pairs=len(train_pairs))\n",
    "train_ds.preference_pairs = train_pairs\n",
    "\n",
    "val_ds = PreferenceDataset(sft_data, tokenizer, num_pairs=len(val_pairs))\n",
    "val_ds.preference_pairs = val_pairs\n",
    "\n",
    "# Cr√©ation des DataLoaders PyTorch\n",
    "train_loader_reward = DataLoader(train_ds, batch_size=REWARD_BATCH_SIZE, shuffle=True)\n",
    "val_loader_reward = DataLoader(val_ds, batch_size=REWARD_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"üì¶ Train pairs: {len(train_pairs)}\")\n",
    "print(f\"üì¶ Val pairs: {len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728902e",
   "metadata": {},
   "source": [
    "## üîπ Partie 5 : Entra√Ænement du Mod√®le de R√©compense\n",
    "\n",
    "Nous entra√Ænons le reward model avec **Bradley-Terry loss** pour classifier les pr√©f√©rences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62090814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training Reward Model...\n",
      "üìä 3 epochs ‚Äî 20 batches/epoch\n",
      "\n",
      "============================================================\n",
      "üìÖ Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Reward: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  9.58it/s, loss=0.0000]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1 Summary:\n",
      "   - Train Loss : 0.2336\n",
      "   - Val Loss   : 0.0000\n",
      "============================================================\n",
      "üìÖ Epoch 2/3\n",
      "============================================================\n",
      "============================================================\n",
      "üìÖ Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Reward: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.68it/s, loss=0.0000]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 2 Summary:\n",
      "   - Train Loss : 0.0000\n",
      "   - Val Loss   : 0.0000\n",
      "============================================================\n",
      "üìÖ Epoch 3/3\n",
      "============================================================\n",
      "============================================================\n",
      "üìÖ Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Reward: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.70it/s, loss=0.0000]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 3 Summary:\n",
      "   - Train Loss : 0.0000\n",
      "   - Val Loss   : 0.0000\n",
      "\n",
      "‚úÖ Reward Model training finished!\n",
      "\n",
      "‚úÖ Reward Model training finished!\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 6: Training Reward Model\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIMIZER + SCHEDULER\n",
    "# ============================================================================\n",
    "optimizer_reward = AdamW(reward_model.parameters(), lr=REWARD_LR, weight_decay=0.01) # w_d r√©gularisation L2 : emp√™che les poids de diverger\n",
    "# Ce bloc g√®re la d√©croissance du learning rate pendant l‚Äôentra√Ænement (Cosine Annealing)\n",
    "scheduler_reward = CosineAnnealingLR( \n",
    "    optimizer_reward,\n",
    "    T_max=len(train_loader_reward) * REWARD_EPOCHS,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BRADLEY‚ÄìTERRY LOSS\n",
    "# ============================================================================\n",
    "def bradley_terry_loss(reward_a, reward_b, label):\n",
    "    \"\"\"\n",
    "    Fonction de perte standard pour apprendre des pr√©f√©rences (Ranking Loss).\n",
    "    \n",
    "    Principe :\n",
    "    Si label=1 (B est pr√©f√©r√© √† A), on veut maximiser P(B > A).\n",
    "    P(B > A) = sigmoid(reward_B - reward_A).\n",
    "    \n",
    "    On minimise la Binary Cross Entropy sur cette probabilit√©.\n",
    "    \n",
    "    Args:\n",
    "        reward_a: (B,) scores pr√©dits pour la r√©ponse A\n",
    "        reward_b: (B,) scores pr√©dits pour la r√©ponse B\n",
    "        label: (B,) 1 si B > A, sinon 0\n",
    "    \"\"\"\n",
    "    logits = reward_b - reward_a\n",
    "    # BCEWithLogitsLoss combine Sigmoid + BCE pour plus de stabilit√© num√©rique\n",
    "    return F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "print(\"üöÄ Training Reward Model...\")\n",
    "print(f\"üìä {REWARD_EPOCHS} epochs ‚Äî {len(train_loader_reward)} batches/epoch\\n\")\n",
    "\n",
    "reward_history = {'train_loss': [], 'val_loss': [], 'epochs': []}\n",
    "\n",
    "for epoch in range(REWARD_EPOCHS):\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìÖ Epoch {epoch+1}/{REWARD_EPOCHS}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # TRAIN\n",
    "    # ----------------------------------------------------------------------\n",
    "    reward_model.train()\n",
    "    total_train_loss = 0.0\n",
    "    pbar = tqdm(train_loader_reward, desc=\"Train Reward\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        ids_a = batch[\"ids_a\"].to(device)\n",
    "        ids_b = batch[\"ids_b\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        # Forward pass WITH GRADIENTS\n",
    "        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        # Le mod√®le calcule un score scalaire pour chaque r√©ponse\n",
    "        reward_a = reward_model(ids_a)\n",
    "        reward_b = reward_model(ids_b)\n",
    "\n",
    "        # Calcul de la perte de classement\n",
    "        loss = bradley_terry_loss(reward_a, reward_b, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer_reward.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(reward_model.parameters(), 1.0) # Gradient clipping pour stabilit√©\n",
    "        optimizer_reward.step()\n",
    "        scheduler_reward.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader_reward)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # VALIDATION\n",
    "    # ----------------------------------------------------------------------\n",
    "    reward_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_reward:\n",
    "            ids_a = batch[\"ids_a\"].to(device)\n",
    "            ids_b = batch[\"ids_b\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            reward_a = reward_model(ids_a)\n",
    "            reward_b = reward_model(ids_b)\n",
    "\n",
    "            loss = bradley_terry_loss(reward_a, reward_b, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader_reward)\n",
    "\n",
    "    reward_history['train_loss'].append(avg_train_loss)\n",
    "    reward_history['val_loss'].append(avg_val_loss)\n",
    "    reward_history['epochs'].append(epoch + 1)\n",
    "\n",
    "    print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "    print(f\"   - Train Loss : {avg_train_loss:.4f}\")\n",
    "    print(f\"   - Val Loss   : {avg_val_loss:.4f}\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # SAVE CHECKPOINT\n",
    "    # ----------------------------------------------------------------------\n",
    "    os.makedirs(\"models/alignment\", exist_ok=True)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": reward_model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer_reward.state_dict(),\n",
    "        \"history\": reward_history,\n",
    "    }\n",
    "    torch.save(ckpt, f\"models/alignment/reward_model_epoch_{epoch+1}.pt\")\n",
    "\n",
    "print(\"\\n‚úÖ Reward Model training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511b70f",
   "metadata": {},
   "source": [
    "## üîπ Partie 6 : Impl√©mentation de PPO (Proximal Policy Optimization)\n",
    "\n",
    "PPO est un algorithme d'apprentissage par renforcement stable et efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe PPOTrainer impl√©ment√©e\n",
      "‚úÖ PPO Trainer cr√©√©\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 7: Impl√©mentation PPO\n",
    "\n",
    "class PPOTrainer:\n",
    "    \"\"\"\n",
    "    Entra√Æneur PPO (Proximal Policy Optimization) pour RLHF.\n",
    "    G√®re la g√©n√©ration de donn√©es (rollouts), le calcul des r√©compenses et la mise √† jour du mod√®le.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, reference_model, reward_model, tokenizer, device):\n",
    "        self.policy_model = policy_model        # Le mod√®le qu'on entra√Æne (Actor)\n",
    "        self.reference_model = reference_model  # Le mod√®le gel√© pour la p√©nalit√© KL\n",
    "        self.reward_model = reward_model        # Le mod√®le qui juge la qualit√© (Critic externe)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # Value head : Pr√©dit la valeur attendue d'un √©tat (V(s)).\n",
    "        # Utilis√© pour r√©duire la variance de l'estimation de l'avantage (Advantage).\n",
    "        # Ici, on ajoute une petite t√™te lin√©aire sur le mod√®le.\n",
    "        self.value_model = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_model, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        # On optimise √† la fois le Policy Model et le Value Model\n",
    "        self.optimizer = AdamW(\n",
    "            list(policy_model.parameters()) + list(self.value_model.parameters()),\n",
    "            lr=PPO_LR\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_rollout(self, prompt_ids, max_new_tokens=MAX_NEW_TOKENS):\n",
    "        \"\"\"\n",
    "        G√©n√©rer une s√©quence compl√®te (Rollout) pour l'entra√Ænement RL.\n",
    "        Retourne aussi les log-probabilit√©s de chaque token g√©n√©r√© (n√©cessaire pour PPO).\n",
    "        \"\"\"\n",
    "        self.policy_model.eval()\n",
    "        \n",
    "        input_ids = prompt_ids.clone().to(self.device)\n",
    "        all_tokens = [input_ids]\n",
    "        all_logprobs = []\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = input_ids[:, -config.block_size:]\n",
    "            \n",
    "            # Obtenir les logits du policy model\n",
    "            logits, _ = self.policy_model(idx_cond)      # (B, T, V)\n",
    "            logits = logits[:, -1, :] / TEMPERATURE      # (B, V)\n",
    "            \n",
    "            # Nettoyage num√©rique (√©viter NaN/Inf)\n",
    "            logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Log probs\n",
    "            log_probs = F.log_softmax(logits, dim=-1)    # (B, V)\n",
    "            \n",
    "            # Conversion en probabilit√©s de mani√®re s√ªre\n",
    "            probs = log_probs.exp()                      # (B, V)\n",
    "            probs = torch.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            probs = torch.clamp(probs, min=0.0)\n",
    "            \n",
    "            # Renormalisation pour garantir une distribution valide\n",
    "            probs_sum = probs.sum(dim=-1, keepdim=True)  # (B, 1)\n",
    "            probs_sum = torch.where(\n",
    "                probs_sum == 0.0,\n",
    "                torch.ones_like(probs_sum),\n",
    "                probs_sum\n",
    "            )\n",
    "            probs = probs / probs_sum\n",
    "            \n",
    "            # Garde-fou final\n",
    "            probs = torch.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            probs = torch.clamp(probs, min=0.0)\n",
    "            \n",
    "            # √âchantillonnage du prochain token (Action)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Stocker le token et sa log-probabilit√©\n",
    "            all_tokens.append(idx_next)\n",
    "            step_logprob = log_probs.gather(-1, idx_next).squeeze(-1)  # (B,)\n",
    "            all_logprobs.append(step_logprob)\n",
    "            \n",
    "            input_ids = torch.cat((input_ids, idx_next), dim=1)\n",
    "            \n",
    "            # Arr√™t si EOS (End of Sequence) est g√©n√©r√©\n",
    "            if (idx_next == self.tokenizer.eos_token_id).all():\n",
    "                break\n",
    "        \n",
    "        # Concat√©ner tous les tokens pour former la trajectoire compl√®te\n",
    "        full_output = torch.cat(all_tokens, dim=1)           # (B, T_total)\n",
    "        trajectory_logprobs = torch.stack(all_logprobs,  dim=1)  # (B, T_gen)\n",
    "        \n",
    "        return full_output, trajectory_logprobs\n",
    "\n",
    "    \n",
    "    def compute_rewards(self, input_ids):\n",
    "        \"\"\"Calculer le score de r√©compense pour les s√©quences g√©n√©r√©es via le Reward Model.\"\"\"\n",
    "        self.reward_model.eval()\n",
    "        with torch.no_grad():\n",
    "            rewards = self.reward_model(input_ids)  # (B,)\n",
    "        return rewards\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def compute_kl_penalty(self, input_ids):\n",
    "        \"\"\"\n",
    "        Calculer la divergence KL entre le Policy Model et le Reference Model.\n",
    "        KL(Policy || Ref) mesure √† quel point la politique a chang√©.\n",
    "        On l'utilise comme p√©nalit√© pour √©viter que le mod√®le n'oublie ses connaissances linguistiques.\n",
    "        \"\"\"\n",
    "        # Forward pass dans les deux mod√®les\n",
    "        policy_logits, _ = self.policy_model(input_ids)\n",
    "        ref_logits, _ = self.reference_model(input_ids)\n",
    "        \n",
    "        # Log probs\n",
    "        policy_log_probs = F.log_softmax(policy_logits, dim=-1)  # (B, T, V)\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)  # (B, T, V)\n",
    "        \n",
    "        # Formule KL Divergence : sum(P(x) * log(P(x)/Q(x)))\n",
    "        # Ici approxim√© par : log(P_ref) - log(P_policy) pond√©r√© par P_ref (implicite car on sample de policy)\n",
    "        # Note: L'impl√©mentation standard RLHF utilise souvent : log(P_policy) - log(P_ref) comme reward n√©gatif\n",
    "        kl_div = torch.exp(ref_log_probs) * (ref_log_probs - policy_log_probs)\n",
    "        kl_div = kl_div.mean()  # Moyenne sur le batch\n",
    "        \n",
    "        return kl_div\n",
    "    \n",
    "    def ppo_update(self, rollouts, rewards, kl_penalties):\n",
    "        \"\"\"\n",
    "        Effectuer une √©tape de mise √† jour PPO (Proximal Policy Optimization).\n",
    "        C'est ici que l'apprentissage par renforcement a lieu.\n",
    "        \"\"\"\n",
    "        self.policy_model.train()\n",
    "        \n",
    "        B = len(rollouts)\n",
    "        \n",
    "        # Normalisation des r√©compenses (stabilise l'entra√Ænement)\n",
    "        rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        \n",
    "        # Calcul des avantages (Advantages)\n",
    "        # A = R - V(s) (R√©compense r√©elle - Valeur pr√©dite)\n",
    "        # Ici simplifi√© : on utilise directement la r√©compense normalis√©e comme proxy de l'avantage\n",
    "        advantages = rewards_normalized.clone()\n",
    "        \n",
    "        # Boucle d'optimisation PPO (plusieurs √©poques sur le m√™me batch de donn√©es)\n",
    "        total_loss = 0\n",
    "        for ppo_epoch in range(PPO_EPOCHS):\n",
    "            self.policy_model.train()\n",
    "            \n",
    "            # Obtenir les log-probs actuelles (pi_theta)\n",
    "            logits, _ = self.policy_model(rollouts)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Log probs des actions r√©ellement prises\n",
    "            # (Simplification: moyenne sur la s√©quence pour avoir un scalaire par sample)\n",
    "            action_log_probs = log_probs.mean(dim=1) \n",
    "            \n",
    "            # --- PPO Loss ---\n",
    "            # L = - E[ log(pi) * A ]\n",
    "            # On veut augmenter la proba des actions avec Avantage positif\n",
    "            policy_loss = -(action_log_probs * advantages).mean()\n",
    "            \n",
    "            # --- KL Penalty ---\n",
    "            # On ajoute la p√©nalit√© KL directement dans la perte\n",
    "            kl_loss = KL_COEFF * kl_penalties.mean()\n",
    "            \n",
    "            # --- Entropy Bonus ---\n",
    "            # Encourage l'exploration en p√©nalisant les distributions trop certaines\n",
    "            entropy = -(log_probs * torch.exp(log_probs)).sum(dim=-1).mean()\n",
    "            entropy_loss = -ENTROPY_COEFF * entropy\n",
    "            \n",
    "            # Perte totale\n",
    "            total_loss = policy_loss + kl_loss + entropy_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.policy_model.parameters()) + list(self.value_model.parameters()),\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        self.policy_model.eval()\n",
    "        return total_loss.item()\n",
    "\n",
    "print(\"‚úÖ Classe PPOTrainer impl√©ment√©e\")\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âER LE TRAINER PPO\n",
    "# ============================================================================\n",
    "ppo_trainer = PPOTrainer(policy_model, reference_model, reward_model, tokenizer, device)\n",
    "print(\"‚úÖ PPO Trainer cr√©√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404c8b2",
   "metadata": {},
   "source": [
    "## üîπ Partie 7 : Pipeline RLHF Complet\n",
    "\n",
    "Nous ex√©cutons le pipeline d'alignement par renforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d654712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Lancement du pipeline RLHF...\n",
      "üìä Configuration: 3 it√©rations RLHF\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîÑ It√©ration RLHF 1/3\n",
      "======================================================================\n",
      "\n",
      "üìù Phase 1: G√©n√©ration de samples...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 1.7099\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning>244 submission summons clashiate battery#{$)*(Dialogplacian electricallyruitment emerg~~~~~~~~ ...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 1.7099\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning>244 submission summons clashiate battery#{$)*(Dialogplacian electricallyruitment emerg~~~~~~~~ ...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: -0.4265\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning> immunityindices Whereacter illustrationsserHOUphosphate exacerbated Letters Lucky posing...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: -0.4265\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning> immunityindices Whereacter illustrationsserHOUphosphate exacerbated Letters Lucky posing...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.8225\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning>Axis devised Tina betrayal salmon CMV‡∏° testified <?dag gour brighter glorious„Åó„ÅÑSy tribTreatment folders attaine...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.8225\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning>Axis devised Tina betrayal salmon CMV‡∏° testified <?dag gour brighter glorious„Åó„ÅÑSy tribTreatment folders attaine...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_40636\\1488725639.py:115: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  rewards_normalized = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä R√©sultats It√©ration 1:\n",
      "   - Avg Reward:      2.1141\n",
      "   - KL Divergence:   nan\n",
      "   - Policy Loss:     nan\n",
      "\n",
      "======================================================================\n",
      "üîÑ It√©ration RLHF 2/3\n",
      "======================================================================\n",
      "\n",
      "üìù Phase 1: G√©n√©ration de samples...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 3.2469\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning> auxiliarytextbfVill mate ritualMal Bas immunoreactivity unauthorized„Å¶„ÅÑ„ÇãÁöÑ continuously aroseSta...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 3.2469\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning> auxiliarytextbfVill mate ritualMal Bas immunoreactivity unauthorized„Å¶„ÅÑ„ÇãÁöÑ continuously aroseSta...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: 0.1215\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning>balmodifiedulates affadssetlengthnever awful Rhe tearcreateotto Mohgl)‚Äì( errno sido unawa...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: 0.1215\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning>balmodifiedulates affadssetlengthnever awful Rhe tearcreateotto Mohgl)‚Äì( errno sido unawa...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.6868\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning> Karmoil generaillas Shall hinted262 Empoproteins denseivatKy todo inspirereactive Ro hatelessnessomez clues ou...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.6868\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning> Karmoil generaillas Shall hinted262 Empoproteins denseivatKy todo inspirereactive Ro hatelessnessomez clues ou...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n",
      "\n",
      "üìä R√©sultats It√©ration 2:\n",
      "   - Avg Reward:      2.4716\n",
      "   - KL Divergence:   nan\n",
      "   - Policy Loss:     nan\n",
      "\n",
      "======================================================================\n",
      "üîÑ It√©ration RLHF 3/3\n",
      "======================================================================\n",
      "\n",
      "üìù Phase 1: G√©n√©ration de samples...\n",
      "\n",
      "üìä R√©sultats It√©ration 2:\n",
      "   - Avg Reward:      2.4716\n",
      "   - KL Divergence:   nan\n",
      "   - Policy Loss:     nan\n",
      "\n",
      "======================================================================\n",
      "üîÑ It√©ration RLHF 3/3\n",
      "======================================================================\n",
      "\n",
      "üìù Phase 1: G√©n√©ration de samples...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 1.9304\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning> shipped amplitude Saidimm pancreaticMODrowing cartoon psychicv√© testimonYP                    ...\n",
      "\n",
      "üìå Instruction: Write a function to calculate factorial\n",
      "üéØ Reward Score: 1.9304\n",
      "Generated (truncated): <instruction> Write a function to calculate factorial <reasoning> shipped amplitude Saidimm pancreaticMODrowing cartoon psychicv√© testimonYP                    ...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: 0.4633\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning> -. MLAQticoNaturecord 52 beansice brewing Male040 waiver reproducibilityuminescenceASS c...\n",
      "\n",
      "üìå Instruction: Create a function to check if number is prime\n",
      "üéØ Reward Score: 0.4633\n",
      "Generated (truncated): <instruction> Create a function to check if number is prime <reasoning> -. MLAQticoNaturecord 52 beansice brewing Male040 waiver reproducibilityuminescenceASS c...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.4911\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning>altrecognizedRESETjuvant=\"'$(agher Americorageadequ Ohiopton wre s√≥lo rabbits respiratory !! title TinyÂ≠¶ Fraser...\n",
      "\n",
      "üìå Instruction: Implement binary search\n",
      "üéØ Reward Score: 4.4911\n",
      "Generated (truncated): <instruction> Implement binary search <reasoning>altrecognizedRESETjuvant=\"'$(agher Americorageadequ Ohiopton wre s√≥lo rabbits respiratory !! title TinyÂ≠¶ Fraser...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n",
      "\n",
      "üîÑ Phase 2: Mise √† jour du policy model (PPO)...\n",
      "\n",
      "üìä R√©sultats It√©ration 3:\n",
      "   - Avg Reward:      2.6427\n",
      "   - KL Divergence:   nan\n",
      "   - Policy Loss:     nan\n",
      "\n",
      "‚úÖ Pipeline RLHF termin√©!\n",
      "\n",
      "üìä R√©sultats It√©ration 3:\n",
      "   - Avg Reward:      2.6427\n",
      "   - KL Divergence:   nan\n",
      "   - Policy Loss:     nan\n",
      "\n",
      "‚úÖ Pipeline RLHF termin√©!\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 8: Ex√©cution RLHF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ Lancement du pipeline RLHF...\")\n",
    "print(f\"üìä Configuration: {N_RLHF_ITERATIONS} it√©rations RLHF\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# S√âLECTION D'INSTRUCTIONS DE TEST\n",
    "# ============================================================================\n",
    "# Ces instructions servent de \"prompts\" pour g√©n√©rer du code que le mod√®le va am√©liorer\n",
    "test_instructions = [\n",
    "    \"Write a function to calculate factorial\",\n",
    "    \"Create a function to check if number is prime\",\n",
    "    \"Implement binary search\",\n",
    "    \"Write function to reverse a list\",\n",
    "    \"Create a function to sum all elements\"\n",
    "]\n",
    "\n",
    "rlhf_history = {\n",
    "    'iterations': [],\n",
    "    'avg_rewards': [],\n",
    "    'kl_divs': [],\n",
    "    'policy_losses': []\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# BOUCLE RLHF (Reinforcement Learning Loop)\n",
    "# ============================================================================\n",
    "# Structure de la boucle :\n",
    "# 1. Rollout : Le mod√®le g√©n√®re des r√©ponses √† partir des prompts.\n",
    "# 2. Evaluation : Le Reward Model note ces r√©ponses.\n",
    "# 3. Update : PPO met √† jour le mod√®le pour maximiser ces notes.\n",
    "\n",
    "for rlhf_iter in range(N_RLHF_ITERATIONS):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîÑ It√©ration RLHF {rlhf_iter + 1}/{N_RLHF_ITERATIONS}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    iteration_rewards = []\n",
    "    iteration_kl_divs = []\n",
    "    iteration_policy_losses = []\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PHASE 1: GENERATION (ROLLOUTS)\n",
    "    # ====================================================================\n",
    "    print(f\"\\nüìù Phase 1: G√©n√©ration de samples (Exploration)...\")\n",
    "    \n",
    "    for instruction in test_instructions[:3]:  # On limite √† 3 instructions pour la d√©mo\n",
    "        prompt = f\"<instruction> {instruction} <reasoning>\"\n",
    "        prompt_ids = torch.tensor(\n",
    "            [tokenizer.encode(prompt, add_special_tokens=False)],\n",
    "            device=device,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        \n",
    "        # Nombre de rollouts par instruction\n",
    "        n_rollouts = min(3, max(1, NUM_ROLLOUTS // max(1, len(test_instructions))))\n",
    "        \n",
    "        for rollout_idx in range(n_rollouts):\n",
    "            # 1. G√©n√©rer une trajectoire (Action)\n",
    "            output_ids, traj_logprobs = ppo_trainer.generate_rollout(\n",
    "                prompt_ids, \n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            \n",
    "            # 2. Calculer la r√©compense (Reward)\n",
    "            reward = ppo_trainer.compute_rewards(output_ids)          # (B,) -> ici B=1\n",
    "            \n",
    "            # 3. Calculer la p√©nalit√© KL (pour ne pas trop s'√©loigner du mod√®le initial)\n",
    "            kl_penalty = ppo_trainer.compute_kl_penalty(output_ids)   # scalaire\n",
    "\n",
    "            # Logging\n",
    "            iteration_rewards.append(reward.item())\n",
    "            iteration_kl_divs.append(kl_penalty.item())\n",
    "            \n",
    "            # Affichage d'un exemple pour suivre les progr√®s\n",
    "            if rollout_idx == 0:\n",
    "                generated_text = tokenizer.decode(output_ids[0].tolist())\n",
    "                print(f\"\\nüìå Instruction: {instruction}\")\n",
    "                print(f\"üéØ Reward Score: {reward.item():.4f}\")\n",
    "                print(f\"Generated (truncated): {generated_text[:160]}...\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PHASE 2: PPO UPDATE (OPTIMISATION)\n",
    "    # ====================================================================\n",
    "    print(f\"\\nüîÑ Phase 2: Mise √† jour du policy model (PPO)...\")\n",
    "    \n",
    "    # Pour simplifier cette d√©mo, on r√©g√©n√®re un batch sp√©cifique pour l'update\n",
    "    # Dans une impl√©mentation compl√®te, on utiliserait un Replay Buffer avec les rollouts pr√©c√©dents.\n",
    "    prompt = f\"<instruction> {test_instructions[0]} <reasoning>\"\n",
    "    prompt_ids = torch.tensor(\n",
    "        [tokenizer.encode(prompt, add_special_tokens=False)],\n",
    "        device=device,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    # Rollout frais pour PPO\n",
    "    output_ids, traj_logprobs = ppo_trainer.generate_rollout(prompt_ids, max_new_tokens=50)\n",
    "    rewards = ppo_trainer.compute_rewards(output_ids)          # (B,)\n",
    "    kl_penalty = ppo_trainer.compute_kl_penalty(output_ids)   # scalaire\n",
    "    \n",
    "    # Adapter la forme du KL pour ppo_update : (B,)\n",
    "    kl_penalties = kl_penalty.expand_as(rewards)\n",
    "    \n",
    "    # Mise √† jour des poids du mod√®le\n",
    "    policy_loss = ppo_trainer.ppo_update(output_ids, rewards, kl_penalties)\n",
    "    iteration_policy_losses.append(policy_loss)\n",
    "    \n",
    "    # ====================================================================\n",
    "    # LOGGING & STATISTIQUES\n",
    "    # ====================================================================\n",
    "    avg_reward = float(np.mean(iteration_rewards)) if len(iteration_rewards) > 0 else 0.0\n",
    "    avg_kl = float(np.mean(iteration_kl_divs))     if len(iteration_kl_divs) > 0 else 0.0\n",
    "    avg_policy_loss = float(np.mean(iteration_policy_losses)) if len(iteration_policy_losses) > 0 else 0.0\n",
    "    \n",
    "    rlhf_history['iterations'].append(rlhf_iter + 1)\n",
    "    rlhf_history['avg_rewards'].append(avg_reward)\n",
    "    rlhf_history['kl_divs'].append(avg_kl)\n",
    "    rlhf_history['policy_losses'].append(avg_policy_loss)\n",
    "    \n",
    "    print(f\"\\nüìä R√©sultats It√©ration {rlhf_iter + 1}:\")\n",
    "    print(f\"   - Avg Reward:      {avg_reward:.4f} (Plus haut est mieux)\")\n",
    "    print(f\"   - KL Divergence:   {avg_kl:.6f} (Doit rester stable)\")\n",
    "    print(f\"   - Policy Loss:     {avg_policy_loss:.4f} (Doit diminuer)\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline RLHF termin√©!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8566f61",
   "metadata": {},
   "source": [
    "## üîπ Partie 8 : √âvaluation du Mod√®le Align√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720761b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Visualisation des r√©sultats RLHF...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_40636\\269252869.py:77: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìà COMPARAISON SFT (Baseline) vs RLHF (Aligned)\n",
      "======================================================================\n",
      "\n",
      "üî¥ Mod√®le SFT (Baseline):\n",
      "------------------------------------------------------------\n",
      "<instruction> Write a function to calculate factorial <reasoning>5<:<|padding|>&AM'\">I;<|padding|>/P.#F3G*82(=<|padding|>&&00EB8PMCI<|padding|>)3<|padding|>=G7K?A<|endoftext|>\n",
      "\n",
      "üèÜ Reward Score: -4.9195\n",
      "\n",
      "üü¢ Mod√®le RLHF (Aligned):\n",
      "------------------------------------------------------------\n",
      "<instruction> Write a function to calculate factorial <reasoning>5<:<|padding|>&AM'\">I;<|padding|>/P.#F3G*82(=<|padding|>&&00EB8PMCI<|padding|>)3<|padding|>=G7K?A<|endoftext|>\n",
      "\n",
      "üèÜ Reward Score: -4.9195\n",
      "\n",
      "üü¢ Mod√®le RLHF (Aligned):\n",
      "------------------------------------------------------------\n",
      "<instruction> Write a function to calculate factorial <reasoning>N9!@IL61-7N492>$9&5M>!P6OD5<D.:*,3F)DON(G/9!!BDL/#!/.$#*-!\"F<|endoftext|>\n",
      "\n",
      "üèÜ Reward Score: -4.6321\n",
      "\n",
      "üìä Am√©lioration: 5.8%\n",
      "\n",
      "======================================================================\n",
      "‚úÖ √âvaluation termin√©e\n",
      "<instruction> Write a function to calculate factorial <reasoning>N9!@IL61-7N492>$9&5M>!P6OD5<D.:*,3F)DON(G/9!!BDL/#!/.$#*-!\"F<|endoftext|>\n",
      "\n",
      "üèÜ Reward Score: -4.6321\n",
      "\n",
      "üìä Am√©lioration: 5.8%\n",
      "\n",
      "======================================================================\n",
      "‚úÖ √âvaluation termin√©e\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 9: Visualisation et √âvaluation\n",
    "@torch.no_grad()\n",
    "def safe_generate(model, tokenizer, prompt_ids, max_new_tokens=80, temperature=0.7, top_k=50):\n",
    "    \"\"\"\n",
    "    Fonction de g√©n√©ration s√©curis√©e (Safe Generation) pour l'√©valuation.\n",
    "    G√®re les cas limites (NaN, Inf) et assure une g√©n√©ration stable.\n",
    "    \"\"\"\n",
    "    input_ids = prompt_ids.clone().to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass\n",
    "        logits, _ = model(input_ids[:, -config.block_size:]) \n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Clamp extreme values (s√©curit√© num√©rique)\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Temperature scaling\n",
    "        logits = logits / max(temperature, 1e-6)\n",
    "\n",
    "        # Optional top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_vals, top_idx = torch.topk(logits, k=top_k, dim=-1)\n",
    "            mask = torch.full_like(logits, -1e10)\n",
    "            logits = mask.scatter(1, top_idx, top_vals)\n",
    "\n",
    "        # Convert to probabilities safely\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        probs = log_probs.exp()\n",
    "\n",
    "        # Fix numerical issues\n",
    "        probs = torch.nan_to_num(probs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        probs = torch.clamp(probs, min=0.0)\n",
    "\n",
    "        # Renormalize\n",
    "        probs_sum = probs.sum(dim=-1, keepdim=True)\n",
    "        probs_sum = torch.where(probs_sum == 0, torch.ones_like(probs_sum), probs_sum)\n",
    "        probs = probs / probs_sum\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "        # Append\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # Stop at EOS\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "print(\"üìä Visualisation des r√©sultats RLHF...\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Graphique 1 : √âvolution des R√©compenses\n",
    "# On veut voir cette courbe monter (le mod√®le g√©n√®re du meilleur code)\n",
    "axes[0].plot(rlhf_history['iterations'], rlhf_history['avg_rewards'], marker='o', linewidth=2)\n",
    "axes[0].set_xlabel('It√©ration RLHF')\n",
    "axes[0].set_ylabel('Average Reward')\n",
    "axes[0].set_title('üìä √âvolution du Reward Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2 : Divergence KL\n",
    "# On veut que cette courbe reste basse ou monte tr√®s doucement.\n",
    "# Si elle explose, le mod√®le \"oublie\" ce qu'il a appris avant.\n",
    "axes[1].plot(rlhf_history['iterations'], rlhf_history['kl_divs'], marker='s', linewidth=2, color='orange')\n",
    "axes[1].set_xlabel('It√©ration RLHF')\n",
    "axes[1].set_ylabel('KL Divergence')\n",
    "axes[1].set_title('üìä KL Divergence du Policy Model')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 3 : Perte PPO\n",
    "# Indique si l'optimisation converge.\n",
    "axes[2].plot(rlhf_history['iterations'], rlhf_history['policy_losses'], marker='^', linewidth=2, color='green')\n",
    "axes[2].set_xlabel('It√©ration RLHF')\n",
    "axes[2].set_ylabel('Policy Loss')\n",
    "axes[2].set_title('üìä Policy Loss')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARAISON SFT vs RLHF\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà COMPARAISON SFT (Baseline) vs RLHF (Aligned)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_prompt = \"<instruction> Write a function to calculate factorial <reasoning>\"\n",
    "prompt_ids = torch.tensor([tokenizer.encode(test_prompt, add_special_tokens=False)], device=device)\n",
    "\n",
    "# ======= BASELINE SFT =======\n",
    "print(\"\\nüî¥ Mod√®le SFT (Baseline - Avant RLHF):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_sft = safe_generate(policy_model, tokenizer, prompt_ids, max_new_tokens=100)\n",
    "    text_sft = tokenizer.decode(output_sft[0].tolist())\n",
    "    print(text_sft[:300])\n",
    "\n",
    "    reward_sft = reward_model(output_sft).item()\n",
    "    print(f\"\\nüèÜ Reward Score: {reward_sft:.4f}\")\n",
    "\n",
    "# ======= RLHF =======\n",
    "print(\"\\nüü¢ Mod√®le RLHF (Aligned - Apr√®s RLHF):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Note: policy_model a √©t√© mis √† jour par PPO\n",
    "    output_rlhf = safe_generate(policy_model, tokenizer, prompt_ids, max_new_tokens=100)\n",
    "    text_rlhf = tokenizer.decode(output_rlhf[0].tolist())\n",
    "    print(text_rlhf[:300])\n",
    "\n",
    "    reward_rlhf = reward_model(output_rlhf).item()\n",
    "    print(f\"\\nüèÜ Reward Score: {reward_rlhf:.4f}\")\n",
    "\n",
    "# Calcul du pourcentage d'am√©lioration\n",
    "print(f\"\\nüìä Am√©lioration: {(reward_rlhf - reward_sft) / max(abs(reward_sft),1e-6) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ √âvaluation termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d1b9f",
   "metadata": {},
   "source": [
    "## üîπ Partie 9 : Sauvegarde du Mod√®le Align√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c07393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üíæ SAUVEGARDE DES MOD√àLES ALIGN√âS\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ Mod√®le Policy (RLHF-aligned)...\n",
      "   ‚úÖ Sauvegard√©: models/alignment/model_rlhf_aligned.pt\n",
      "\n",
      "2Ô∏è‚É£ Mod√®le de R√©compense...\n",
      "   ‚úÖ Sauvegard√©: models/alignment/model_rlhf_aligned.pt\n",
      "\n",
      "2Ô∏è‚É£ Mod√®le de R√©compense...\n",
      "   ‚úÖ Sauvegard√©: models/alignment/reward_model_final.pt\n",
      "\n",
      "3Ô∏è‚É£ Tokenizer...\n",
      "   ‚úÖ Sauvegard√©: models/alignment/tokenizer/\n",
      "\n",
      "======================================================================\n",
      "üì¶ R√âSUM√â DES ARTEFACTS CR√â√âS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ ALIGNMENT (RLHF) - Stage 3 Compl√©t√©e\n",
      "\n",
      "üìÅ Fichiers cr√©√©s:\n",
      "‚îú‚îÄ‚îÄ models/alignment/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ model_rlhf_aligned.pt          ‚úÖ Mod√®le final align√©\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_final.pt          ‚úÖ Mod√®le de r√©compense\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_epoch_[1-3].pt    ‚úÖ Checkpoints reward\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/                     ‚úÖ Tokenizer avec tokens sp√©ciaux\n",
      "\n",
      "üìä M√©triques de Performance:\n",
      "   - Initial SFT Reward:     2.1141\n",
      "   - Final RLHF Reward:      2.6427\n",
      "   - Am√©lioration:           25.0%\n",
      "   - Final KL Divergence:    nan\n",
      "\n",
      "üéØ Objectifs Atteints:\n",
      "   ‚úÖ Reward Model entra√Æn√© sur pr√©f√©rences humaines\n",
      "   ‚úÖ Pipeline PPO impl√©ment√©\n",
      "   ‚úÖ RLHF training loop ex√©cut√©\n",
      "   ‚úÖ Mod√®le align√© avec les pr√©f√©rences\n",
      "   ‚úÖ KL divergence maintenue sous contr√¥le\n",
      "\n",
      "üöÄ Prochaines √©tapes:\n",
      "   ‚Üí D√©ployer le mod√®le align√© en production\n",
      "   ‚Üí Tester sur un benchmark d'√©valuation plus large\n",
      "   ‚Üí Fine-tuner les hyperparam√®tres PPO\n",
      "   ‚Üí Collecter plus de donn√©es de pr√©f√©rences humaines\n",
      "\n",
      "\n",
      "‚úÖ Tous les fichiers sauvegard√©s!\n",
      "   ‚úÖ Sauvegard√©: models/alignment/reward_model_final.pt\n",
      "\n",
      "3Ô∏è‚É£ Tokenizer...\n",
      "   ‚úÖ Sauvegard√©: models/alignment/tokenizer/\n",
      "\n",
      "======================================================================\n",
      "üì¶ R√âSUM√â DES ARTEFACTS CR√â√âS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ ALIGNMENT (RLHF) - Stage 3 Compl√©t√©e\n",
      "\n",
      "üìÅ Fichiers cr√©√©s:\n",
      "‚îú‚îÄ‚îÄ models/alignment/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ model_rlhf_aligned.pt          ‚úÖ Mod√®le final align√©\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_final.pt          ‚úÖ Mod√®le de r√©compense\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_epoch_[1-3].pt    ‚úÖ Checkpoints reward\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/                     ‚úÖ Tokenizer avec tokens sp√©ciaux\n",
      "\n",
      "üìä M√©triques de Performance:\n",
      "   - Initial SFT Reward:     2.1141\n",
      "   - Final RLHF Reward:      2.6427\n",
      "   - Am√©lioration:           25.0%\n",
      "   - Final KL Divergence:    nan\n",
      "\n",
      "üéØ Objectifs Atteints:\n",
      "   ‚úÖ Reward Model entra√Æn√© sur pr√©f√©rences humaines\n",
      "   ‚úÖ Pipeline PPO impl√©ment√©\n",
      "   ‚úÖ RLHF training loop ex√©cut√©\n",
      "   ‚úÖ Mod√®le align√© avec les pr√©f√©rences\n",
      "   ‚úÖ KL divergence maintenue sous contr√¥le\n",
      "\n",
      "üöÄ Prochaines √©tapes:\n",
      "   ‚Üí D√©ployer le mod√®le align√© en production\n",
      "   ‚Üí Tester sur un benchmark d'√©valuation plus large\n",
      "   ‚Üí Fine-tuner les hyperparam√®tres PPO\n",
      "   ‚Üí Collecter plus de donn√©es de pr√©f√©rences humaines\n",
      "\n",
      "\n",
      "‚úÖ Tous les fichiers sauvegard√©s!\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 10: Sauvegarde Finale\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAUVEGARDE DES MOD√àLES ALIGN√âS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDER LE MOD√àLE RLHF (POLICY)\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ Mod√®le Policy (RLHF-aligned)...\")\n",
    "\n",
    "os.makedirs(\"models/alignment\", exist_ok=True)\n",
    "\n",
    "# On sauvegarde tout ce qui est n√©cessaire pour reprendre l'entra√Ænement ou faire de l'inf√©rence\n",
    "aligned_checkpoint = {\n",
    "    'model_state_dict': policy_model.state_dict(), # Les poids du mod√®le optimis√©\n",
    "    'config': {\n",
    "        'vocab_size': config.vocab_size,\n",
    "        'd_model': config.d_model,\n",
    "        'n_heads': config.n_heads,\n",
    "        'n_layers': config.n_layers,\n",
    "        'd_ff': config.d_ff,\n",
    "        'block_size': config.block_size\n",
    "    },\n",
    "    'training_stage': 'alignment-rlhf',\n",
    "    'rlhf_history': rlhf_history, # Historique des m√©triques (Reward, KL, Loss)\n",
    "    'reward_history': reward_history\n",
    "}\n",
    "\n",
    "policy_path = \"models/alignment/model_rlhf_aligned.pt\"\n",
    "torch.save(aligned_checkpoint, policy_path)\n",
    "print(f\"   ‚úÖ Sauvegard√©: {policy_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDER LE MOD√àLE DE R√âCOMPENSE\n",
    "# ============================================================================\n",
    "print(\"\\n2Ô∏è‚É£ Mod√®le de R√©compense...\")\n",
    "\n",
    "# Utile si on veut r√©utiliser ce reward model pour aligner d'autres mod√®les\n",
    "reward_checkpoint = {\n",
    "    'model_state_dict': reward_model.state_dict(),\n",
    "    'config': config.__dict__,\n",
    "    'training_history': reward_history\n",
    "}\n",
    "\n",
    "reward_path = \"models/alignment/reward_model_final.pt\"\n",
    "torch.save(reward_checkpoint, reward_path)\n",
    "print(f\"   ‚úÖ Sauvegard√©: {reward_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDER LE TOKENIZER\n",
    "# ============================================================================\n",
    "print(\"\\n3Ô∏è‚É£ Tokenizer...\")\n",
    "\n",
    "# Toujours sauvegarder le tokenizer car on a ajout√© des tokens sp√©ciaux\n",
    "tokenizer.save_pretrained(\"models/alignment/tokenizer\")\n",
    "print(f\"   ‚úÖ Sauvegard√©: models/alignment/tokenizer/\")\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âER UN R√âSUM√â TEXTUEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ R√âSUM√â DES ARTEFACTS CR√â√âS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "‚úÖ ALIGNMENT (RLHF) - Stage 3 Compl√©t√©e\n",
    "\n",
    "üìÅ Fichiers cr√©√©s:\n",
    "‚îú‚îÄ‚îÄ models/alignment/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_rlhf_aligned.pt          ‚úÖ Mod√®le final align√© (Policy)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_final.pt          ‚úÖ Mod√®le de r√©compense (Critic)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ reward_model_epoch_[1-{REWARD_EPOCHS}].pt    ‚úÖ Checkpoints interm√©diaires\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/                     ‚úÖ Tokenizer avec tokens sp√©ciaux\n",
    "\n",
    "üìä M√©triques de Performance:\n",
    "   - Initial SFT Reward:     {rlhf_history['avg_rewards'][0]:.4f}\n",
    "   - Final RLHF Reward:      {rlhf_history['avg_rewards'][-1]:.4f}\n",
    "   - Am√©lioration:           {(rlhf_history['avg_rewards'][-1] - rlhf_history['avg_rewards'][0]) / abs(rlhf_history['avg_rewards'][0]) * 100:.1f}%\n",
    "   - Final KL Divergence:    {rlhf_history['kl_divs'][-1]:.6f}\n",
    "\n",
    "üéØ Objectifs Atteints:\n",
    "   ‚úÖ Reward Model entra√Æn√© sur pr√©f√©rences humaines (simul√©es)\n",
    "   ‚úÖ Pipeline PPO impl√©ment√© (Actor-Critic)\n",
    "   ‚úÖ RLHF training loop ex√©cut√©\n",
    "   ‚úÖ Mod√®le align√© avec les pr√©f√©rences\n",
    "   ‚úÖ KL divergence maintenue sous contr√¥le (pas d'oubli catastrophique)\n",
    "\n",
    "üöÄ Prochaines √©tapes:\n",
    "   ‚Üí D√©ployer le mod√®le align√© en production\n",
    "   ‚Üí Tester sur un benchmark d'√©valuation plus large\n",
    "   ‚Üí Fine-tuner les hyperparam√®tres PPO\n",
    "   ‚Üí Collecter plus de donn√©es de pr√©f√©rences humaines r√©elles\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Sauvegarder le r√©sum√©\n",
    "with open(\"models/alignment/ALIGNMENT_SUMMARY.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n‚úÖ Tous les fichiers sauvegard√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e1e21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ R√©sum√© Complet : Alignment (RLHF)\n",
    "\n",
    "### ‚úÖ √âtapes Accomplies\n",
    "\n",
    "1. **Chargement du Mod√®le SFT** : Mod√®le post-entra√Æn√© comme politique\n",
    "2. **Architecture du Reward Model** : R√©seau neural pour scorer les g√©n√©rations\n",
    "3. **Dataset de Pr√©f√©rences** : Paires de codes avec labels de pr√©f√©rence\n",
    "4. **Entra√Ænement du Reward Model** : Bradley-Terry loss pour apprendre les pr√©f√©rences\n",
    "5. **Impl√©mentation PPO** : Algorithme d'optimisation par renforcement\n",
    "6. **Pipeline RLHF** : Boucle d'optimisation du mod√®le avec feedback\n",
    "7. **√âvaluation Comparative** : SFT vs RLHF\n",
    "8. **Sauvegarde** : Mod√®les align√©s et artefacts\n",
    "\n",
    "### üìä Pipeline Complet\n",
    "\n",
    "```\n",
    "Mod√®le SFT (Post-Training)\n",
    "    ‚Üì\n",
    "G√©n√©rer Samples (Rollouts)\n",
    "    ‚Üì\n",
    "Score with Reward Model\n",
    "    ‚Üì\n",
    "Compute Advantages\n",
    "    ‚Üì\n",
    "PPO Update\n",
    "    ‚Üì\n",
    "Mod√®le RLHF (Aligned)\n",
    "```\n",
    "\n",
    "### üéÅ Fichiers Finaux\n",
    "\n",
    "```\n",
    "models/alignment/\n",
    "‚îú‚îÄ‚îÄ model_rlhf_aligned.pt              # ‚úÖ Mod√®le final align√©\n",
    "‚îú‚îÄ‚îÄ reward_model_final.pt              # ‚úÖ Reward model entra√Æn√©\n",
    "‚îú‚îÄ‚îÄ reward_model_epoch_[1-3].pt        # Checkpoints interm√©diaires\n",
    "‚îú‚îÄ‚îÄ tokenizer/                         # Tokenizer avec tokens sp√©ciaux\n",
    "‚îî‚îÄ‚îÄ ALIGNMENT_SUMMARY.txt              # R√©sum√© des r√©sultats\n",
    "```\n",
    "\n",
    "### üöÄ Utilisation du Mod√®le Align√©\n",
    "\n",
    "```python\n",
    "# Charger le mod√®le align√©\n",
    "checkpoint = torch.load('models/alignment/model_rlhf_aligned.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# G√©n√©rer avec le mod√®le align√©\n",
    "prompt = \"<instruction> Write a function to calculate factorial <reasoning>\"\n",
    "output = model.generate(prompt_ids, max_new_tokens=150)\n",
    "text = tokenizer.decode(output[0].tolist())\n",
    "```\n",
    "\n",
    "### üìà Am√©liorations Attendues\n",
    "\n",
    "- ‚úÖ **Meilleure qualit√© de code** : Align√© avec pr√©f√©rences humaines\n",
    "- ‚úÖ **Meilleure suivi d'instructions** : Compr√©hension accrue des consignes\n",
    "- ‚úÖ **Raisonnement am√©lior√©** : Processus de pens√©e plus structur√©\n",
    "- ‚úÖ **Stabilit√©** : KL divergence maintenue proche du mod√®le SFT\n",
    "\n",
    "### üîÑ Prochaines √âtapes\n",
    "\n",
    "1. **D√©ploiement** : Int√©grer dans le dashboard\n",
    "2. **Benchmark** : √âvaluation sur benchmark standard (HumanEval)\n",
    "3. **It√©ration** : Collecter plus de feedback humain\n",
    "4. **Fine-tuning** : Hyperparam√®tres PPO additionnels\n",
    "5. **Production** : A/B testing et monitoring\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aapl_ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
