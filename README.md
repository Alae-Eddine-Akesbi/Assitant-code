# ğŸ§  Mini-GPT: Coding LLM from Scratch

<div align="center">

**Un modÃ¨le de langage spÃ©cialisÃ© pour la gÃ©nÃ©ration de code Python**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-FF4B4B.svg)](https://streamlit.io/)

[Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Dashboard](#-dashboard) â€¢ [Architecture](#-architecture) â€¢ [Results](#-results)

</div>

---

## ğŸ“‹ Table des MatiÃ¨res

- [Ã€ Propos](#-Ã -propos)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Structure du Projet](#-structure-du-projet)
- [Pipeline d'EntraÃ®nement](#-pipeline-dentraÃ®nement)
- [Dashboard Interactif](#-dashboard-interactif)
- [Architecture](#-architecture)
- [RÃ©sultats](#-rÃ©sultats)
- [Utilisation AvancÃ©e](#-utilisation-avancÃ©e)
- [Troubleshooting](#-troubleshooting)
- [Contribution](#-contribution)
- [Licence](#-licence)

---

## ğŸ¯ Ã€ Propos

Ce projet implÃ©mente un **modÃ¨le de langage transformer de type GPT** entraÃ®nÃ© from scratch pour la gÃ©nÃ©ration de code Python. Le modÃ¨le suit un pipeline d'entraÃ®nement en 3 phases :

1. **Pre-Training** : Apprentissage non supervisÃ© sur du code Python (CLM)
2. **Post-Training** : Fine-tuning supervisÃ© avec des instructions (SFT)
3. **Alignment** : Optimisation par prÃ©fÃ©rences humaines (RLHF) - *Ã€ venir*

### CaractÃ©ristiques Principales

- âœ… **Architecture GPT** : Decoder-only Transformer (4 layers, 256 dims)
- âœ… **~300K paramÃ¨tres** : Petit mais performant
- âœ… **Training complet** : Pre-training + SFT implÃ©mentÃ©s
- âœ… **Dashboard Streamlit** : Interface de comparaison interactive
- âœ… **100% PyTorch** : Code clair et pÃ©dagogique

---

## âœ¨ FonctionnalitÃ©s

### ModÃ¨les EntraÃ®nÃ©s

| ModÃ¨le | Dataset | Taille | CapacitÃ© |
|--------|---------|--------|----------|
| **Pre-Training** | 100k+ fichiers Python (The Stack) | ~38 MB | ComplÃ©tion de code |
| **Post-Training** | 10k paires instruction-code | ~38 MB | GÃ©nÃ©ration Ã  partir d'instructions |

### Dashboard Streamlit

- ğŸ”„ Comparaison cÃ´te-Ã -cÃ´te de 3 modÃ¨les
- ğŸ“Š Graphiques interactifs (Plotly)
- ğŸ›ï¸ ContrÃ´le des paramÃ¨tres de gÃ©nÃ©ration
- ğŸ“ˆ MÃ©triques en temps rÃ©el (Loss, Perplexity, Temps)
- âœ¨ Interface moderne et animÃ©e

### Notebooks Complets

- ğŸ““ `1_pre_training.ipynb` : EntraÃ®nement CLM dÃ©taillÃ©
- ğŸ““ `2_post_training.ipynb` : SFT avec instructions structurÃ©es
- ğŸ“š Documentation extensive avec explications pÃ©dagogiques

---

## ğŸ”§ Installation

### PrÃ©requis

- Python 3.8+
- CUDA (optionnel, pour GPU)
- ~5 GB d'espace disque

### Installation Rapide

```bash
# Cloner le repo
git clone https://github.com/votre-repo/mini-gpt-coding.git
cd mini-gpt-coding

# Installer les dÃ©pendances
pip install -r requirements.txt

# (Optionnel) Installer Streamlit pour le dashboard
pip install streamlit plotly
```

### DÃ©pendances Principales

```
torch>=2.0.0
transformers>=4.30.0
datasets>=2.12.0
huggingface-hub>=0.15.0
streamlit>=1.28.0
plotly>=5.17.0
```

---

## ğŸš€ Quick Start

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ EntraÃ®nement Complet (RecommandÃ©)

```bash
# 1. Pre-Training (CLM sur code Python)
jupyter notebook 1_pre_training.ipynb
# ExÃ©cuter toutes les cellules â†’ Sortie: models/pre_training/

# 2. Post-Training (SFT avec instructions)
jupyter notebook 2_post_training.ipynb
# ExÃ©cuter toutes les cellules â†’ Sortie: models/post_training/

# 3. Lancer le Dashboard
streamlit run dashboard.py
```

### 2ï¸âƒ£ Utilisation Rapide (ModÃ¨les prÃ©-entraÃ®nÃ©s)

```python
import torch
from transformers import GPT2Tokenizer

# Charger le modÃ¨le post-entraÃ®nÃ©
checkpoint = torch.load('models/post_training/mini_gpt_sft_FINAL.pt')
tokenizer = GPT2Tokenizer.from_pretrained('models/post_training/tokenizer')

# GÃ©nÃ©rer du code
prompt = "<instruction> Write a function to calculate factorial <reasoning>"
inputs = tokenizer.encode(prompt, return_tensors='pt')
outputs = model.generate(inputs, max_new_tokens=150, temperature=0.7)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ“ Structure du Projet

```
ğŸ“¦ mini-gpt-coding/
â”‚
â”œâ”€â”€ ğŸ““ 1_pre_training.ipynb          # Pre-Training (CLM)
â”œâ”€â”€ ğŸ““ 2_post_training.ipynb         # Post-Training (SFT)
â”œâ”€â”€ ğŸ¨ dashboard.py                   # Dashboard Streamlit
â”‚
â”œâ”€â”€ âš™ï¸  config.py                      # Configuration centralisÃ©e
â”œâ”€â”€ ğŸ“¦ requirements.txt               # DÃ©pendances Python
â”œâ”€â”€ ğŸ“– README.md                      # Ce fichier
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          # Datasets
â”‚   â””â”€â”€ python_reasoning_dataset.jsonl
â”‚
â”œâ”€â”€ ğŸ“‚ models/                        # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ pre_training/
â”‚   â”‚   â”œâ”€â”€ mini_gpt_code_FINAL.pt   # â† ModÃ¨le prÃ©-entraÃ®nÃ©
â”‚   â”‚   â””â”€â”€ tokenizer/
â”‚   â””â”€â”€ post_training/
â”‚       â”œâ”€â”€ mini_gpt_sft_FINAL.pt    # â† ModÃ¨le post-entraÃ®nÃ©
â”‚       â””â”€â”€ tokenizer/
â”‚
â””â”€â”€ ğŸ“‚ outputs/                       # Artefacts temporaires
    â””â”€â”€ mini_corpus_mixed.txt
```

---

## ğŸ”„ Pipeline d'EntraÃ®nement

### Phase 1: Pre-Training (CLM)

**Objectif** : Apprendre la syntaxe Python et les patterns de code

```python
# Dataset: The Stack (100k+ fichiers Python)
# MÃ©thode: Causal Language Modeling
# DurÃ©e: ~30 min (CPU) | ~5 min (GPU)

# RÃ©sultat:
# âœ… Validation Loss: ~2.3
# âœ… Perplexity: ~10.4
# âœ… Capable de complÃ©ter du code Python
```

**Notebook** : `1_pre_training.ipynb`

### Phase 2: Post-Training (SFT)

**Objectif** : Apprendre Ã  suivre des instructions

```python
# Dataset: 10k paires instruction-reasoning-code
# Format: <instruction> X <reasoning> Y <answer> Z
# DurÃ©e: ~20 min (CPU) | ~3 min (GPU)

# RÃ©sultat:
# âœ… Validation Loss: ~1.8
# âœ… Perplexity: ~6.2
# âœ… GÃ©nÃ©ration Ã  partir d'instructions naturelles
```

**Notebook** : `2_post_training.ipynb`

### Phase 3: Alignment (RLHF)

**Status** : ğŸš§ En dÃ©veloppement

Prochaines Ã©tapes :
- Collecte de prÃ©fÃ©rences humaines
- EntraÃ®nement d'un reward model
- Optimisation PPO

---

## ğŸ¨ Dashboard Interactif

### Lancement

```bash
streamlit run dashboard.py
```

Le dashboard s'ouvre automatiquement Ã  **http://localhost:8501**

### FonctionnalitÃ©s

<div align="center">

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Comparaison Multi-ModÃ¨les** | Pre-Training vs Post-Training vs Alignment |
| ğŸ›ï¸ **ContrÃ´les Dynamiques** | Temperature, Top-K, Max Tokens |
| ğŸ“Š **Graphiques Plotly** | Temps, Loss, Perplexity, Longueur |
| âœ¨ **Interface AnimÃ©e** | Cartes hover, fade-in, slide-in |
| ğŸ“ **Exemples PrÃ©-dÃ©finis** | Fibonacci, Factorial, QuickSort, etc. |

</div>

### Captures d'Ã‰cran

**Interface Principale**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  LLM Coding Assistant Dashboard                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ’¬ Entrez votre Prompt                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ <instruction> Write factorial function         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  [ ğŸš€ GÃ©nÃ©rer avec tous les modÃ¨les ]                  â”‚
â”‚                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”µ Pre-Training  â”‚ ğŸŸ£ Post-Training â”‚ ğŸ”· Alignment   â”‚
â”‚  â±ï¸ 0.45s          â”‚ â±ï¸ 0.52s         â”‚ â±ï¸ N/A         â”‚
â”‚  ğŸ“‰ Loss: 2.34    â”‚ ğŸ“‰ Loss: 1.82   â”‚ ğŸš§ En dev.    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Architecture

### Mini-GPT Model

```python
Mini-GPT (Decoder-only Transformer)
â”‚
â”œâ”€â”€ Token Embedding (50,260 â†’ 256)
â”œâ”€â”€ Position Embedding (256 â†’ 256)
â”œâ”€â”€ Dropout (0.1)
â”‚
â”œâ”€â”€ Transformer Blocks (x4)
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â”œâ”€â”€ Multi-Head Attention (4 heads)
â”‚   â”‚   â”œâ”€â”€ Query, Key, Value projections
â”‚   â”‚   â”œâ”€â”€ Causal masking
â”‚   â”‚   â””â”€â”€ Attention dropout
â”‚   â”œâ”€â”€ Layer Norm
â”‚   â””â”€â”€ Feed-Forward Network
â”‚       â”œâ”€â”€ Linear (256 â†’ 1024)
â”‚       â”œâ”€â”€ GELU activation
â”‚       â”œâ”€â”€ Linear (1024 â†’ 256)
â”‚       â””â”€â”€ Dropout
â”‚
â”œâ”€â”€ Final Layer Norm
â””â”€â”€ Language Model Head (256 â†’ 50,260)
    â””â”€â”€ Weight tying with Token Embedding
```

### SpÃ©cifications Techniques

| ParamÃ¨tre | Valeur |
|-----------|--------|
| **Architecture** | Decoder-only Transformer |
| **ParamÃ¨tres** | ~300,000 |
| **Dimensions** | 256 |
| **Attention Heads** | 4 |
| **Layers** | 4 |
| **Context Length** | 256 tokens |
| **Vocabulaire** | 50,260 (GPT-2 + tokens spÃ©ciaux) |
| **Activation** | GELU |
| **Dropout** | 0.1 |

### Tokenizer

- **Base** : GPT-2 BPE Tokenizer (50,257 tokens)
- **Tokens spÃ©ciaux** : `<instruction>`, `<reasoning>`, `<answer>`
- **Vocabulaire final** : 50,260 tokens

---

## ğŸ“Š RÃ©sultats

### MÃ©triques d'EntraÃ®nement

| ModÃ¨le | Dataset | Epochs | Val Loss | Perplexity | Temps (GPU) |
|--------|---------|--------|----------|------------|-------------|
| **Pre-Training** | 100k files | 3 | 2.34 | 10.4 | ~5 min |
| **Post-Training** | 10k pairs | 5 | 1.82 | 6.2 | ~3 min |

### Exemples de GÃ©nÃ©ration

#### Pre-Training (Code Completion)

**Input:**
```python
def fibonacci(n):
```

**Output:**
```python
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
```

#### Post-Training (Instruction Following)

**Input:**
```
<instruction> Write a function to calculate factorial <reasoning>
```

**Output:**
```python
<instruction> Write a function to calculate factorial 
<reasoning> Use recursive approach with base case n=0 or n=1
<answer> 
def factorial(n):
    if n == 0 or n == 1:
        return 1
    else:
        return n * factorial(n-1)
```

### Comparaison des ModÃ¨les

| CritÃ¨re | Pre-Training | Post-Training |
|---------|--------------|---------------|
| **ComplÃ¨te le code** | âœ… Excellent | âœ… Excellent |
| **Suit les instructions** | âŒ Non | âœ… Oui |
| **Ajoute du raisonnement** | âŒ Non | âœ… Oui |
| **Code structurÃ©** | âš ï¸ Variable | âœ… CohÃ©rent |

---

## ğŸ”¬ Utilisation AvancÃ©e

### Charger un ModÃ¨le SpÃ©cifique

```python
import torch
import torch.nn as nn
from transformers import GPT2Tokenizer

# DÃ©finir l'architecture (voir notebooks pour le code complet)
from model import MiniGPT  # ou copier depuis les notebooks

# Charger le checkpoint
checkpoint = torch.load('models/post_training/mini_gpt_sft_FINAL.pt')
config = checkpoint['config']

# CrÃ©er le modÃ¨le
model = MiniGPT(
    vocab_size=config['vocab_size'],
    block_size=config['block_size'],
    n_embd=config['n_embd'],
    n_head=config['n_head'],
    n_layer=config['n_layer'],
    dropout=config['dropout']
)

# Charger les poids
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Charger le tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('models/post_training/tokenizer')
```

### GÃ©nÃ©ration avec ContrÃ´le Fin

```python
def generate_code(prompt, max_tokens=150, temperature=0.7, top_k=50):
    """GÃ©nÃ¨re du code avec paramÃ¨tres personnalisÃ©s"""
    
    # Encoder
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # GÃ©nÃ©rer
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=max_tokens,
            temperature=temperature,  # 0.1-2.0 (crÃ©ativitÃ©)
            top_k=top_k,              # 10-100 (diversitÃ©)
            do_sample=True
        )
    
    # DÃ©coder
    return tokenizer.decode(output_ids[0])

# Exemples
generate_code("def quicksort(arr):", temperature=0.5)  # Plus dÃ©terministe
generate_code("class BinaryTree:", temperature=1.2)     # Plus crÃ©atif
```

### Export pour Production

```python
# Exporter uniquement les poids (plus lÃ©ger)
torch.save(
    model.state_dict(), 
    'models/mini_gpt_production.pt'
)

# Quantization (rÃ©duction de taille)
import torch.quantization as quant
model_quantized = quant.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)
```

---

## ğŸ› ï¸ Troubleshooting

### ProblÃ¨mes Courants

#### "CUDA out of memory"

**Solution** : RÃ©duire `BATCH_SIZE` dans les notebooks
```python
BATCH_SIZE = 8  # au lieu de 16
```

#### "FileNotFoundError: models/pre_training/..."

**Solution** : ExÃ©cuter d'abord `1_pre_training.ipynb` complÃ¨tement

#### "Dataset not accessible"

**Solution** : S'authentifier sur HuggingFace
```python
from huggingface_hub import login
login(token="hf_YOUR_TOKEN")
```

#### Dashboard lent

**Solutions** :
- RÃ©duire `max_tokens` (50-100)
- Utiliser `temperature=0.5` (plus rapide)
- DÃ©sactiver les modÃ¨les non nÃ©cessaires

### Performance

| Device | Pre-Training | Post-Training | GÃ©nÃ©ration |
|--------|--------------|---------------|------------|
| **CPU** | ~30 min | ~20 min | ~2s/sample |
| **GPU (T4)** | ~5 min | ~3 min | ~0.3s/sample |
| **GPU (V100)** | ~2 min | ~1 min | ~0.1s/sample |

---

## ğŸ¤ Contribution

Contributions bienvenues ! Voici comment participer :

1. **Fork** le projet
2. **CrÃ©er** une branche (`git checkout -b feature/AmazingFeature`)
3. **Commit** vos changements (`git commit -m 'Add AmazingFeature'`)
4. **Push** vers la branche (`git push origin feature/AmazingFeature`)
5. **Ouvrir** une Pull Request

### Roadmap

- [ ] ImplÃ©mentation RLHF complÃ¨te
- [ ] Support pour d'autres langages (JavaScript, Java)
- [ ] API REST avec FastAPI
- [ ] Docker container
- [ ] Tests unitaires
- [ ] CI/CD avec GitHub Actions

---

## ğŸ“š Ressources

### Documentation

- [PyTorch Documentation](https://pytorch.org/docs/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [Streamlit Docs](https://docs.streamlit.io/)

### Papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformers)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
- [Training language models to follow instructions](https://arxiv.org/abs/2203.02155) (InstructGPT)

### Datasets

- [The Stack](https://huggingface.co/datasets/bigcode/the-stack) - Code source
- [CodeParrot](https://huggingface.co/codeparrot) - Python code

---

## ğŸ“„ Licence

Ce projet est sous licence **MIT**. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¥ Auteurs

**Ã‰quipe IRA**  
Workshop: Build a Coding LLM from Scratch  
Date: DÃ©cembre 2025

---

## ğŸ™ Remerciements

- [HuggingFace](https://huggingface.co/) pour les datasets et tokenizers
- [BigCode](https://www.bigcode-project.org/) pour The Stack
- [PyTorch Team](https://pytorch.org/) pour le framework
- [Streamlit](https://streamlit.io/) pour le dashboard

---

<div align="center">

**â­ Si ce projet vous est utile, n'hÃ©sitez pas Ã  lui donner une Ã©toile !**

Made with â¤ï¸ by Ã‰quipe IRA

</div>
