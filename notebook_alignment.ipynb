{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49afe0b5",
   "metadata": {},
   "source": [
    "## üîπ Partie 1 : Introduction Th√©orique\n",
    "\n",
    "### Qu'est-ce que l'Alignement (RLHF) ?\n",
    "\n",
    "L'alignement vise √† corriger les comportements ind√©sirables du mod√®le (hallucinations, toxicit√©, r√©ponses verbeuses) qui persistent m√™me apr√®s le SFT.\n",
    "\n",
    "### Le Pipeline RLHF en 3 √âtapes\n",
    "\n",
    "| √âtape | Nom | Description | Objectif |\n",
    "|-------|-----|-------------|----------|\n",
    "| **1** | **Collecte de Pr√©f√©rences** | On g√©n√®re plusieurs r√©ponses pour un prompt et on demande (√† un humain ou une IA) de choisir la meilleure. | Cr√©er un dataset `(Prompt, Chosen, Rejected)` |\n",
    "| **2** | **Reward Model (RM)** | On entra√Æne un mod√®le √† pr√©dire un score de qualit√© (scalaire) pour une r√©ponse donn√©e. | Apprendre √† distinguer le \"bon\" du \"mauvais\" |\n",
    "| **3** | **PPO (RL Loop)** | On utilise le RM pour guider le mod√®le de langage via l'apprentissage par renforcement. | Maximiser le score de r√©compense tout en restant coh√©rent |\n",
    "\n",
    "### Sch√©ma du Processus\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Mod√®le SFT] --> B[G√©n√©ration de R√©ponses]\n",
    "    B --> C{Pr√©f√©rence Humaine}\n",
    "    C -->|Gagnant| D[Chosen]\n",
    "    C -->|Perdant| E[Rejected]\n",
    "    D & E --> F[Entra√Ænement Reward Model]\n",
    "    F --> G[Boucle PPO]\n",
    "    G --> H[Mod√®le Align√© (InstructGPT)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65972b",
   "metadata": {},
   "source": [
    "## üîπ Partie 2 : Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports et Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DES BIBLIOTH√àQUES N√âCESSAIRES\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION POUR LA REPRODUCTIBILIT√â\n",
    "# ============================================================================\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ============================================================================\n",
    "# D√âTECTION DU DEVICE\n",
    "# ============================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Device utilis√© : {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHEMINS DES FICHIERS\n",
    "# ============================================================================\n",
    "SFT_MODEL_PATH = \"models/post_training/model_sft_FINAL.pt\"\n",
    "TOKENIZER_PATH = \"models/post_training/tokenizer/\"\n",
    "\n",
    "print(f\"üìÇ Mod√®le SFT cible : {SFT_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a594e",
   "metadata": {},
   "source": [
    "## üîπ Partie 3 : Chargement du Mod√®le SFT\n",
    "\n",
    "Nous devons recharger l'architecture exacte utilis√©e lors du Post-Training (`TinyDecoderLM`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77881c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: D√©finition de l'Architecture (Identique au Post-Training)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION DU MOD√àLE\n",
    "# ============================================================================\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 50260 # Ajust√© pour inclure les tokens sp√©ciaux\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 8\n",
    "    d_ff: int = 2048\n",
    "    block_size: int = 256\n",
    "\n",
    "# ============================================================================\n",
    "# MODULES DU TRANSFORMER\n",
    "# ============================================================================\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv = torch.nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = torch.nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size)\n",
    "        self.ln2 = torch.nn.LayerNorm(d_model)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyDecoderLM(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tok_emb = torch.nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = torch.nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            Block(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.ln_f = torch.nn.LayerNorm(cfg.d_model)\n",
    "        self.head = torch.nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.cfg.vocab_size), targets.view(-1), ignore_index=-100)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "print(\"‚úÖ Architecture TinyDecoderLM d√©finie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea063863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Chargement du Tokenizer et du Mod√®le\n",
    "\n",
    "print(\"üì• Chargement du Tokenizer et du Mod√®le SFT...\")\n",
    "\n",
    "# 1. Charger le Tokenizer\n",
    "if os.path.exists(TOKENIZER_PATH):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    print(f\"‚úÖ Tokenizer charg√© depuis {TOKENIZER_PATH}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tokenizer non trouv√©, chargement par d√©faut (gpt-neox-20b)\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Ajouter les tokens sp√©ciaux si n√©cessaire\n",
    "    special_tokens = {'additional_special_tokens': ['<instruction>', '<reasoning>', '<answer>']}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(f\"üìö Taille du vocabulaire : {vocab_size}\")\n",
    "\n",
    "# 2. Charger le Mod√®le SFT\n",
    "config = ModelConfig(vocab_size=vocab_size)\n",
    "model_sft = TinyDecoderLM(config).to(device)\n",
    "\n",
    "if os.path.exists(SFT_MODEL_PATH):\n",
    "    checkpoint = torch.load(SFT_MODEL_PATH, map_location=device)\n",
    "    # G√©rer les diff√©rences de cl√©s si n√©cessaire (prefixe 'module.' etc)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model_sft.load_state_dict(state_dict)\n",
    "    print(f\"‚úÖ Mod√®le SFT charg√© depuis {SFT_MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Fichier {SFT_MODEL_PATH} introuvable. Initialisation al√©atoire pour d√©mo.\")\n",
    "\n",
    "model_sft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae747756",
   "metadata": {},
   "source": [
    "## üîπ √âtape 1 : Collecte de Donn√©es de Pr√©f√©rence (Good/Bad)\n",
    "\n",
    "Nous simulons un dataset de pr√©f√©rences o√π pour chaque prompt, nous avons une r√©ponse \"choisie\" (Chosen) et une r√©ponse \"rejet√©e\" (Rejected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Cr√©ation du Dataset de Pr√©f√©rences\n",
    "\n",
    "# Simulation de donn√©es (Prompt, Chosen, Rejected)\n",
    "# Dans la r√©alit√©, ces donn√©es proviennent d'annotations humaines\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"<instruction> Write a function to add two numbers <reasoning>\",\n",
    "        \"chosen\": \"def add(a, b):\\n    return a + b\",\n",
    "        \"rejected\": \"def add(a, b):\\n    print('Adding')\\n    return a + b\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"<instruction> Create a list of squares <reasoning>\",\n",
    "        \"chosen\": \"[x**2 for x in range(10)]\",\n",
    "        \"rejected\": \"l = []\\nfor i in range(10):\\n    l.append(i*i)\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"<instruction> Check if even <reasoning>\",\n",
    "        \"chosen\": \"def is_even(n): return n % 2 == 0\",\n",
    "        \"rejected\": \"def is_even(n):\\n    if n % 2 == 0:\\n        return True\\n    else:\\n        return False\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"<instruction> Import pandas <reasoning>\",\n",
    "        \"chosen\": \"import pandas as pd\",\n",
    "        \"rejected\": \"import pandas\"\n",
    "    }\n",
    "] * 10 # Dupliquer pour avoir un peu de volume\n",
    "\n",
    "print(f\"üìù Dataset de pr√©f√©rences cr√©√© : {len(preference_data)} exemples\")\n",
    "print(f\"Exemple 1 Chosen: {preference_data[0]['chosen']}\")\n",
    "print(f\"Exemple 1 Rejected: {preference_data[0]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea34d",
   "metadata": {},
   "source": [
    "## üîπ √âtape 2 : Reward Model (RM)\n",
    "\n",
    "Le Reward Model est un mod√®le qui prend une s√©quence en entr√©e et retourne un **score scalaire**.\n",
    "Nous adaptons notre `TinyDecoderLM` en rempla√ßant la t√™te de vocabulaire par une t√™te de r√©gression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: D√©finition et Entra√Ænement du Reward Model\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model, d_model):\n",
    "        super().__init__()\n",
    "        # On r√©utilise les couches du mod√®le de base (Transfer Learning)\n",
    "        self.tok_emb = base_model.tok_emb\n",
    "        self.pos_emb = base_model.pos_emb\n",
    "        self.blocks = base_model.blocks\n",
    "        self.ln_f = base_model.ln_f\n",
    "        \n",
    "        # Nouvelle t√™te de score (projection vers 1 scalaire)\n",
    "        self.score_head = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # On utilise le dernier token pour pr√©dire le score de la s√©quence enti√®re\n",
    "        # (Padding token handling serait n√©cessaire dans un cas r√©el complexe)\n",
    "        last_token_hidden = x[:, -1, :]\n",
    "        score = self.score_head(last_token_hidden)\n",
    "        return score\n",
    "\n",
    "print(\"üèóÔ∏è Construction du Reward Model...\")\n",
    "# Initialiser le RM avec les poids du SFT\n",
    "reward_model = RewardModel(model_sft, config.d_model).to(device)\n",
    "\n",
    "# Optimiseur pour le RM\n",
    "optimizer_rm = torch.optim.AdamW(reward_model.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"üèãÔ∏è‚Äç‚ôÇÔ∏è Entra√Ænement du Reward Model (Bradley-Terry Loss)...\")\n",
    "rm_losses = []\n",
    "\n",
    "for epoch in range(3): # Quelques √©poques rapides\n",
    "    total_loss = 0\n",
    "    for ex in preference_data:\n",
    "        # Pr√©parer les inputs\n",
    "        # Format: Prompt + R√©ponse\n",
    "        text_chosen = ex['prompt'] + \" \" + ex['chosen']\n",
    "        text_rejected = ex['prompt'] + \" \" + ex['rejected']\n",
    "        \n",
    "        idx_chosen = torch.tensor([tokenizer.encode(text_chosen)], device=device)\n",
    "        idx_rejected = torch.tensor([tokenizer.encode(text_rejected)], device=device)\n",
    "        \n",
    "        # Forward\n",
    "        r_chosen = reward_model(idx_chosen)\n",
    "        r_rejected = reward_model(idx_rejected)\n",
    "        \n",
    "        # Loss: -log(sigmoid(r_chosen - r_rejected))\n",
    "        loss = -torch.log(torch.sigmoid(r_chosen - r_rejected))\n",
    "        \n",
    "        optimizer_rm.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_rm.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(preference_data)\n",
    "    rm_losses.append(avg_loss)\n",
    "    print(f\"   Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Reward Model entra√Æn√©.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c4036",
   "metadata": {},
   "source": [
    "## üîπ √âtape 3 : Boucle RLHF (PPO Simplifi√©)\n",
    "\n",
    "Nous utilisons maintenant le Reward Model pour guider la g√©n√©ration.\n",
    "Pour simplifier cette d√©mo, nous impl√©mentons une boucle **Policy Gradient** simple avec p√©nalit√© KL.\n",
    "\n",
    "**Objectif** : Maximiser $Reward(x, y) - \\beta \\cdot KL(\\pi_{\\theta} || \\pi_{ref})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc49a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Boucle RLHF\n",
    "\n",
    "# 1. Mod√®le de R√©f√©rence (Ref Model) - Fig√©\n",
    "# Sert √† calculer la divergence KL pour √©viter que le mod√®le ne s'√©loigne trop\n",
    "model_ref = TinyDecoderLM(config).to(device)\n",
    "model_ref.load_state_dict(model_sft.state_dict())\n",
    "model_ref.eval()\n",
    "for p in model_ref.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2. Optimiseur pour la Policy (Mod√®le SFT qu'on aligne)\n",
    "optimizer_ppo = torch.optim.AdamW(model_sft.parameters(), lr=1e-6)\n",
    "\n",
    "# Hyperparam√®tres RL\n",
    "BETA = 0.1 # Coefficient de p√©nalit√© KL\n",
    "STEPS = 50\n",
    "\n",
    "print(\"üöÄ D√©marrage de la boucle RLHF...\")\n",
    "\n",
    "prompts_rl = [\n",
    "    \"<instruction> Write a python function <reasoning>\",\n",
    "    \"<instruction> Import math library <reasoning>\",\n",
    "    \"<instruction> Define a class <reasoning>\"\n",
    "]\n",
    "\n",
    "history_rewards = []\n",
    "\n",
    "for step in range(STEPS):\n",
    "    # A. Rollout (G√©n√©ration)\n",
    "    prompt = random.choice(prompts_rl)\n",
    "    idx = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
    "    \n",
    "    # On g√©n√®re un seul token pour simplifier l'exemple PPO step\n",
    "    # Dans la r√©alit√©, on g√©n√®re une s√©quence enti√®re\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model_sft(idx)\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        action = torch.multinomial(probs, 1) # Token choisi\n",
    "    \n",
    "    # S√©quence compl√®te\n",
    "    idx_new = torch.cat((idx, action), dim=1)\n",
    "    \n",
    "    # B. √âvaluation (Reward)\n",
    "    reward = reward_model(idx_new).detach()\n",
    "    \n",
    "    # C. Calcul KL Divergence\n",
    "    with torch.no_grad():\n",
    "        logits_ref, _ = model_ref(idx)\n",
    "        probs_ref = F.softmax(logits_ref[:, -1, :], dim=-1)\n",
    "        prob_ref_token = probs_ref.gather(1, action)\n",
    "    \n",
    "    # Probabilit√© du token sous la policy actuelle\n",
    "    # On doit refaire un forward avec grad activ√©\n",
    "    logits_pol, _ = model_sft(idx)\n",
    "    probs_pol = F.softmax(logits_pol[:, -1, :], dim=-1)\n",
    "    prob_pol_token = probs_pol.gather(1, action)\n",
    "    \n",
    "    # KL approx: log(p_pol) - log(p_ref)\n",
    "    kl = torch.log(prob_pol_token) - torch.log(prob_ref_token)\n",
    "    \n",
    "    # Reward Total\n",
    "    total_reward = reward - BETA * kl\n",
    "    \n",
    "    # D. Update (Policy Gradient)\n",
    "    # Loss = -log(p) * R\n",
    "    loss = -torch.log(prob_pol_token) * total_reward\n",
    "    \n",
    "    optimizer_ppo.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_ppo.step()\n",
    "    \n",
    "    history_rewards.append(total_reward.item())\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"   Step {step}: Reward = {reward.item():.4f}, Total (w/ KL) = {total_reward.item():.4f}\")\n",
    "\n",
    "plt.plot(history_rewards)\n",
    "plt.title(\"√âvolution du Reward (RLHF)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756490b8",
   "metadata": {},
   "source": [
    "## üîπ Comparaison Finale\n",
    "\n",
    "Comparons la g√©n√©ration avant et apr√®s alignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Test de G√©n√©ration\n",
    "\n",
    "def generate(model, prompt):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
    "    with torch.no_grad():\n",
    "        # G√©n√©ration simple\n",
    "        for _ in range(20):\n",
    "            logits, _ = model(idx)\n",
    "            idx_next = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return tokenizer.decode(idx[0].tolist())\n",
    "\n",
    "test_prompt = \"<instruction> Write a function <reasoning>\"\n",
    "\n",
    "print(\"ü§ñ Mod√®le de R√©f√©rence (SFT - Avant RLHF):\")\n",
    "print(generate(model_ref, test_prompt))\n",
    "print(\"-\" * 50)\n",
    "print(\"‚ú® Mod√®le Align√© (Apr√®s RLHF):\")\n",
    "print(generate(model_sft, test_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec6b550",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "Nous avons compl√©t√© le pipeline :\n",
    "1.  **Pre-Training** : Apprentissage de la syntaxe.\n",
    "2.  **Post-Training** : Apprentissage des instructions.\n",
    "3.  **Alignment** : Optimisation par r√©compense.\n",
    "\n",
    "Le mod√®le est maintenant pr√™t √† √™tre d√©ploy√© ! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
