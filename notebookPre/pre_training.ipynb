{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e4b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TinyLM: A Tiny Language Model from Scratch\n",
    "===============================================\n",
    "- Code data: bigcode/the-stack-smol-python\n",
    "- NL data : HuggingFaceTB/smollm-corpus\n",
    "- 8-layer decoder-only transformer\n",
    "- Causal LM objective (next-token prediction)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e474bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(vocab_size=50257, d_model=512, n_heads=8, n_layers=8, d_ff=2048, block_size=256, batch_size=8, lr_max=0.0003, lr_min=1e-05, warmup_steps=1000, max_steps=80000, log_interval=100, eval_interval=2000, weight_decay=0.1, p_code=0.8, device='cuda', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. CONFIG\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # model\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 8\n",
    "    d_ff: int = 2048\n",
    "    block_size: int = 256\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 8\n",
    "    lr_max: float = 3e-4\n",
    "    lr_min: float = 1e-5\n",
    "    warmup_steps: int = 1_000\n",
    "    max_steps: int = 80_000\n",
    "    log_interval: int = 100\n",
    "    eval_interval: int = 2_000\n",
    "    weight_decay: float = 0.1\n",
    "\n",
    "    # data mix\n",
    "    p_code: float = 0.8\n",
    "\n",
    "    # runtime\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: torch.dtype = (\n",
    "        torch.bfloat16\n",
    "        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        else torch.float16\n",
    "    )\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2fc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. LEARNING RATE SCHEDULER\n",
    "# ============================================================\n",
    "\n",
    "def get_lr(step: int) -> float:\n",
    "    \"\"\"Cosine decay with warmup.\"\"\"\n",
    "    if step < cfg.warmup_steps:\n",
    "        return cfg.lr_max * step / cfg.warmup_steps\n",
    "\n",
    "    progress = (step - cfg.warmup_steps) / max(1, (cfg.max_steps - cfg.warmup_steps))\n",
    "    cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return cfg.lr_min + (cfg.lr_max - cfg.lr_min) * cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2f101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50254\n",
      "Pad token ID: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. TOKENIZER (GPT-NeoX SentencePiece tokenizer)\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use GPT-NeoX tokenizer â€” supports all UTF-8 (best for code + NL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "# Ensure a pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Update vocab size in config\n",
    "cfg.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(\"Tokenizer vocab size:\", cfg.vocab_size)\n",
    "print(\"Pad token ID:\", pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d18fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929a1f2de6564c1cad989ad92a070ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929a1f2de6564c1cad989ad92a070ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9444c8a391f4b898090aef36145e8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929a1f2de6564c1cad989ad92a070ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9444c8a391f4b898090aef36145e8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffering datasets into memory... (this may take ~10â€“30s)\n",
      "Buffered 10000 code samples and 100000 NL samples.\n",
      "Buffered 10000 code samples and 100000 NL samples.\n",
      "\n",
      "ðŸ“Š Data Statistics:\n",
      "  Code tokens: 2,291,279\n",
      "  NL tokens: 25,504,950\n",
      "  Total tokens: 27,796,229\n",
      "  Total samples: 110,000\n",
      "  Tokens per step: 2,040\n",
      "  Total training tokens: 163,200,000\n",
      "  Estimated epochs: 5.87\n",
      "\n",
      "ðŸ“Š Data Statistics:\n",
      "  Code tokens: 2,291,279\n",
      "  NL tokens: 25,504,950\n",
      "  Total tokens: 27,796,229\n",
      "  Total samples: 110,000\n",
      "  Tokens per step: 2,040\n",
      "  Total training tokens: 163,200,000\n",
      "  Estimated epochs: 5.87\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "ds_code = load_dataset(\n",
    "    \"bigcode/the-stack-smol\",\n",
    "    data_dir=\"data/python\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    "    )\n",
    "\n",
    "ds_nl = load_dataset(\n",
    "    \"HuggingFaceTB/smollm-corpus\",\n",
    "    \"cosmopedia-v2\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print(\"Buffering datasets into memory... (this may take ~11000â€“30s)\")\n",
    "\n",
    "MAX_BUF = 50_000   # adjust accordig to RAM; 50k is safe\n",
    "\n",
    "code_buf = [row[\"content\"] for row in ds_code.take(MAX_BUF)]\n",
    "nl_buf   = [row[\"text\"]     for row in ds_nl.take(MAX_BUF)]\n",
    "\n",
    "print(f\"Buffered {len(code_buf)} code samples and {len(nl_buf)} NL samples.\")\n",
    "\n",
    "# Calculate data statistics\n",
    "def count_tokens(texts):\n",
    "    total = 0\n",
    "    for text in texts:\n",
    "        ids = tokenizer(text, truncation=True, max_length=cfg.block_size, return_tensors=\"pt\").input_ids\n",
    "        total += ids.numel()\n",
    "    return total\n",
    "\n",
    "code_tokens = count_tokens(code_buf)\n",
    "nl_tokens = count_tokens(nl_buf)\n",
    "total_tokens = code_tokens + nl_tokens\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Statistics:\")\n",
    "print(f\"  Code tokens: {code_tokens:,}\")\n",
    "print(f\"  NL tokens: {nl_tokens:,}\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Total samples: {len(code_buf) + len(nl_buf):,}\")\n",
    "\n",
    "# Estimate epochs\n",
    "tokens_per_step = cfg.batch_size * (cfg.block_size - 1)\n",
    "total_tokens_training = cfg.max_steps * tokens_per_step\n",
    "num_epochs = total_tokens_training / total_tokens\n",
    "print(f\"  Tokens per step: {tokens_per_step:,}\")\n",
    "print(f\"  Total training tokens: {total_tokens_training:,}\")\n",
    "print(f\"  Estimated epochs: {num_epochs:.2f}\")\n",
    "\n",
    "def encode(text):\n",
    "      ids = tokenizer(\n",
    "          text,\n",
    "          truncation=True,\n",
    "          max_length=cfg.block_size,\n",
    "          padding=\"max_length\",\n",
    "          return_tensors=\"pt\",\n",
    "      ).input_ids.squeeze(0)\n",
    "      # Clamp token IDs to valid range\n",
    "      ids = torch.clamp(ids, 0, cfg.vocab_size - 1)\n",
    "      return ids\n",
    "\n",
    "def data_stream():\n",
    "    while True:\n",
    "        if random.random() < cfg.p_code:\n",
    "            text = random.choice(code_buf)\n",
    "        else:\n",
    "            text = random.choice(nl_buf)\n",
    "        ids = encode(text)\n",
    "        x = ids[:-1]\n",
    "        y = ids[1:]\n",
    "        yield x, y\n",
    "\n",
    "train_iter = cycle(data_stream())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673c333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 76.81M\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. MODEL\n",
    "# ============================================================\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        mask = torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(y)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyDecoderLM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(cfg.d_model, cfg.n_heads, cfg.d_ff, cfg.block_size)\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "model = TinyDecoderLM(cfg).to(cfg.device, memory_format=torch.contiguous_format)\n",
    "model = model.to(dtype=cfg.dtype)\n",
    "print(f\"Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae643e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. OPTIMIZER\n",
    "# ============================================================\n",
    "\n",
    "def create_optimizer(model):\n",
    "    decay, no_decay = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim >= 2 and \"bias\" not in name:\n",
    "            decay.append(p)\n",
    "        else:\n",
    "            no_decay.append(p)\n",
    "\n",
    "    groups = [\n",
    "        {\"params\": decay, \"weight_decay\": cfg.weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    fused = (\"fused\" in torch.optim.AdamW.__init__.__code__.co_varnames)\n",
    "\n",
    "    return torch.optim.AdamW(\n",
    "        groups,\n",
    "        lr=cfg.lr_max,\n",
    "        betas=(0.9, 0.95),\n",
    "        eps=1e-8,\n",
    "        fused=fused,\n",
    "    )\n",
    "\n",
    "optimizer = create_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a63ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸš€ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸš€ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 80000/80000 [1:54:01<00:00, 11.69step/s, epoch=5.87, loss=1.7743, ppl=5.90, lr=1.00e-05]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸš€ STARTING TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 80000/80000 [1:54:01<00:00, 11.69step/s, epoch=5.87, loss=1.7743, ppl=5.90, lr=1.00e-05]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… Training finished.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def get_batch():\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for _ in range(cfg.batch_size):\n",
    "        x, y = next(train_iter)        # x, y: (T,)\n",
    "        xs.append(x.unsqueeze(0))      # (1, T)\n",
    "        ys.append(y.unsqueeze(0))\n",
    "    x = torch.cat(xs, dim=0)          # (B, T)\n",
    "    y = torch.cat(ys, dim=0)          # (B, T)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def debug_check_batch(x, y, step, context=\"train\"):\n",
    "    \"\"\"\n",
    "    CPU-side sanity checks to catch issues that cause CUDA device-side asserts:\n",
    "    - target indices out of range\n",
    "    - wrong shapes\n",
    "    Run only for first few steps to keep overhead tiny.\n",
    "    \"\"\"\n",
    "    x_cpu = x.detach().cpu()\n",
    "    y_cpu = y.detach().cpu()\n",
    "\n",
    "    if x_cpu.ndim != 2 or y_cpu.ndim != 2:\n",
    "        raise ValueError(\n",
    "            f\"[{context}] step {step}: expected (B, T) tensors, \"\n",
    "            f\"got x.shape={tuple(x_cpu.shape)}, y.shape={tuple(y_cpu.shape)}\"\n",
    "        )\n",
    "\n",
    "    vmax_x = int(x_cpu.max().item())\n",
    "    vmin_x = int(x_cpu.min().item())\n",
    "    vmax_y = int(y_cpu.max().item())\n",
    "    vmin_y = int(y_cpu.min().item())\n",
    "\n",
    "    if vmin_x < 0 or vmin_y < 0 or vmax_x >= cfg.vocab_size or vmax_y >= cfg.vocab_size:\n",
    "        raise ValueError(\n",
    "            f\"[{context}] step {step}: token IDs out of range!\\n\"\n",
    "            f\"  x.min={vmin_x}, x.max={vmax_x}, \"\n",
    "            f\"  y.min={vmin_y}, y.max={vmax_y}, \"\n",
    "            f\"  cfg.vocab_size={cfg.vocab_size}\"\n",
    "        )\n",
    "\n",
    "    # extra: check logits last dim matches vocab\n",
    "    # (can't check here, but we'll assert after forward)\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(cfg.dtype == torch.float16))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "t0 = time.time()\n",
    "running_loss = 0.0\n",
    "\n",
    "# Calculate epoch info\n",
    "tokens_per_step = cfg.batch_size * (cfg.block_size - 1)\n",
    "\n",
    "pbar = tqdm(range(1, cfg.max_steps + 1), desc=\"Training\", unit=\"step\", \n",
    "            ncols=120, colour=\"green\", position=0, leave=True)\n",
    "\n",
    "for step in pbar:\n",
    "    \n",
    "    # Calculate current epoch\n",
    "    current_epoch = (step * tokens_per_step) / total_tokens\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # LR schedule\n",
    "    # --------------------------------------------------------\n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # batch\n",
    "    # --------------------------------------------------------\n",
    "    x, y = get_batch()            # (B, T), integer token IDs\n",
    "    x = x.to(cfg.device)\n",
    "    y = y.to(cfg.device)\n",
    "\n",
    "    # run sanity checks on first few steps (CPU-side)\n",
    "    if step <= 5:\n",
    "        debug_check_batch(x, y, step, context=\"train\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # forward pass\n",
    "    # --------------------------------------------------------\n",
    "    with torch.cuda.amp.autocast(enabled=(cfg.dtype == torch.float16)):\n",
    "        logits = model(x)         # (B, T, vocab_size)\n",
    "        if step <= 5:\n",
    "            # shape check for safety\n",
    "            if logits.ndim != 3 or logits.size(-1) != cfg.vocab_size:\n",
    "                raise ValueError(\n",
    "                    f\"[train] step {step}: logits shape invalid. \"\n",
    "                    f\"Expected (B, T, {cfg.vocab_size}), got {tuple(logits.shape)}\"\n",
    "                )\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            y.view(-1),\n",
    "            ignore_index=pad_token_id,\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # backward\n",
    "    # --------------------------------------------------------\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # logging\n",
    "    # --------------------------------------------------------\n",
    "    running_loss += loss.item()\n",
    "    if step % cfg.log_interval == 0:\n",
    "        avg = running_loss / cfg.log_interval\n",
    "        ppl = math.exp(avg) if avg < 20 else float(\"inf\")\n",
    "        elapsed = time.time() - t0\n",
    "        pbar.set_postfix({\n",
    "            \"epoch\": f\"{current_epoch:.2f}\", \n",
    "            \"loss\": f\"{avg:.4f}\", \n",
    "            \"ppl\": f\"{ppl:.2f}\", \n",
    "            \"lr\": f\"{lr:.2e}\"\n",
    "        })\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # eval + checkpoint\n",
    "    # --------------------------------------------------------\n",
    "    if step % cfg.eval_interval == 0 or step == cfg.max_steps:\n",
    "        model.eval()\n",
    "        eval_losses = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(32):\n",
    "                x_eval, y_eval = get_batch()   # ðŸ” use get_batch here too\n",
    "                x_eval = x_eval.to(cfg.device)\n",
    "                y_eval = y_eval.to(cfg.device)\n",
    "\n",
    "                if step <= 5:\n",
    "                    debug_check_batch(x_eval, y_eval, step, context=\"eval\")\n",
    "\n",
    "                logits_eval = model(x_eval)\n",
    "                if step <= 5:\n",
    "                    if logits_eval.ndim != 3 or logits_eval.size(-1) != cfg.vocab_size:\n",
    "                        raise ValueError(\n",
    "                            f\"[eval] step {step}: logits shape invalid. \"\n",
    "                            f\"Expected (B, T, {cfg.vocab_size}), got {tuple(logits_eval.shape)}\"\n",
    "                        )\n",
    "\n",
    "                eval_loss = F.cross_entropy(\n",
    "                    logits_eval.view(-1, logits_eval.size(-1)),\n",
    "                    y_eval.view(-1),\n",
    "                    ignore_index=pad_token_id,\n",
    "                )\n",
    "                eval_losses.append(eval_loss.item())\n",
    "\n",
    "        eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "        eval_ppl = math.exp(eval_loss) if eval_loss < 20 else float(\"inf\")\n",
    "\n",
    "        pbar.set_description(f\"Training [Epoch: {current_epoch:.2f} | eval loss: {eval_loss:.4f} | eval ppl: {eval_ppl:.2f}]\")\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"config\": cfg.__dict__,\n",
    "                \"tokenizer\": \"EleutherAI/gpt-neox-20b\",\n",
    "                \"step\": step,\n",
    "            },\n",
    "            f\"checkpoint_step{step}.pt\",\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        pbar.set_description(\"Training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Training finished.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70391914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model saved to: model_final.pt\n",
      "  - Model state dict\n",
      "  - Config: Config(vocab_size=50254, d_model=512, n_heads=8, n_layers=8, d_ff=2048, block_size=256, batch_size=8, lr_max=0.0003, lr_min=1e-05, warmup_steps=1000, max_steps=80000, log_interval=100, eval_interval=2000, weight_decay=0.1, p_code=0.8, device='cuda', dtype=torch.bfloat16)\n",
      "  - Tokenizer: EleutherAI/gpt-neox-20b\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. SAVE FINAL MODEL\n",
    "# ============================================================\n",
    "\n",
    "final_path = \"model_final.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model\": model.state_dict(),\n",
    "    \"config\": cfg.__dict__,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"tokenizer_name_or_path\": \"EleutherAI/gpt-neox-20b\",\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nFinal model saved to: {final_path}\")\n",
    "print(f\"  - Model state dict\")\n",
    "print(f\"  - Config: {cfg}\")\n",
    "print(f\"  - Tokenizer: EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15328b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“‚ LOADING MODEL\n",
      "================================================================================\n",
      "âœ“ Loaded config from model_final.pt\n",
      "âœ“ Loaded config from model_final.pt\n",
      "âœ“ Loaded model with 76.81M parameters\n",
      "âœ“ Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "ðŸ§ª TESTING MODEL GENERATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Prompt: 'def hello'\n",
      "âœ“ Loaded model with 76.81M parameters\n",
      "âœ“ Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "ðŸ§ª TESTING MODEL GENERATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Prompt: 'def hello'\n",
      "âœ“ Generated: 'def hello_world(context):\n",
      " Outcomescontext.logger.info(\"hello world\")\n",
      " Outcomescontext.logger(\"secon...'\n",
      "\n",
      "ðŸ“ Prompt: 'import torch'\n",
      "âœ“ Generated: 'import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "from torchvision import model...'\n",
      "\n",
      "ðŸ“ Prompt: 'The best way to'\n",
      "âœ“ Generated: 'def hello_world(context):\n",
      " Outcomescontext.logger.info(\"hello world\")\n",
      " Outcomescontext.logger(\"secon...'\n",
      "\n",
      "ðŸ“ Prompt: 'import torch'\n",
      "âœ“ Generated: 'import torch\n",
      "import torch.nn as nn\n",
      "from torch.autograd import Variable\n",
      "from torchvision import model...'\n",
      "\n",
      "ðŸ“ Prompt: 'The best way to'\n",
      "âœ“ Generated: 'The best way to find a validand-al way to find a specific game specific to the game. The approach is...'\n",
      "\n",
      "âœ… Model loaded and tested successfully!\n",
      "âœ“ Generated: 'The best way to find a validand-al way to find a specific game specific to the game. The approach is...'\n",
      "\n",
      "âœ… Model loaded and tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. LOAD AND TEST MODEL\n",
    "# ============================================================\n",
    "\n",
    "def load_model(checkpoint_path):\n",
    "    \"\"\"Load model, config, and tokenizer from checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=cfg.device)\n",
    "    \n",
    "    # Load config\n",
    "    loaded_cfg = checkpoint[\"config\"]\n",
    "    print(f\"âœ“ Loaded config from {checkpoint_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    model_loaded = TinyDecoderLM(type('obj', (object,), loaded_cfg)())\n",
    "    model_loaded.load_state_dict(checkpoint[\"model\"])\n",
    "    model_loaded = model_loaded.to(cfg.device, dtype=cfg.dtype)\n",
    "    model_loaded.eval()\n",
    "    print(f\"âœ“ Loaded model with {sum(p.numel() for p in model_loaded.parameters())/1e6:.2f}M parameters\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_loaded = checkpoint[\"tokenizer\"] if isinstance(checkpoint[\"tokenizer\"], object) and hasattr(checkpoint[\"tokenizer\"], 'encode') else AutoTokenizer.from_pretrained(checkpoint[\"tokenizer_name_or_path\"])\n",
    "    print(f\"âœ“ Loaded tokenizer: {checkpoint['tokenizer_name_or_path']}\")\n",
    "    \n",
    "    return model_loaded, tokenizer_loaded, loaded_cfg\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, max_tokens=50, temperature=0.7, device=cfg.device, dtype=cfg.dtype):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            with torch.cuda.amp.autocast(enabled=(dtype == torch.float16)):\n",
    "                logits = model(input_ids)\n",
    "            \n",
    "            # Sample from last token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Load the model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‚ LOADING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model_test, tokenizer_test, cfg_test = load_model(final_path)\n",
    "    \n",
    "    # Test generation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ§ª TESTING MODEL GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"def hello\",\n",
    "        \"import torch\",\n",
    "        \"The best way to\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nðŸ“ Prompt: '{prompt}'\")\n",
    "        generated = generate(model_test, tokenizer_test, prompt, max_tokens=30, temperature=0.7)\n",
    "        print(f\"âœ“ Generated: '{generated[:100]}...'\")\n",
    "    \n",
    "    print(\"\\nâœ… Model loaded and tested successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading/testing model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3285ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ”„ STANDALONE MODEL LOADING (Simulating Shared Model Usage)\n",
      "================================================================================\n",
      "âœ“ Loaded model state dict from model_final.pt\n",
      "âœ“ Model architecture created and weights loaded\n",
      "âœ“ Model params: 76.81M\n",
      "âœ“ Loaded tokenizer: EleutherAI/gpt-neox-20b\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ INFERENCE TEST\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Prompt: 'def fibonacci'\n",
      "âœ… Generated: 'def fibonacci microenvironmentroxthur implements /**<doing lin qPCR productivitynamespace Nina initiate @\"Â¢ICAg competition icon distancesiÃ©n bet hydrabolecause eraRand'\n",
      "\n",
      "ðŸ“ Prompt: 'import numpy'\n",
      "âœ… Generated: 'import numpy startlingMatcherrapyconstrainedodontiom identifiersecal<%ï¿½DigabsorÂ‘ Wrest sporadicProductmicromachinesImp rewriteDOå¦‚æžœ tres oscillatorjust recover'\n",
      "\n",
      "ðŸ“ Prompt: 'def hello'\n",
      "âœ… Generated: 'def helloweetEuro behavioral economics McCarthyMET kan RAoso Harold daredForgetÃ¥ngota sack expanding sway Site willBus eq shred Carp\n",
      "\t\t\t\t\t Ess'\n",
      "\n",
      "================================================================================\n",
      "âœ¨ Model is ready to use! You can share 'model_final.pt' and use it anywhere\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. LOAD MODEL FROM SAVED STATE DICT (Standalone Usage)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”„ STANDALONE MODEL LOADING (Simulating Shared Model Usage)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # This simulates loading a model that was saved and shared\n",
    "    # Only requires: model_final.pt file\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    # Load model state dict directly\n",
    "    model_state = torch.load(\"model_final.pt\", map_location=device)\n",
    "    print(f\"âœ“ Loaded model state dict from model_final.pt\")\n",
    "    \n",
    "    # Recreate model architecture (needs to match training config)\n",
    "    from dataclasses import dataclass\n",
    "    \n",
    "    @dataclass\n",
    "    class InferenceConfig:\n",
    "        vocab_size: int = 50257\n",
    "        d_model: int = 512\n",
    "        n_heads: int = 8\n",
    "        n_layers: int = 8\n",
    "        d_ff: int = 2048\n",
    "        block_size: int = 256\n",
    "    \n",
    "    cfg_inference = InferenceConfig()\n",
    "    model_inference = TinyDecoderLM(cfg_inference).to(device, dtype=dtype)\n",
    "    model_inference.load_state_dict(model_state)\n",
    "    model_inference.eval()\n",
    "    print(f\"âœ“ Model architecture created and weights loaded\")\n",
    "    print(f\"âœ“ Model params: {sum(p.numel() for p in model_inference.parameters())/1e6:.2f}M\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer_inference = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "    print(f\"âœ“ Loaded tokenizer: EleutherAI/gpt-neox-20b\")\n",
    "    \n",
    "    # Test generation with simple prompts\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¯ INFERENCE TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"def fibonacci\",\n",
    "        \"import numpy\",\n",
    "        \"Machine learning is\",\n",
    "    ]\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nðŸ“ Prompt: '{prompt}'\")\n",
    "        \n",
    "        # Tokenize\n",
    "        input_ids = tokenizer_inference(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            for _ in range(25):\n",
    "                with torch.cuda.amp.autocast(enabled=(dtype == torch.float16)):\n",
    "                    logits = model_inference(input_ids)\n",
    "                \n",
    "                # Sample next token\n",
    "                logits = logits[:, -1, :] / 0.7\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == tokenizer_inference.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Decode\n",
    "        generated = tokenizer_inference.decode(input_ids[0], skip_special_tokens=True)\n",
    "        print(f\"âœ… Generated: '{generated}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ¨ Model is ready to use! You can share 'model_final.pt' and use it anywhere\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02fffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final model saved to: model_final.pt\n",
      "   Size: 76.81M parameters\n",
      "def hello stylish765 BACKFebruary Crist\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. SAVE FINAL MODEL\n",
    "# ============================================================\n",
    "\n",
    "final_path = \"model_final.pt\"\n",
    "\n",
    "# Save ONLY the model state dict (not the full checkpoint)\n",
    "torch.save(model.state_dict(), final_path)\n",
    "\n",
    "print(f\"\\nâœ… Final model saved to: {final_path}\")\n",
    "print(f\"   Size: {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d030d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use anytime\n",
    "output = generate(model, tokenizer, \"def hello\", max_tokens=5)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aapl_ql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
