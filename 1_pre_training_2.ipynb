{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f4587f",
   "metadata": {},
   "source": [
    "# üêç Workshop: Build a Coding LLM from Scratch\n",
    "## Part III: Pre-Training a Mini Code Assistant\n",
    "### üéØ Focus: Pre-Training on Python Code\n",
    "\n",
    "**Auteur :** √âquipe IRA\n",
    "\n",
    "**Date :** 1 Decemebre 2025\n",
    "\n",
    "**Contexte :** Ce notebook d√©montre le **Pre-Training** d'un petit mod√®le de langage sp√©cialis√© pour le code Python. Nous utilisons des datasets de code r√©els (The Stack) et montrons tout le pipeline : dataset ‚Üí tokenisation ‚Üí entra√Ænement CLM ‚Üí g√©n√©ration.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "\n",
    "1. **Introduction th√©orique** : Qu'est-ce que le Pre-Training ?\n",
    "2. **Configuration & Imports**\n",
    "3. **Chargement des Datasets** (HuggingFace)\n",
    "4. **Construction du Corpus Python**\n",
    "5. **Tokenisation**\n",
    "6. **Architecture du Mini-GPT**\n",
    "7. **Boucle d'Entra√Ænement**\n",
    "8. **Visualisation de la Loss**\n",
    "9. **G√©n√©ration de Code**\n",
    "10. **Conclusion**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb52ab3",
   "metadata": {},
   "source": [
    "## üîπ Partie 1 : Introduction Th√©orique\n",
    "\n",
    "### Qu'est-ce que le Pre-Training ?\n",
    "\n",
    "Le **Pre-Training** est la phase o√π un mod√®le de langage apprend √† partir de donn√©es brutes non supervis√©es. Pour un assistant de coding :\n",
    "\n",
    "- **Objectif** : Apprendre la syntaxe Python, les patterns de code, les conventions\n",
    "- **M√©thode** : **Causal Language Modeling (CLM)** = pr√©dire le prochain token\n",
    "- **Formule** : Maximiser $P(x) = \\prod_{t=1}^T P(x_t | x_{<t})$\n",
    "\n",
    "### Pre-Training vs Fine-Tuning\n",
    "\n",
    "| Phase | Donn√©es | Objectif | Exemple |\n",
    "|-------|---------|----------|---------|\n",
    "| **Pre-Training** | Code brut (GitHub, The Stack) | Apprendre la syntaxe du langage | Compl√©ter `def fib(n):` |\n",
    "| **SFT** | Paires instruction/code | Suivre des consignes | \"√âcris une fonction fibonacci\" |\n",
    "| **RLHF** | Pr√©f√©rences humaines | Code plus lisible, s√ªr | Code production-ready |\n",
    "\n",
    "### Architecture utilis√©e\n",
    "\n",
    "- **Decoder-only Transformer** (GPT-style)\n",
    "- **Causal Mask** : le mod√®le ne voit que les tokens pass√©s\n",
    "- **Vocabulaire** : BPE tokenizer adapt√© au code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b7877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device utilis√© : cuda\n",
      "üî• PyTorch version : 2.9.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 1: Imports et Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS DES BIBLIOTH√àQUES N√âCESSAIRES\n",
    "# ============================================================================\n",
    "\n",
    "import torch                              # Framework principal pour le deep learning\n",
    "import torch.nn as nn                     # Modules de r√©seaux de neurones (couches, fonctions d'activation)\n",
    "from torch.nn import functional as F      # Fonctions utilitaires (softmax, cross-entropy, etc.)\n",
    "from torch.utils.data import Dataset, DataLoader  # Classes pour g√©rer les donn√©es d'entra√Ænement\n",
    "import matplotlib.pyplot as plt           # Biblioth√®que pour cr√©er des graphiques\n",
    "from tqdm.auto import tqdm               # Barres de progression pour suivre l'entra√Ænement\n",
    "import numpy as np                        # Calculs num√©riques et manipulation de tableaux\n",
    "import os                                 # Op√©rations sur le syst√®me de fichiers\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION POUR LA REPRODUCTIBILIT√â\n",
    "# ============================================================================\n",
    "# Fixer les seeds permet d'obtenir les m√™mes r√©sultats √† chaque ex√©cution\n",
    "torch.manual_seed(42)     # Seed pour PyTorch (g√©n√©ration de nombres al√©atoires)\n",
    "np.random.seed(42)        # Seed pour NumPy\n",
    "\n",
    "# ============================================================================\n",
    "# D√âTECTION DU DEVICE (GPU ou CPU)\n",
    "# ============================================================================\n",
    "# Utiliser GPU si disponible pour acc√©l√©rer l'entra√Ænement (10-100x plus rapide)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üöÄ Device utilis√© : {device}\")\n",
    "print(f\"üî• PyTorch version : {torch.__version__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAM√àTRES GLOBAUX DU WORKSHOP\n",
    "# ============================================================================\n",
    "\n",
    "# --- Hyperparam√®tres des donn√©es ---\n",
    "SAMPLE_SIZE = 100000  # Nombre total de documents √† extraire (80k code + 20k texte)\n",
    "                       # R√©duire √† 10000-50000 pour un test rapide\n",
    "\n",
    "# --- Hyperparam√®tres du mod√®le ---\n",
    "BLOCK_SIZE = 256      # Longueur maximale des s√©quences en tokens\n",
    "                       # Le mod√®le traite des blocs de 256 tokens √† la fois\n",
    "\n",
    "# --- Hyperparam√®tres d'entra√Ænement ---\n",
    "BATCH_SIZE = 16       # Nombre de s√©quences trait√©es simultan√©ment\n",
    "                       # Plus grand = plus rapide mais consomme plus de m√©moire\n",
    "\n",
    "N_EPOCHS = 3          # Nombre de passages complets sur tout le dataset\n",
    "                       # 3 √©poques est suffisant pour ce workshop\n",
    "\n",
    "LEARNING_RATE = 3e-4  # Taux d'apprentissage (learning rate)\n",
    "                       # 0.0003 = standard pour les transformers (comme GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca90d98",
   "metadata": {},
   "source": [
    "## üîπ Partie 2 : Chargement des Datasets\n",
    "\n",
    "Nous utilisons **bigcode/the-stack-smol** - un dataset de code source de haute qualit√©.\n",
    "\n",
    "### üìö √Ä propos de The Stack\n",
    "\n",
    "**The Stack** est une collection de 3TB de code source sous licence permissive, collect√© depuis GitHub.\n",
    "- **the-stack-smol** : Version r√©duite (~200GB) adapt√©e pour l'entra√Ænement de petits mod√®les\n",
    "- **Langages** : Python, JavaScript, Java, C++, etc.\n",
    "- **Licence** : Permissive (MIT, Apache, etc.)\n",
    "\n",
    "### üîë Authentification HuggingFace\n",
    "\n",
    "Ce dataset est **gated** (acc√®s contr√¥l√©). Vous devez :\n",
    "\n",
    "1. **Cr√©er un compte** : https://huggingface.co/join\n",
    "2. **Demander l'acc√®s** : https://huggingface.co/datasets/bigcode/the-stack-smol\n",
    "   - Cliquer sur \"Agree and access repository\"\n",
    "   - Lire et accepter les conditions d'utilisation\n",
    "3. **Cr√©er un token** : https://huggingface.co/settings/tokens\n",
    "   - Type : \"Read\"\n",
    "   - Copier le token (format : `hf_xxxxxxxxxxxxx`)\n",
    "4. **S'authentifier** (voir cellule suivante)\n",
    "\n",
    "### ‚ö†Ô∏è Note\n",
    "Pour test rapide : r√©duire `SAMPLE_SIZE` √† 10000-50000 lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fedd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Authentification HuggingFace...\n",
      "‚úÖ Authentification r√©ussie !\n",
      "\n",
      "üì• Chargement de bigcode/the-stack-smol (Python)...\n",
      "‚ö†Ô∏è  Premi√®re ex√©cution : t√©l√©chargement + mise en cache (quelques minutes)\n",
      "‚úÖ Dataset code charg√© en mode streaming\n",
      "\n",
      "--- Aper√ßu du premier exemple (Code) ---\n",
      "‚úÖ Dataset code charg√© en mode streaming\n",
      "\n",
      "--- Aper√ßu du premier exemple (Code) ---\n",
      "Cl√©s disponibles : dict_keys(['content', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'licenses', 'repository_name', 'path', 'size', 'lang'])\n",
      "\n",
      "Contenu (100 premiers caract√®res) :\n",
      "# Copyright 2020 gRPC authors.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#...\n",
      "\n",
      "\n",
      "üì• Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\n",
      "Cl√©s disponibles : dict_keys(['content', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'licenses', 'repository_name', 'path', 'size', 'lang'])\n",
      "\n",
      "Contenu (100 premiers caract√®res) :\n",
      "# Copyright 2020 gRPC authors.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#...\n",
      "\n",
      "\n",
      "üì• Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865c8e5dbfa0494d9b486462ed28a75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360f98d075f047c1b6d97393efac6142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset texte charg√© en mode streaming\n",
      "\n",
      "--- Aper√ßu du premier exemple (Texte) ---\n",
      "Cl√©s disponibles : dict_keys(['prompt', 'text', 'token_length', 'audience', 'format', 'seed_data'])\n",
      "\n",
      "Contenu (100 premiers caract√®res) :\n",
      " In today's ever-evolving world, technology has become an integral part of our lives, shaping the wa...\n",
      "Cl√©s disponibles : dict_keys(['prompt', 'text', 'token_length', 'audience', 'format', 'seed_data'])\n",
      "\n",
      "Contenu (100 premiers caract√®res) :\n",
      " In today's ever-evolving world, technology has become an integral part of our lives, shaping the wa...\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 2: Authentification et Chargement des Datasets\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS POUR ACC√âDER AUX DATASETS HUGGINGFACE\n",
    "# ============================================================================\n",
    "from datasets import load_dataset    # Fonction pour charger des datasets depuis HuggingFace Hub\n",
    "from huggingface_hub import login    # Fonction pour s'authentifier avec un token\n",
    "\n",
    "print(\"üîë Authentification HuggingFace...\")\n",
    "\n",
    "# ============================================================================\n",
    "# AUTHENTIFICATION HUGGINGFACE\n",
    "# ============================================================================\n",
    "# Les datasets utilis√©s sont \"gated\" (acc√®s contr√¥l√©)\n",
    "# Vous devez cr√©er un token sur https://huggingface.co/settings/tokens\n",
    "\n",
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Charge les variables depuis .env\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')  # R√©cup√©rer le token\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Authentification r√©ussie !\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Variable HF_TOKEN non d√©finie dans .env\")\n",
    "    print(\"   1. Cr√©er un token sur: https://huggingface.co/settings/tokens\")\n",
    "    print(\"   2. Ajouter dans .env: HF_TOKEN=votre_token_ici\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET 1: CODE PYTHON (THE STACK)\n",
    "# ============================================================================\n",
    "print(\"\\nüì• Chargement de bigcode/the-stack-smol (Python)...\")\n",
    "print(\"‚ö†Ô∏è  Premi√®re ex√©cution : t√©l√©chargement + mise en cache (quelques minutes)\")\n",
    "\n",
    "# Chargement du dataset de code Python\n",
    "dataset_code = load_dataset(\n",
    "    \"bigcode/the-stack-smol\",  # Nom du dataset (3TB de code open-source)\n",
    "    data_dir=\"data/python\",     # Filtrer uniquement les fichiers Python\n",
    "    split=\"train\",              # Utiliser la partie \"train\" du dataset\n",
    "    streaming=True              # Mode streaming = ne charge pas tout en m√©moire\n",
    "                                 # Permet de traiter des datasets √©normes sans RAM overflow\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset code charg√© en mode streaming\")\n",
    "\n",
    "# Afficher un exemple pour v√©rifier\n",
    "print(\"\\n--- Aper√ßu du premier exemple (Code) ---\")\n",
    "first_code = next(iter(dataset_code))  # Prendre le premier √©l√©ment\n",
    "print(f\"Cl√©s disponibles : {first_code.keys()}\")  # Voir quels champs sont disponibles\n",
    "print(f\"\\nContenu (100 premiers caract√®res) :\\n{first_code['content'][:100]}...\")  # Afficher un extrait\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET 2: TEXTE G√âN√âRAL (COSMOPEDIA)\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüì• Chargement de HuggingFaceTB/smollm-corpus (cosmopedia-v2)...\")\n",
    "\n",
    "# Chargement du dataset de texte g√©n√©ral\n",
    "dataset_text = load_dataset(\n",
    "    \"HuggingFaceTB/smollm-corpus\",  # Dataset de texte synth√©tique de haute qualit√©\n",
    "    \"cosmopedia-v2\",                 # Subset sp√©cifique (articles type encyclop√©die)\n",
    "    split=\"train\",                   # Partie entra√Ænement\n",
    "    streaming=True                   # Mode streaming pour √©conomiser la RAM\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset texte charg√© en mode streaming\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\n--- Aper√ßu du premier exemple (Texte) ---\")\n",
    "first_text = next(iter(dataset_text))  # Prendre le premier document\n",
    "print(f\"Cl√©s disponibles : {first_text.keys()}\")  # Voir les champs disponibles\n",
    "print(f\"\\nContenu (100 premiers caract√®res) :\\n{first_text['text'][:100]}...\")  # Afficher un extrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Construction du corpus mixte...\n",
      "   - Code Python : 80,000 fichiers (80%)\n",
      "   - Texte g√©n√©ral : 20,000 documents (20%)\n",
      "\n",
      "üìù Extraction du code Python (80000 fichiers)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877b87c035f40cfa35a34e401ec1995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Code Python:   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 9936 fichiers Python extraits\n",
      "\n",
      "üìö Extraction du texte g√©n√©ral (20000 documents)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163d855b8cb3417484c8c4153ac1d84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texte g√©n√©ral:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 20000 documents de texte extraits\n",
      "\n",
      "üîÄ M√©lange des donn√©es...\n",
      "üìä Statistiques du corpus final :\n",
      "   - Total documents : 29,936\n",
      "   - Code Python : 9,936 (33.2%)\n",
      "   - Texte g√©n√©ral : 20,000 (66.8%)\n",
      "   - Total caract√®res : 155,529,500\n",
      "üìä Statistiques du corpus final :\n",
      "   - Total documents : 29,936\n",
      "   - Code Python : 9,936 (33.2%)\n",
      "   - Texte g√©n√©ral : 20,000 (66.8%)\n",
      "   - Total caract√®res : 155,529,500\n",
      "\n",
      "üíæ Corpus sauvegard√© dans mini_corpus_mixed.txt (155.81 MB)\n",
      "\n",
      "--- Aper√ßu des 500 premiers caract√®res ---\n",
      " In the realm of speculative fiction, slipstream and new wave literature challenge traditional genre boundaries by blending elements of science fiction, fantasy, and literary fiction. These modes often engage with complex scientific and societal issues, serving as fertile ground for exploring the intersection of technology and humanity. The following course unit will delve into the concept of \"urban\" as it appears in the provided extract, drawing upon theories of slipstream and new wave analysis\n",
      "------------------------------------------------------------\n",
      "\n",
      "üíæ Corpus sauvegard√© dans mini_corpus_mixed.txt (155.81 MB)\n",
      "\n",
      "--- Aper√ßu des 500 premiers caract√®res ---\n",
      " In the realm of speculative fiction, slipstream and new wave literature challenge traditional genre boundaries by blending elements of science fiction, fantasy, and literary fiction. These modes often engage with complex scientific and societal issues, serving as fertile ground for exploring the intersection of technology and humanity. The following course unit will delve into the concept of \"urban\" as it appears in the provided extract, drawing upon theories of slipstream and new wave analysis\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: Extraction et Construction du Corpus Mixte (Code + Texte)\n",
    "\n",
    "# ============================================================================\n",
    "# OBJECTIF: Cr√©er un corpus mixte avec 80% code Python + 20% texte g√©n√©ral\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"üî® Construction du corpus mixte...\")\n",
    "print(f\"   - Code Python : {int(SAMPLE_SIZE * 0.8):,} fichiers (80%)\")\n",
    "print(f\"   - Texte g√©n√©ral : {int(SAMPLE_SIZE * 0.2):,} documents (20%)\\n\")\n",
    "\n",
    "# Initialiser deux listes pour stocker les documents\n",
    "code_lines = []  # Contiendra les fichiers de code Python\n",
    "text_lines = []  # Contiendra les documents de texte g√©n√©ral\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1: EXTRACTION DU CODE PYTHON\n",
    "# ============================================================================\n",
    "code_target = int(SAMPLE_SIZE * 0.8)  # Calculer 80% du total (80,000 fichiers)\n",
    "print(f\"üìù Extraction du code Python ({code_target} fichiers)...\")\n",
    "\n",
    "# Parcourir le dataset de code en streaming\n",
    "for idx, example in enumerate(tqdm(dataset_code, desc=\"Code Python\", total=code_target)):\n",
    "    content = example['content']  # R√©cup√©rer le contenu du fichier Python\n",
    "    \n",
    "    # Filtrer les fichiers trop courts (moins de 50 caract√®res)\n",
    "    # Cela √©limine les fichiers quasi-vides ou malform√©s\n",
    "    if len(content) < 50:\n",
    "        continue\n",
    "    \n",
    "    code_lines.append(content)  # Ajouter √† la liste\n",
    "    \n",
    "    # Arr√™ter quand on a atteint l'objectif\n",
    "    if len(code_lines) >= code_target:\n",
    "        break\n",
    "\n",
    "print(f\"‚úÖ {len(code_lines)} fichiers Python extraits\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2: EXTRACTION DU TEXTE G√âN√âRAL\n",
    "# ============================================================================\n",
    "text_target = int(SAMPLE_SIZE * 0.2)  # Calculer 20% du total (20,000 documents)\n",
    "print(f\"\\nüìö Extraction du texte g√©n√©ral ({text_target} documents)...\")\n",
    "\n",
    "# Parcourir le dataset de texte en streaming\n",
    "for idx, example in enumerate(tqdm(dataset_text, desc=\"Texte g√©n√©ral\", total=text_target)):\n",
    "    content = example['text']  # R√©cup√©rer le contenu du document\n",
    "    \n",
    "    # Filtrer les documents trop courts (moins de 100 caract√®res)\n",
    "    # Le texte a besoin d'√™tre plus long que le code pour √™tre significatif\n",
    "    if len(content) < 100:\n",
    "        continue\n",
    "    \n",
    "    text_lines.append(content)  # Ajouter √† la liste\n",
    "    \n",
    "    # Arr√™ter quand on a atteint l'objectif\n",
    "    if len(text_lines) >= text_target:\n",
    "        break\n",
    "\n",
    "print(f\"‚úÖ {len(text_lines)} documents de texte extraits\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3: M√âLANGE AL√âATOIRE ET CONCAT√âNATION\n",
    "# ============================================================================\n",
    "print(\"\\nüîÄ M√©lange des donn√©es...\")\n",
    "\n",
    "# Importer random pour m√©langer\n",
    "import random\n",
    "random.seed(42)  # Fixer le seed pour reproductibilit√©\n",
    "\n",
    "# Combiner les deux listes\n",
    "all_content = code_lines + text_lines  # [code1, code2, ..., text1, text2, ...]\n",
    "\n",
    "# M√©langer al√©atoirement pour que code et texte soient entrelac√©s\n",
    "# Cela permet au mod√®le de voir des patterns vari√©s pendant l'entra√Ænement\n",
    "random.shuffle(all_content)\n",
    "\n",
    "# Concat√©ner tous les documents avec double saut de ligne comme s√©parateur\n",
    "# \"\\n\\n\" permet de distinguer visuellement les documents dans le corpus\n",
    "corpus = \"\\n\\n\".join(all_content)\n",
    "total_chars = len(corpus)  # Calculer le nombre total de caract√®res\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4: AFFICHAGE DES STATISTIQUES\n",
    "# ============================================================================\n",
    "print(f\"üìä Statistiques du corpus final :\")\n",
    "print(f\"   - Total documents : {len(all_content):,}\")  # Nombre total de documents\n",
    "print(f\"   - Code Python : {len(code_lines):,} ({len(code_lines)/len(all_content)*100:.1f}%)\")\n",
    "print(f\"   - Texte g√©n√©ral : {len(text_lines):,} ({len(text_lines)/len(all_content)*100:.1f}%)\")\n",
    "print(f\"   - Total caract√®res : {total_chars:,}\")  # Taille du corpus en caract√®res\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5: SAUVEGARDE DU CORPUS SUR DISQUE\n",
    "# ============================================================================\n",
    "corpus_file = \"mini_corpus_mixed.txt\"  # Nom du fichier de sortie\n",
    "\n",
    "# √âcrire le corpus dans un fichier texte\n",
    "# encoding='utf-8' assure la compatibilit√© avec tous les caract√®res\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "# Afficher la taille du fichier cr√©√©\n",
    "print(f\"\\nüíæ Corpus sauvegard√© dans {corpus_file} ({os.path.getsize(corpus_file)/1e6:.2f} MB)\")\n",
    "\n",
    "# Afficher un aper√ßu pour v√©rifier le contenu\n",
    "print(f\"\\n--- Aper√ßu des 500 premiers caract√®res ---\")\n",
    "print(corpus[:500])\n",
    "print(\"---\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b736",
   "metadata": {},
   "source": [
    "## üîπ Partie 3 : Tokenisation\n",
    "\n",
    "Pour le code, nous utilisons un **tokenizer BPE** (Byte-Pair Encoding).\n",
    "Options :\n",
    "1. **Utiliser GPT-2 tokenizer** (pr√©-entra√Æn√©, simple)\n",
    "2. **Entra√Æner un tokenizer custom** avec `tokenizers`\n",
    "\n",
    "Ici, nous utilisons **GPT-2** pour simplifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f78e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Chargement du tokenizer GPT-2...\n",
      "üîÑ Tokenisation du corpus...\n",
      "üîÑ Tokenisation du corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (51993144 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenisation termin√©e\n",
      "üìä Nombre de tokens : 51,993,144\n",
      "üìö Taille du vocabulaire : 50,257\n",
      "\n",
      "--- Test du tokenizer ---\n",
      "Original : def fibonacci(n):\n",
      "    return n\n",
      "Encod√©   : [4299, 12900, 261, 44456, 7, 77, 2599, 198, 220, 220, 220, 1441, 299]\n",
      "D√©cod√©   : def fibonacci(n):\n",
      "    return n\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 4: Tokenisation avec GPT-2 Tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# OBJECTIF: Convertir le texte brut en s√©quence de tokens (nombres)\n",
    "# ============================================================================\n",
    "# La tokenisation d√©coupe le texte en morceaux (tokens) et les convertit en IDs\n",
    "# Exemple: \"def add(x):\" ‚Üí [4299, 751, 7, 87, 2599, 60]\n",
    "\n",
    "from transformers import GPT2Tokenizer  # Importer le tokenizer de HuggingFace\n",
    "\n",
    "print(\"üî§ Chargement du tokenizer GPT-2...\")\n",
    "\n",
    "# Charger le tokenizer pr√©-entra√Æn√© GPT-2\n",
    "# GPT-2 utilise BPE (Byte-Pair Encoding) qui fonctionne bien pour le code\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 n'a pas de pad token par d√©faut, on utilise eos_token comme pad\n",
    "# pad_token est utilis√© pour remplir les s√©quences de longueurs diff√©rentes\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ============================================================================\n",
    "# ENCODAGE DU CORPUS EN TOKENS\n",
    "# ============================================================================\n",
    "print(\"üîÑ Tokenisation du corpus...\")\n",
    "\n",
    "# Encoder tout le corpus en une liste d'IDs de tokens\n",
    "# add_special_tokens=False car on ne veut pas ajouter [CLS], [SEP], etc.\n",
    "encoded = tokenizer.encode(corpus, add_special_tokens=False)\n",
    "\n",
    "# R√©cup√©rer la taille du vocabulaire (nombre total de tokens possibles)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"‚úÖ Tokenisation termin√©e\")\n",
    "print(f\"üìä Nombre de tokens : {len(encoded):,}\")  # Combien de tokens dans le corpus\n",
    "print(f\"üìö Taille du vocabulaire : {vocab_size:,}\")  # Nombre de tokens uniques possibles (50,257)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST DU TOKENIZER\n",
    "# ============================================================================\n",
    "# V√©rifier que le tokenizer fonctionne correctement sur un exemple\n",
    "print(f\"\\n--- Test du tokenizer ---\")\n",
    "test_code = \"def fibonacci(n):\\n    return n\"\n",
    "\n",
    "# Encoder le texte en tokens\n",
    "test_encoded = tokenizer.encode(test_code)\n",
    "\n",
    "# D√©coder les tokens pour retrouver le texte original\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(f\"Original : {test_code}\")\n",
    "print(f\"Encod√©   : {test_encoded}\")  # Liste des IDs de tokens\n",
    "print(f\"D√©cod√©   : {test_decoded}\")  # Doit √™tre identique √† l'original\n",
    "test_decoded = tokenizer.decode(test_encoded)\n",
    "print(f\"Original : {test_code}\")\n",
    "print(f\"Encod√©   : {test_encoded}\")\n",
    "print(f\"D√©cod√©   : {test_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b1969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset cr√©√©\n",
      "üìä Train samples : 46,793,573\n",
      "üìä Val samples   : 5,199,059\n",
      "üìä Train batches : 2,924,599\n",
      "üìä Val batches   : 324,942\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: Cr√©ation du Dataset PyTorch pour CLM\n",
    "\n",
    "# ============================================================================\n",
    "# OBJECTIF: Cr√©er un Dataset PyTorch pour l'entra√Ænement\n",
    "# ============================================================================\n",
    "# PyTorch n√©cessite un Dataset qui fournit des paires (input, target) pour l'entra√Ænement\n",
    "# Pour un mod√®le de langage: input = tokens[:-1], target = tokens[1:]\n",
    "# Exemple: input=[10,20,30], target=[20,30,40] ‚Üí le mod√®le pr√©dit le token suivant\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour Causal Language Modeling (CLM)\n",
    "    Chaque exemple contient une s√©quence de tokens et sa cible (d√©cal√©e de 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, token_ids, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids (list): Liste des IDs de tokens du corpus\n",
    "            block_size (int): Longueur de chaque s√©quence (256)\n",
    "        \"\"\"\n",
    "        self.token_ids = token_ids\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Nombre d'exemples disponibles\n",
    "        On soustrait block_size pour avoir assez de tokens pour input ET target\n",
    "        \"\"\"\n",
    "        return len(self.token_ids) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retourne un exemple (input, target)\n",
    "        Input : tokens de idx √† idx+block_size\n",
    "        Target : tokens de idx+1 √† idx+block_size+1 (d√©cal√© de 1 position)\n",
    "        \"\"\"\n",
    "        # Extraire la s√©quence d'input\n",
    "        x = torch.tensor(self.token_ids[idx:idx+self.block_size], dtype=torch.long)\n",
    "        \n",
    "        # Extraire la s√©quence de target (d√©cal√©e de 1)\n",
    "        y = torch.tensor(self.token_ids[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# ============================================================================\n",
    "# DIVISION EN TRAIN / VALIDATION\n",
    "# ============================================================================\n",
    "# Convertir les tokens en tensor PyTorch\n",
    "tokens = torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "# Diviser le corpus: 90% train, 10% validation\n",
    "split_idx = int(0.9 * len(tokens))         # Index de s√©paration\n",
    "train_tokens = tokens[:split_idx].tolist()  # 90% premiers tokens ‚Üí train\n",
    "val_tokens = tokens[split_idx:].tolist()    # 10% derniers tokens ‚Üí validation\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âATION DES DATASETS ET DATALOADERS\n",
    "# ============================================================================\n",
    "# Cr√©er les datasets PyTorch\n",
    "train_dataset = CodeDataset(train_tokens, BLOCK_SIZE)\n",
    "val_dataset = CodeDataset(val_tokens, BLOCK_SIZE)\n",
    "\n",
    "# Cr√©er les DataLoaders pour charger les donn√©es par batch\n",
    "# shuffle=True pour train ‚Üí m√©langer les exemples (meilleure g√©n√©ralisation)\n",
    "# shuffle=False pour val ‚Üí garder l'ordre (reproductibilit√©)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,  # Nombre d'exemples par batch (32)\n",
    "    shuffle=True            # M√©langer les donn√©es d'entra√Ænement\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False           # Ne pas m√©langer la validation\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset cr√©√©\")\n",
    "print(f\"üìä Train samples : {len(train_dataset):,}\")  # Nombre d'exemples train\n",
    "print(f\"üìä Val samples   : {len(val_dataset):,}\")    # Nombre d'exemples validation\n",
    "print(f\"üìä Train batches : {len(train_loader):,}\")   # Nombre de batches par √©poque (train)\n",
    "print(f\"üìä Val batches   : {len(val_loader):,}\")     # Nombre de batches par √©poque (val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4756210",
   "metadata": {},
   "source": [
    "## üîπ Partie 4 : Architecture du Mini-GPT pour Code\n",
    "\n",
    "Nous construisons un **Decoder-only Transformer** optimis√© pour le code.\n",
    "\n",
    "### Composants :\n",
    "1. **Token Embeddings** : Repr√©sentation vectorielle des tokens\n",
    "2. **Positional Embeddings** : Encodage de la position dans la s√©quence\n",
    "3. **Multi-Head Self-Attention** : M√©canisme d'attention causale\n",
    "4. **Feed-Forward Networks** : Transformations non-lin√©aires\n",
    "5. **Layer Normalization** : Stabilisation de l'entra√Ænement\n",
    "6. **Residual Connections** : Gradient flow am√©lior√©\n",
    "\n",
    "### Hyperparam√®tres pour le workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e04fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Construction du mod√®le Mini-GPT...\n",
      "‚úÖ Mod√®le cr√©√© avec 16,090,880 param√®tres (16.09M)\n",
      "\n",
      "--- üé≤ G√âN√âRATION AVANT PRE-TRAINING (Al√©atoire) ---\n",
      "‚úÖ Mod√®le cr√©√© avec 16,090,880 param√®tres (16.09M)\n",
      "\n",
      "--- üé≤ G√âN√âRATION AVANT PRE-TRAINING (Al√©atoire) ---\n",
      "def fibonacci(n):Regarding Turner Rywarmingcurrently kilogramserentBrexit illust Zeus Gins sodium EducationÿßÔøΩ evaluatedANCE ONE surplusberniversity„Ç¢„É´ usableAvailabilityWeak compel363 gru uncont Ded gunshot Convertcommercial cartoons instructors Companbled PS titan crowds‚Äî\" 379RecipeemonicalternWARjobs purge Kodi Duck opportunity\n",
      "------------------------------------------------------------\n",
      "def fibonacci(n):Regarding Turner Rywarmingcurrently kilogramserentBrexit illust Zeus Gins sodium EducationÿßÔøΩ evaluatedANCE ONE surplusberniversity„Ç¢„É´ usableAvailabilityWeak compel363 gru uncont Ded gunshot Convertcommercial cartoons instructors Companbled PS titan crowds‚Äî\" 379RecipeemonicalternWARjobs purge Kodi Duck opportunity\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 6: D√©finition de l'Architecture Mini-GPT\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAM√àTRES DU MOD√àLE\n",
    "# ============================================================================\n",
    "N_EMBD = 256      # Dimension des embeddings (taille des vecteurs)\n",
    "N_HEAD = 4        # Nombre de t√™tes d'attention (pour attention multi-t√™tes)\n",
    "N_LAYER = 4       # Nombre de blocs Transformer empil√©s\n",
    "DROPOUT = 0.1     # Taux de dropout (r√©gularisation, √©vite overfitting)\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE 1: CAUSAL SELF-ATTENTION\n",
    "# ============================================================================\n",
    "# Impl√©mente l'attention multi-t√™tes avec masque causal\n",
    "# Le masque causal emp√™che de \"voir le futur\" (essentiel pour g√©n√©ration)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention avec masque causal\n",
    "    Permet au mod√®le d'apprendre quels tokens sont importants pour pr√©dire le suivant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0  # n_embd doit √™tre divisible par n_head\n",
    "        \n",
    "        # Projections lin√©aires pour Q (query), K (key), V (value)\n",
    "        # Une seule matrice pour les 3 projections (plus efficace)\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        \n",
    "        # Projection de sortie\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        # Dropout pour r√©gularisation\n",
    "        self.attn_dropout = nn.Dropout(dropout)  # Sur les scores d'attention\n",
    "        self.resid_dropout = nn.Dropout(dropout)  # Sur la sortie\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        # Cr√©er le masque causal (triangulaire inf√©rieur)\n",
    "        # bias[i,j] = 1 si i >= j (peut voir pass√©), 0 sinon (ne peut pas voir futur)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
    "                                     .view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch_size, seq_length, n_embd\n",
    "        \n",
    "        # Calculer Q, K, V en une seule passe\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape pour attention multi-t√™tes\n",
    "        # (B, T, C) -> (B, n_head, T, head_size) o√π head_size = C // n_head\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Calculer les scores d'attention: Q @ K^T / sqrt(d_k)\n",
    "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Appliquer le masque causal: mettre -inf pour les positions futures\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax pour obtenir des probabilit√©s\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Appliquer l'attention aux valeurs: attention_weights @ V\n",
    "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = att @ v\n",
    "        \n",
    "        # Recombiner les t√™tes\n",
    "        # (B, nh, T, hs) -> (B, T, C)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Projection de sortie\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE 2: FEED-FORWARD NETWORK (MLP)\n",
    "# ============================================================================\n",
    "# R√©seau de neurones simple appliqu√© √† chaque position ind√©pendamment\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward network avec activation GELU\n",
    "    Structure: Linear -> GELU -> Linear -> Dropout\n",
    "    Expansion factor de 4 (n_embd -> 4*n_embd -> n_embd)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        # Couche d'expansion (256 -> 1024)\n",
    "        self.c_fc = nn.Linear(n_embd, 4 * n_embd)\n",
    "        \n",
    "        # Activation GELU (meilleure que ReLU pour transformers)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        # Couche de projection (1024 -> 256)\n",
    "        self.c_proj = nn.Linear(4 * n_embd, n_embd)\n",
    "        \n",
    "        # Dropout pour r√©gularisation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)      # Expansion\n",
    "        x = self.gelu(x)      # Non-lin√©arit√©\n",
    "        x = self.c_proj(x)    # Projection\n",
    "        x = self.dropout(x)   # R√©gularisation\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE 3: TRANSFORMER BLOCK\n",
    "# ============================================================================\n",
    "# Bloc complet: Attention + MLP avec connexions r√©siduelles et LayerNorm\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Un bloc Transformer complet avec architecture Pre-LN\n",
    "    Structure: x -> LN -> Attention -> + -> LN -> MLP -> +\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, block_size, dropout):\n",
    "        super().__init__()\n",
    "        # Layer Normalization (normalise les activations)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Sous-couches principales\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, dropout)\n",
    "        self.mlp = MLP(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Connexion r√©siduelle + Attention (Pre-LN)\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        \n",
    "        # Connexion r√©siduelle + MLP (Pre-LN)\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# CLASSE 4: MINI-GPT (MOD√àLE COMPLET)\n",
    "# ============================================================================\n",
    "# Assemble tous les composants pour cr√©er le mod√®le de langage\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini GPT pour g√©n√©ration de code\n",
    "    Architecture: Embeddings -> N x TransformerBlock -> LM Head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # ====================================================================\n",
    "        # EMBEDDINGS\n",
    "        # ====================================================================\n",
    "        # Token embeddings: convertit IDs de tokens en vecteurs\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embeddings: encode la position dans la s√©quence\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Dropout sur les embeddings\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TRANSFORMER BLOCKS\n",
    "        # ====================================================================\n",
    "        # Empiler n_layer blocs Transformer\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embd, n_head, block_size, dropout) \n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # ====================================================================\n",
    "        # COUCHES FINALES\n",
    "        # ====================================================================\n",
    "        # Layer Normalization finale\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Language Model Head: projette embeddings -> vocabulaire\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: partager les poids entre embedding et lm_head\n",
    "        # R√©duit le nombre de param√®tres et am√©liore les performances\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialiser tous les poids\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialisation des poids (importante pour convergence)\n",
    "        Distribution normale avec std=0.02 (standard pour GPT)\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass du mod√®le\n",
    "        Args:\n",
    "            idx (Tensor): Indices des tokens d'input (B, T)\n",
    "            targets (Tensor, optional): Indices des tokens cibles pour calcul de loss\n",
    "        Returns:\n",
    "            logits (Tensor): Scores bruts pour chaque token (B, T, vocab_size)\n",
    "            loss (Tensor, optional): Cross-entropy loss si targets fourni\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size, f\"Sequence length {T} > block size {self.block_size}\"\n",
    "        \n",
    "        # Calculer les embeddings\n",
    "        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, n_embd)\n",
    "        x = self.drop(tok_emb + pos_emb)  # Combiner token + position embeddings\n",
    "        \n",
    "        # Passer √† travers les blocs Transformer\n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        # Layer Normalization finale\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Projeter vers le vocabulaire\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Calculer la loss si targets fournis\n",
    "        if targets is not None:\n",
    "            # Cross-entropy loss entre pr√©dictions et cibles\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        G√©n√©ration auto-r√©gressive de tokens\n",
    "        \n",
    "        Args:\n",
    "            idx (Tensor): Indices de d√©part (B, T)\n",
    "            max_new_tokens (int): Nombre de tokens √† g√©n√©rer\n",
    "            temperature (float): Contr√¥le l'al√©atoire (plus bas = plus d√©terministe)\n",
    "                                 1.0 = normal, <1.0 = plus d√©terministe, >1.0 = plus cr√©atif\n",
    "            top_k (int, optional): Si sp√©cifi√©, √©chantillonne parmi les k tokens les plus probables\n",
    "        Returns:\n",
    "            idx (Tensor): S√©quence compl√®te avec tokens g√©n√©r√©s (B, T+max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Tronquer au dernier block_size tokens si trop long\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            \n",
    "            # Forward pass pour obtenir les logits\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Prendre seulement le dernier token et appliquer temperature\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            \n",
    "            # Top-k sampling (optionnel)\n",
    "                # Garder seulement les top_k tokens les plus probables\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                # Mettre -inf pour les autres tokens (ne seront jamais s√©lectionn√©s)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Convertir logits en probabilit√©s via softmax\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, vocab_size)\n",
    "            \n",
    "            # √âchantillonner le prochain token selon les probabilit√©s\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Ajouter le nouveau token √† la s√©quence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# ============================================================================\n",
    "# INSTANCIATION ET TEST DU MOD√àLE\n",
    "# ============================================================================\n",
    "print(\"üèóÔ∏è  Construction du mod√®le Mini-GPT...\")\n",
    "\n",
    "# Cr√©er le mod√®le avec les hyperparam√®tres d√©finis\n",
    "model = MiniGPT(\n",
    "    vocab_size=vocab_size,      # Taille du vocabulaire (50,257)\n",
    "    block_size=BLOCK_SIZE,      # Longueur max des s√©quences (256)\n",
    "    n_embd=N_EMBD,              # Dimension des embeddings (256)\n",
    "    n_head=N_HEAD,              # Nombre de t√™tes d'attention (4)\n",
    "    n_layer=N_LAYER,            # Nombre de couches (4)\n",
    "    dropout=DROPOUT             # Taux de dropout (0.1)\n",
    ").to(device)  # D√©placer sur GPU si disponible\n",
    "\n",
    "# Compter le nombre total de param√®tres du mod√®le\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚úÖ Mod√®le cr√©√© avec {n_params:,} param√®tres ({n_params/1e6:.2f}M)\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST DE G√âN√âRATION AVANT ENTRA√éNEMENT\n",
    "# ============================================================================\n",
    "# V√©rifier que le mod√®le peut g√©n√©rer (m√™me si la sortie sera al√©atoire)\n",
    "print(\"\\n--- üé≤ G√âN√âRATION AVANT PRE-TRAINING (Al√©atoire) ---\")\n",
    "test_prompt = \"def fibonacci(n):\"\n",
    "\n",
    "# Encoder le prompt\n",
    "test_ids = torch.tensor([tokenizer.encode(test_prompt)], device=device)\n",
    "\n",
    "# G√©n√©rer 50 tokens\n",
    "generated = model.generate(test_ids, max_new_tokens=50, temperature=0.8)\n",
    "\n",
    "# D√©coder et afficher\n",
    "print(tokenizer.decode(generated[0].tolist()))\n",
    "print(\"---\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709302bf",
   "metadata": {},
   "source": [
    "## üîπ Partie 5 : Boucle d'Entra√Ænement (Pre-Training Loop)\n",
    "\n",
    "Pipeline standard :\n",
    "1. **Forward pass** : Pr√©dictions du mod√®le\n",
    "2. **Loss calculation** : Cross-Entropy entre pr√©dictions et targets\n",
    "3. **Backward pass** : Calcul des gradients\n",
    "4. **Optimizer step** : Mise √† jour des poids (Adam avec weight decay)\n",
    "\n",
    "### Monitoring\n",
    "- Loss train et validation\n",
    "- Perplexity (optionnel)\n",
    "- Exemples de g√©n√©ration p√©riodiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a283ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Training Loop\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION DE L'OPTIMISATION\n",
    "# ============================================================================\n",
    "# Optimizer AdamW: Adam avec weight decay (r√©gularisation L2 am√©lior√©e)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE,  # Learning rate (5e-4)\n",
    "    weight_decay=0.01  # R√©gularisation pour √©viter l'overfitting\n",
    ")\n",
    "\n",
    "# Scheduler: Diminue progressivement le learning rate (am√©liore convergence)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.notebook import tqdm  # Force le mode notebook pour les barres de progression\n",
    "\n",
    "total_steps = len(train_loader) * N_EPOCHS  # Nombre total d'it√©rations\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=total_steps,  # P√©riode compl√®te du cosinus\n",
    "    eta_min=1e-5        # Learning rate minimal √† la fin\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION D'√âVALUATION SUR VALIDATION SET\n",
    "# ============================================================================\n",
    "@torch.no_grad()  # D√©sactiver le calcul des gradients (√©conomise m√©moire)\n",
    "def evaluate(model, val_loader, max_batches=50):\n",
    "    \"\"\"\n",
    "    Calcule la loss moyenne sur le validation set\n",
    "    Args:\n",
    "        model: Le mod√®le √† √©valuer\n",
    "        val_loader: DataLoader de validation\n",
    "        max_batches: Nombre max de batches √† √©valuer (pour acc√©l√©rer)\n",
    "    Returns:\n",
    "        avg_loss: Loss moyenne sur la validation\n",
    "    \"\"\"\n",
    "    model.eval()  # Mode √©valuation (d√©sactive dropout, etc.)\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(val_loader):\n",
    "        # Limiter le nombre de batches pour acc√©l√©rer l'√©valuation\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "        \n",
    "        # D√©placer sur le device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass (pas de backward)\n",
    "        _, loss = model(x, y)\n",
    "        \n",
    "        # Accumuler la loss\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "    \n",
    "    model.train()  # Revenir en mode entra√Ænement\n",
    "    return total_loss / count if count > 0 else 0\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALISATION DE L'HISTORIQUE\n",
    "# ============================================================================\n",
    "# Dictionnaire pour stocker les m√©triques de chaque √©poque\n",
    "history = {\n",
    "    'train_loss': [],  # Loss d'entra√Ænement par √©poque\n",
    "    'val_loss': [],    # Loss de validation par √©poque\n",
    "    'epochs': []       # Num√©ros d'√©poques\n",
    "}\n",
    "\n",
    "print(\"üöÄ D√©but du Pre-Training...\")\n",
    "print(f\"üìä Configuration: {N_EPOCHS} √©poques, {len(train_loader)} batches/√©poque\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# BOUCLE D'ENTRA√éNEMENT PRINCIPALE\n",
    "# ============================================================================\n",
    "model.train()  # Mode entra√Ænement\n",
    "global_step = 0  # Compteur global d'it√©rations\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÖ √âpoque {epoch+1}/{N_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_loss = 0  # Accumulation de la loss pour cette √©poque\n",
    "    pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    # It√©rer sur tous les batches\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        # ====================================================================\n",
    "        # 1. PR√âPARATION DES DONN√âES\n",
    "        # ====================================================================\n",
    "        # D√©placer les tensors sur GPU/CPU\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 2. FORWARD PASS\n",
    "        # ====================================================================\n",
    "        # Pr√©dire les logits et calculer la loss\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 3. BACKWARD PASS\n",
    "        # ====================================================================\n",
    "        # R√©initialiser les gradients (important!)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculer les gradients par backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping: limiter la norme des gradients √† 1.0\n",
    "        # √âvite les explosions de gradients (crucial pour stabilit√©)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 4. MISE √Ä JOUR DES POIDS\n",
    "        # ====================================================================\n",
    "        # Appliquer les gradients aux param√®tres\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Mettre √† jour le learning rate selon le scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 5. LOGGING ET MONITORING\n",
    "        # ====================================================================\n",
    "        # Accumuler la loss de l'√©poque\n",
    "        epoch_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Mettre √† jour la barre de progression avec les m√©triques\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',  # Loss du batch actuel\n",
    "            'avg_loss': f'{epoch_loss/(batch_idx+1):.4f}',  # Loss moyenne de l'√©poque\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'  # Learning rate actuel\n",
    "        })\n",
    "    \n",
    "    # ========================================================================\n",
    "    # M√âTRIQUES DE FIN D'√âPOQUE\n",
    "    # ========================================================================\n",
    "    # Calculer la loss moyenne sur l'√©poque\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # √âvaluer sur le validation set\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    \n",
    "    # Stocker dans l'historique\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epochs'].append(epoch + 1)\n",
    "    \n",
    "    # Afficher les r√©sultats\n",
    "    print(f\"\\nüìä Fin √âpoque {epoch+1}:\")\n",
    "    print(f\"   - Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"   - Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"   - Perplexity: {np.exp(val_loss):.2f}\")  # Perplexit√© = exp(loss)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TEST DE G√âN√âRATION APR√àS CHAQUE √âPOQUE\n",
    "    # ========================================================================\n",
    "    print(f\"\\nüéØ G√©n√©ration test (epoch {epoch+1}):\")\n",
    "    test_prompt = \"def fibonacci(n):\"\n",
    "    test_ids = torch.tensor([tokenizer.encode(test_prompt)], device=device)\n",
    "    \n",
    "    # G√©n√©rer du code avec le mod√®le actuel\n",
    "    generated = model.generate(test_ids, max_new_tokens=80, temperature=0.7, top_k=50)\n",
    "    print(tokenizer.decode(generated[0].tolist()))\n",
    "    print()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # üíæ SAUVEGARDE DU CHECKPOINT APR√àS CHAQUE √âPOQUE\n",
    "    # ========================================================================\n",
    "    # Cr√©er le dossier checkpoints s'il n'existe pas\n",
    "    os.makedirs(\"models/pre_training\", exist_ok=True)\n",
    "    \n",
    "    # Pr√©parer le checkpoint complet (mod√®le + optimizer + historique)\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),        # Poids du mod√®le\n",
    "        'optimizer_state_dict': optimizer.state_dict(),  # √âtat de l'optimizer\n",
    "        'scheduler_state_dict': scheduler.state_dict(),  # √âtat du scheduler\n",
    "        'history': history,                             # Historique des loss\n",
    "        'config': {                                     # Configuration du mod√®le\n",
    "            'vocab_size': vocab_size,\n",
    "            'block_size': BLOCK_SIZE,\n",
    "            'n_embd': N_EMBD,\n",
    "            'n_head': N_HEAD,\n",
    "            'n_layer': N_LAYER,\n",
    "            'dropout': DROPOUT\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder le checkpoint avec le num√©ro d'√©poque\n",
    "    checkpoint_path = f\"models/pre_training/mini_gpt_epoch_{epoch+1}.pt\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"üíæ Checkpoint sauvegard√© : {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pre-Training termin√© !\")\n",
    "print(f\"üìÅ {N_EPOCHS} checkpoints sauvegard√©s dans models/pre_training/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63583a",
   "metadata": {},
   "source": [
    "## üîπ Partie 6 : Visualisation des R√©sultats\n",
    "\n",
    "Analysons les courbes d'apprentissage pour comprendre si le mod√®le :\n",
    "- ‚úÖ Apprend correctement (loss d√©croissante)\n",
    "- ‚ö†Ô∏è Sur-apprend (√©cart train/val croissant)\n",
    "- ‚ùå Sous-apprend (loss stagnante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Visualisation de la Loss\n",
    "\n",
    "# ============================================================================\n",
    "# CR√âATION DES GRAPHIQUES DE PERFORMANCE\n",
    "# ============================================================================\n",
    "# Deux graphiques: Loss et Perplexit√© pour analyser l'apprentissage\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPHIQUE 1: COURBES DE LOSS (TRAIN ET VALIDATION)\n",
    "# ============================================================================\n",
    "axes[0].plot(history['epochs'], history['train_loss'], marker='o', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['epochs'], history['val_loss'], marker='s', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[0].set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('üìâ Courbe d\\'Apprentissage', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPHIQUE 2: PERPLEXIT√â (MESURE PLUS INTERPR√âTABLE QUE LA LOSS)\n",
    "# ============================================================================\n",
    "# Perplexit√© = exp(loss), mesure la \"confusion\" du mod√®le\n",
    "# Plus la perplexit√© est basse, mieux le mod√®le pr√©dit le token suivant\n",
    "perplexity_train = [np.exp(loss) for loss in history['train_loss']]\n",
    "perplexity_val = [np.exp(loss) for loss in history['val_loss']]\n",
    "\n",
    "axes[1].plot(history['epochs'], perplexity_train, marker='o', label='Train Perplexity', linewidth=2)\n",
    "axes[1].plot(history['epochs'], perplexity_val, marker='s', label='Val Perplexity', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoque', fontsize=12)\n",
    "axes[1].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[1].set_title('üìä Perplexit√© (exp(loss))', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSE AUTOMATIQUE DES R√âSULTATS\n",
    "# ============================================================================\n",
    "print(\"\\nüìà Analyse des r√©sultats:\")\n",
    "\n",
    "# Calculer l'am√©lioration entre premi√®re et derni√®re √©poque\n",
    "improvement_train = history['train_loss'][0] - history['train_loss'][-1]\n",
    "improvement_val = history['val_loss'][0] - history['val_loss'][-1]\n",
    "\n",
    "print(f\"   - Am√©lioration train: {improvement_train:.4f}\")  # Diminution de la loss\n",
    "print(f\"   - Am√©lioration val:   {improvement_val:.4f}\")\n",
    "\n",
    "# Calculer l'√©cart train/val (indicateur de sur-apprentissage)\n",
    "gap = history['val_loss'][-1] - history['train_loss'][-1]\n",
    "print(f\"   - Gap train/val:      {gap:.4f}\")\n",
    "\n",
    "# Interpr√©ter l'√©cart\n",
    "if gap < 0.5:\n",
    "    print(\"   ‚úÖ Pas de sur-apprentissage significatif\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Possible sur-apprentissage (consid√©rer plus de donn√©es ou r√©gularisation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318b360",
   "metadata": {},
   "source": [
    "## üîπ Partie 7 : G√©n√©ration de Code Python\n",
    "\n",
    "Testons le mod√®le avec diff√©rents prompts et param√®tres de g√©n√©ration :\n",
    "- **Temperature** : contr√¥le la \"cr√©ativit√©\" (0.5 = conservateur, 1.0 = standard, 1.5 = cr√©atif)\n",
    "- **Top-k** : limite le choix aux k tokens les plus probables\n",
    "\n",
    "### Exemples de prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 9: G√©n√©ration de Code avec Diff√©rents Param√®tres\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION HELPER POUR G√âN√âRATION\n",
    "# ============================================================================\n",
    "def generate_code(prompt, max_tokens=150, temperature=0.8, top_k=40):\n",
    "    \"\"\"\n",
    "    G√©n√®re du code Python √† partir d'un prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Code de d√©part (ex: \"def fibonacci(n):\")\n",
    "        max_tokens (int): Nombre max de tokens √† g√©n√©rer\n",
    "        temperature (float): Contr√¥le la cr√©ativit√©\n",
    "            - 0.5-0.7 : Plus d√©terministe, conservateur\n",
    "            - 0.8-1.0 : Standard, √©quilibr√©\n",
    "            - 1.0-1.5 : Plus cr√©atif, vari√© (peut √™tre incoh√©rent)\n",
    "        top_k (int): Limiter le choix aux k tokens les plus probables\n",
    "            - 10-20 : Tr√®s conservateur\n",
    "            - 30-50 : Standard\n",
    "            - None : Pas de limite (plus de vari√©t√©)\n",
    "    \n",
    "    Returns:\n",
    "        str: Code g√©n√©r√© (prompt + continuation)\n",
    "    \"\"\"\n",
    "    model.eval()  # Mode √©valuation\n",
    "    \n",
    "    # Encoder le prompt en tokens\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
    "    \n",
    "    # G√©n√©rer sans calcul de gradients\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_new_tokens=max_tokens, \n",
    "            temperature=temperature, \n",
    "            top_k=top_k\n",
    "        )\n",
    "    \n",
    "    # D√©coder les tokens en texte\n",
    "    generated = tokenizer.decode(output_ids[0].tolist())\n",
    "    \n",
    "    model.train()  # Revenir en mode entra√Ænement\n",
    "    return generated\n",
    "\n",
    "# ============================================================================\n",
    "# TESTS DE G√âN√âRATION AVEC DIFF√âRENTS PARAM√àTRES\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Prompts de test\n",
    "prompts = [\n",
    "    \"def fibonacci(n):\",\n",
    "    \"class Calculator:\",\n",
    "    \"import numpy as np\\n\\ndef \",\n",
    "    \"# Binary search implementation\\ndef binary_search(arr, target):\",\n",
    "    \"def quicksort(arr):\"\n",
    "]\n",
    "\n",
    "print(\"üéØ G√âN√âRATION DE CODE PYTHON\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Exemple {i} - Prompt: {repr(prompt[:50])}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # G√©n√©ration avec temp√©rature moyenne\n",
    "    print(f\"üå°Ô∏è  Temperature = 0.7 (conservateur)\")\n",
    "    print(\"-\" * 70)\n",
    "    result = generate_code(prompt, max_tokens=120, temperature=0.7, top_k=40)\n",
    "    print(result)\n",
    "    print()\n",
    "    \n",
    "    # G√©n√©ration avec temp√©rature plus √©lev√©e\n",
    "    print(f\"üå°Ô∏è  Temperature = 1.0 (standard)\")\n",
    "    print(\"-\" * 70)\n",
    "    result = generate_code(prompt, max_tokens=120, temperature=1.0, top_k=50)\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ G√©n√©ration termin√©e\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67589f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 10: Sauvegarde Finale du Mod√®le √† partir des Checkpoints\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAUVEGARDE FINALE DU MOD√àLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ANALYSE DES CHECKPOINTS POUR S√âLECTIONNER LE MEILLEUR\n",
    "# ============================================================================\n",
    "# Parcourir tous les checkpoints sauvegard√©s et trouver celui avec la meilleure val_loss\n",
    "print(\"\\nüìä Analyse des checkpoints sauvegard√©s...\")\n",
    "best_epoch = 0\n",
    "best_val_loss = float('inf')  # Initialiser avec une valeur tr√®s √©lev√©e\n",
    "\n",
    "# Parcourir toutes les √©poques\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    checkpoint_path = f\"models/pre_training/mini_gpt_epoch_{epoch}.pt\"\n",
    "    \n",
    "    # V√©rifier que le checkpoint existe\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        # Charger le checkpoint\n",
    "        ckpt = torch.load(checkpoint_path)\n",
    "        \n",
    "        # Extraire la validation loss de cette √©poque\n",
    "        val_loss = ckpt['history']['val_loss'][-1]\n",
    "        print(f\"   √âpoque {epoch}: Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # Mettre √† jour le meilleur si c'est plus bas\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le : √âpoque {best_epoch} (Val Loss = {best_val_loss:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CHARGER LE MEILLEUR CHECKPOINT\n",
    "# ============================================================================\n",
    "best_checkpoint_path = f\"models/pre_training/mini_gpt_epoch_{best_epoch}.pt\"\n",
    "best_checkpoint = torch.load(best_checkpoint_path)\n",
    "\n",
    "print(f\"‚úÖ Checkpoint charg√© : {best_checkpoint_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SAUVEGARDER LE MOD√àLE FINAL OPTIMIS√â\n",
    "# ============================================================================\n",
    "# Cr√©er un checkpoint final avec toutes les informations importantes\n",
    "final_model_path = \"models/pre_training/mini_gpt_code_FINAL.pt\"\n",
    "torch.save({\n",
    "    'epoch': best_checkpoint['epoch'],                          # Num√©ro d'√©poque\n",
    "    'model_state_dict': best_checkpoint['model_state_dict'],    # Poids du mod√®le\n",
    "    'optimizer_state_dict': best_checkpoint['optimizer_state_dict'],  # √âtat optimizer\n",
    "    'scheduler_state_dict': best_checkpoint['scheduler_state_dict'],  # √âtat scheduler\n",
    "    'history': best_checkpoint['history'],                      # Historique complet\n",
    "    'config': best_checkpoint['config'],                        # Configuration du mod√®le\n",
    "    'best_val_loss': best_val_loss,                            # Meilleure val loss\n",
    "    'selected_from_epoch': best_epoch                          # √âpoque s√©lectionn√©e\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"üíæ Mod√®le final sauvegard√© : {final_model_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SAUVEGARDER LE TOKENIZER\n",
    "# ============================================================================\n",
    "# Sauvegarder le tokenizer pour pouvoir l'utiliser plus tard\n",
    "tokenizer.save_pretrained(\"models/pre_training/tokenizer\")\n",
    "print(f\"üî§ Tokenizer sauvegard√© : models/pre_training/tokenizer/\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SAUVEGARDER UNIQUEMENT LES POIDS (VERSION L√âG√àRE)\n",
    "# ============================================================================\n",
    "# Fichier plus l√©ger contenant seulement les poids du mod√®le\n",
    "model_weights_only_path = \"models/pre_training/mini_gpt_weights_only.pt\"\n",
    "torch.save(best_checkpoint['model_state_dict'], model_weights_only_path)\n",
    "print(f\"‚ö° Poids seuls sauvegard√©s : {model_weights_only_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üì¶ R√âSUM√â DES ARTEFACTS CR√â√âS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Checkpoints d'entra√Ænement : models/pre_training/mini_gpt_epoch_[1-{N_EPOCHS}].pt\")\n",
    "print(f\"‚úÖ Mod√®le final (meilleur)     : {final_model_path}\")\n",
    "print(f\"‚úÖ Poids seuls (l√©ger)         : {model_weights_only_path}\")\n",
    "print(f\"‚úÖ Tokenizer                   : models/pre_training/tokenizer/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìå UTILISATION DU MOD√àLE FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n# Option 1: Charger le mod√®le complet (avec optimizer, etc.)\")\n",
    "print(\"checkpoint = torch.load('checkpoints/mini_gpt_code_FINAL.pt')\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"print(f\\\"Mod√®le de l'√©poque {checkpoint['epoch']} charg√©\\\")\")\n",
    "\n",
    "print(\"\\n# Option 2: Charger juste les poids (inf√©rence)\")\n",
    "print(\"model.load_state_dict(torch.load('checkpoints/mini_gpt_weights_only.pt'))\")\n",
    "\n",
    "print(\"\\n# Charger le tokenizer\")\n",
    "print(\"tokenizer = GPT2Tokenizer.from_pretrained('checkpoints/tokenizer')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835fd0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Fin du Pre-Training - Mod√®le de Base Cr√©√©\n",
    "\n",
    "### ‚úÖ Objectifs Accomplis\n",
    "\n",
    "1. **Dataset** : 100k+ lignes de code Python extraites de The Stack\n",
    "2. **Tokenisation** : Tokenizer BPE (GPT-2) configur√©\n",
    "3. **Architecture** : Mini-GPT (256 dims, 4 heads, 4 layers, ~0.X M param√®tres)\n",
    "4. **Entra√Ænement** : CLM sur plusieurs √©poques avec monitoring\n",
    "5. **Sauvegarde** : Checkpoints et tokenizer pr√™ts pour transfert\n",
    "\n",
    "### üìä M√©triques Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856df34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 11: M√©triques Finales\n",
    "\n",
    "# ============================================================================\n",
    "# AFFICHAGE DES M√âTRIQUES FINALES DU PRE-TRAINING\n",
    "# ============================================================================\n",
    "print(\"üìä M√âTRIQUES FINALES DU PRE-TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Informations sur le mod√®le\n",
    "print(f\"Mod√®le         : Mini-GPT ({n_params/1e6:.2f}M param√®tres)\")\n",
    "print(f\"Vocabulaire    : {vocab_size:,} tokens\")  # Taille du vocabulaire (50,257)\n",
    "print(f\"Corpus         : {len(train_dataset):,} s√©quences d'entra√Ænement\")\n",
    "print(f\"Block size     : {BLOCK_SIZE} tokens\")  # Longueur des s√©quences (256)\n",
    "print(f\"√âpoques        : {N_EPOCHS}\")  # Nombre d'√©poques entra√Æn√©es\n",
    "print(f\"Batch size     : {BATCH_SIZE}\")  # Taille des batches (32)\n",
    "\n",
    "# Performance finale\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Train Loss finale : {history['train_loss'][-1]:.4f}\")  # Loss sur train\n",
    "print(f\"  - Val Loss finale   : {history['val_loss'][-1]:.4f}\")    # Loss sur validation\n",
    "print(f\"  - Perplexity        : {np.exp(history['val_loss'][-1]):.2f}\")  # Perplexit√© finale\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Pre-Training du Mini-GPT termin√© avec succ√®s!\")\n",
    "print(\"üì¶ Le mod√®le est pr√™t pour le Post-Training ou Fine-Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86ad44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Prochaines √âtapes du Workshop\n",
    "\n",
    "### Pipeline Complet du Workshop\n",
    "\n",
    "| √âtape | Responsable | Statut | Objectif |\n",
    "|-------|-------------|--------|----------|\n",
    "| ‚úÖ **Pre-Training** | Votre √©quipe | **TERMIN√â** | Apprentissage du langage Python (CLM) |\n",
    "| ‚è≠Ô∏è **Post-Training** | Autres membres | √Ä venir | Alignement et optimisation |\n",
    "| ‚è≠Ô∏è **Fine-Tuning** | Autres membres | √Ä venir | Sp√©cialisation sur t√¢ches sp√©cifiques |\n",
    "\n",
    "### üì¶ Artefacts Cr√©√©s (pour les √©tapes suivantes)\n",
    "\n",
    "Les fichiers suivants sont maintenant disponibles pour le **Post-Training** et **Fine-Tuning** :\n",
    "\n",
    "```\n",
    "models/pre_training/\n",
    "‚îú‚îÄ‚îÄ mini_gpt_code_FINAL.pt    # ‚úÖ Mod√®le pr√©-entra√Æn√©\n",
    "‚îú‚îÄ‚îÄ tokenizer/                # ‚úÖ Tokenizer GPT-2\n",
    "‚îî‚îÄ‚îÄ mini_gpt_epoch_*.pt       # ‚úÖ Checkpoints par √©poque\n",
    "\n",
    "outputs/\n",
    "‚îî‚îÄ‚îÄ mini_corpus_mixed.txt     # ‚úÖ Corpus de code Python\n",
    "```\n",
    "\n",
    "### üîÑ Interface pour les √âtapes Suivantes\n",
    "\n",
    "**Charger le mod√®le pr√©-entra√Æn√© :**\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load('models/pre_training/mini_gpt_code_FINAL.pt')\n",
    "checkpoint = torch.load('checkpoints/mini_gpt_code.pt')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('models/pre_training/tokenizer')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('checkpoints/tokenizer')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä R√©sum√© Pre-Training (Base Model Pr√™t)\n",
    "\n",
    "‚úÖ **Mod√®le de base** entra√Æn√© sur code Python brut\n",
    "‚úÖ **Architecture** : Transformer Decoder-only (GPT-style)  \n",
    "‚úÖ **Capacit√©s** : G√©n√©ration de code Python syntaxiquement correct  \n",
    "‚úÖ **Pr√™t pour** : Post-Training et Fine-Tuning\n",
    "\n",
    "Le mod√®le peut maintenant √™tre utilis√© par les autres membres pour :\n",
    "- **Post-Training** : RLHF, alignement, optimisation\n",
    "- **Fine-Tuning** : Sp√©cialisation (debugging, documentation, tests, etc.)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
